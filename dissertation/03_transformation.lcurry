
In this chapter we introduce our engine for Generating and Altering Subexpressions, of the GAS system.
This system proves to be incredibly versatile and is the main workhorse of the compiler and optimizer.
We show how to construct, combine, and improve the efficiency of transformations,
as well as how the system in implemented.
We then show an extended example of using the GAS system to transform FlatCurry programs
into a canonical form so that we can compile them to C code, as discussed in the last chapter.



\subsection{Building Optimizations}

Optimization is usually considered the most difficult aspect of writing a modern compiler.
It's easy to see why.
There are dozens of small optimizations to make, 
and each one needs to be written, shown correct, and tested.

Furthermore, there are several levels where an optimization can be applied.
Some optimizations apply to a programs AST, some to another intermediate representation,
some to the generated code, and even some to the runtime system.
There are even optimizations that are applied during transformations between representations.
For this chapter, we will be describing a system to apply optimizations to FlatCurry programs.
While this is not the only area of the compiler where we applied optimizations,
it is by far the most extensive, so it's worth understanding how our optimization engine works.

Generally speaking, most optimizations have the same structure.
Find an area in the AST where the optimization applies, and then replace it with the optimized version.
As an example, consider the code for the absolute value function defined below.

> abs x
>  | x < 0      = -x
>  | otherwise  = x

This will be translated into FlatCurry as

> abs x = case  (x < 0) of
>               True   -> -x
>               False  -> case  otherwise of
>                               True   -> x
>                               False  -> EXEMPT

While this transformation is obviously inefficient, 
it is general and has a straightforward implementation.
A good optimizer should be able to recognize that |otherwise| reduces to |True|,
and reduce the case-expression.
So for this one example, we have two different optimizations we need to implement.
We need to reduce |otherwise| to |True|, then we can reduce the second case expression to |x|.

There are two common approaches to solving this problem.
The first is to make a separate function for each optimization.
Each function will traverse the AST and try to apply its optimization.
The second option is to make a few large functions that attempt to apply several optimizations at once.
There are trade-offs for each.

The first option has the advantage that each optimization is easy to write and understand.
However, is suffers from a lot of code duplication,
and it's not very efficient.  We must traverse the entire AST every time we want to apply an optimization.
Both LLVM and the JVM fall into this category. \cite{llvm, jvm}
The second option is more efficient, and there is less code duplication,
but it leads to large functions that are difficult to maintain or extend.

Using these two options generally leads to optimizers that are difficult to maintain.
To combat this problem, many compilers will provide a language to describe optimization transformation.
Then the compiler writer can use this domain specific language to develop their optimizations.
With the optimization descriptions, the compiler can search the AST of a program to find
any places where optimizations apply.
However, It is difficult or impossible to write many common optimizations in this style. \cite{playingByTheRules}

The aim of our solution is to try to get the best of all three worlds.
We've developed an approach to simplify Generating and Altering Subexpressions (GAS)
\index{Generating and Altering Subexpressions (GAS)}.
Our approach was to do optimization entirely by rewriting.
This has several advantages, and might be the most useful result of this work.
First, developing new optimizations is simple.
We can write down new optimizations in this system within minutes.
It was often easier to write down the optimization and test it,
than it was to try to describe the optimization in English.
Second, any performance improvement we made to the optimization engine
would apply to every optimization.
Third, optimizations were easy to maintain and extend.
If one optimization didn't work, we could look at it and test it in isolation.
Fourth, this code is much smaller than a traditional optimizer.
This isn't really a fair comparison given the relative immaturity of our compiler,
but we were able to implement 16 optimizations and code transformations in under 150 lines of code.
This gives a sense of scale of how much easier it is to implement optimizations in this system.
Fifth, Since We're optimizing by rewrite rules,
the compiler can easily output what rule was used,
and the position where it was used.
This is enough information to entirely reconstruct the optimization derivation.
We found this very helpful in debugging.
Finally, optimizations are written in Curry.
We didn't need to develop a DSL to describe the optimizations,
and there are no new ideas for programmers to learn if they want to extend the compiler.

We should note that there are some potential disadvantages to the GAS system as well.
The first disadvantage is that there are some optimizations and transformations
that are not easily described by rewriting.
The second is that, while we've improved the efficiency of the algorithm considerably,
it still takes longer to optimize programs then we'd like.

The first problem isn't really a problem at all.
If there is an optimization that doesn't lend itself well to rewriting,
we can always write it as a traditional optimization.
Furthermore, as we'll see later, we don't have to stay strictly in the bounds of rewriting.
The second problem is actually more fundamental to Curry.
Our implementation relies on finding a single value from a set generated by a non-deterministic function.
Current implementations are inefficient, but there are new implementations being developed.
\cite{synthesizedSetFunctions}
We also believe that an optimizing compiler would help with this problem \cite{this}.

\subsection{The Structure of an Optimization}

The goal with GAS is to make optimizations simple to implement and easily readable.
While this is a challenging problem, we can actually leverage Curry here.
Remember that the semantics of Curry are already non-deterministic rewriting.

Each optimization is going to be a function from a FlatCurry expression to another FlatCurry expression.

> type Opt = Expr -> Expr

For readability we use the FlatCurry syntax defined in figure \ref{fig:flatSyntax},
While this version of FlatCurry is easier to read,
we will need the actual representation of FlatCurry programs to implement the compiler.
This representation is given in figure \ref{fig:flatRep}, and the transformation
from the FlatCurry syntax to the FlatCurry representation is given in figure \ref{fig:flatTranslate}.
We can describe an optimization by simply describing what it does to each expression.
As an example consider the definition for floating let-expressions:

\begin{figure}

> type QName = (String, String)
> type Arity = Int
> type VarIndex = Int
> data Visibility = Public | Private
>
> data FuncDecl = Func QName Arity Rule
> data Rule
>   = Rule [VarIndex] Expr
>   | External String
> data CombType = FuncCall | ConsCall | FuncPartCall Arity | ConsPartCall Arity
> data Expr
>   = Var VarIndex
>   | Lit Literal
>   | Comb CombType QName [Expr]
>   | Let [(VarIndex, Expr)] Expr
>   | Free [VarIndex] Expr
>   | Or Expr Expr
>   | Case Expr [BranchExpr]
> data BranchExpr = Branch Pattern Expr
> data Pattern
>   = Pattern QName [VarIndex]
>   | LPattern Literal
> data Literal
>   = Intc   Int
>   | Floatc Float
>   | Charc  Char
\caption{Curry representation of FlatCurry programs\\
This is the standard representation of FlatCurry programs as defined in \cite{FlatCurry},
We have removed |CaseType| and |Typed| from |Expr|, and 
|TypeExpr| and |Visibility| from |FuncDecl|, because they are not used in our translation.}
\label{fig:flatRep}
\end{figure}

\begin{figure}

> code (f v_1,v_2 ... v_n = e ) = FuncDecl f n (Rule [v_1,v_2 ... v_n] (code e))
> emptyspace
> code v                                     = Var v
> code l                                     = Lit (code l)
> code (e_1 ? e_2)                           = Or (code e_1) (code e_2)
> code EXEMPT                                = Comb ConsCall ("","FAIL") []
> code (f_k e_1 e_2 ... e_n)   | k == 0      = Comb FuncCall f [code e_1, code e_2 ... code e_n]
>                              | otherwise   = Comb (PartFuncCall k) f [code e_1, code e_2 ... code e_n]
> code (C_k e_1 e_2 ... e_n)   | k == 0      = Comb ConsCall f [code e_1, code e_2 ... code e_n]
>                              | otherwise   = Comb (PartConsCall k) f [code e_1, code e_2 ... code e_n]
> code (let v_1 = e_1, ... v_n = e_n in e)   = Let [(v_1,code e_1) ... (v_n,code e_n)] (code e)
> code (let v_1, v_2 ... v_n free in e)      = Free [v_1,v_2, ... v_n] (code e)
> code (case e of branches)                  = Case e (code branches)
> emptyspace
> code (l -> e)                  = LPattern (code l) (code e)
> code (C v_1 v_2 ... v_n -> e)  = Pattern C [v_1,v_2 ... v_n] (code e)
> emptyspace
> code l =  | isInt l    = Intc l
>           | isFloat l  = Floatc l
>           | isChar l   = Charc l
\caption{Translation from FlatCurry syntax to the Curry representation of FlatCurry.}
\label{fig:flatTranslate}
\end{figure}



> float (Comb ct f (as++[Let vs e]++bs) = Let vs (Comb ct f (as++[e]++bs))

This optimization tells us that, if an argument to a function application is a |let| expression,
then we can move the let-expression outside.
This works for let-expressions, but what if there's a free variable declaration inside of a function?
Well, we can define that case with another rule.

> float (Comb ct f (as++[Let vs e]++bs)   = Let vs (Comb ct f (as++[e]++bs))
> float (Comb ct f (as++[Free vs e]++bs)  = Free vs (Comb ct f (as++[e]++bs))

This is where the non-determinism comes in.
Suppose we have an expression:

> f (let x = 1 in x) (let r free in 2)

This could be matched by either rule.
The trick is that we don't care which rule matches, as long as they both do eventually.
This will be transformed into one of the following:

> let r free in let x = 1 in f x 2
> let x = 1 in let r free in f x 2

Either of these options is acceptable.
In fact, we could remove the ambiguity by making our rules a confluent system,
as shown by the code below.
However, we will not worry about confluence for most optimizations.

> float (Comb ct f (as++[Let vs e]++bs)   = Let vs (Comb ct f (as++[e]++bs))
> float (Comb ct f (as++[Free vs e]++bs)  = Free vs (Comb ct f (as++[e]++bs))
> float (Let vs (Free ws e)               = Free ws (Let vs e)

Great, now we can make an optimization.
It was easy to write, but it's not a very complex optimization.
In fact, most optimizations we write won't be very complex.
The power of optimization comes from making small improvements several times.

Now that we can do simple examples, let's look at a more substantial transformation.
Let-expressions are deceptively complicated.
They allow us to make arbitrarily complex, mutually recursive, definitions.
However, most of the time a large let expression could be broken down into several small let expressions.
Consider the definition below:
> let  a = b
>      b = c
>      c = d + e
>      d = b
>      e = 1
> in   a

This is a perfectly valid definition,
but we could also break it up into the three nested let expressions below.

> let  e = 1
> in   let  b = c
>           c = d + e
>           d = b
>      in   let  a = b
>           in   a

It's debatable which version is better coding style, but the second version
is inarguably more useful for the compiler.
There are several optimizations that can be safely performed on a single let bound variable.
Unfortunately, splitting the let expression into blocks isn't trivial.
The algorithm involves making a graph out of all references in the let block,
then finding the strongly connected components of that reference graph,
and, finally, rebuilding the let expression from the component graph.
The full algorithm is given below.

> blocks (Let vs e) | numBlocks > 1 = e'
>   where (e', numBlocks) = makeBlocks es e

> makeBlocks vs e = (letExp, length comps)
>  where  letExp = foldr makeBlock e comps
>         makeBlock comp = \exp -> Let (map getExp comp) exp
>         getExp (_++[(n,exp)]++_) = (n,exp)
> 
>         comps = scc (vs >>= makeEdges)
>         makeEdges (v, exp) = [(v,f) | f <- freeVars exp `intersect` map fst vs]

While this optimization is significantly more complicated then the |float| example,
We can still implement it in our system.
Furthermore, we're able to factor out the code for building the graph
and finding the strongly connected components.
This is the advantage of using Curry functions as opposed to strict rewrite rules.
We have much more freedom in constructing the right-hand side of our rules.

Now that we can create optimizations, what if we want both |blocks| and |float| to be able to run?
This is an important part of the compilation process to get expressions into a canonical form.
It turns out that combining two optimizations is simple.
We just make a non-deterministic choice between them.

> floatBlocks = float ? blocks

This is a new optimization that will apply either |float| or |blocks|.
The ability to compose optimizations with |?| is the heart of the GAS system.
Each optimization can be developed and tested in isolation,
then they can be combined for efficiency.

\subsection{An Initial Attempt}

Our first attempt is quite simple, really.
We pick an arbitrary subexpression with |subExpr|
and apply an optimization.
We can then use a non-deterministic fix point operator to find
all transformations that can be applied to the current expression.
We can define the non-deterministic fix point operator using either
the Findall library, or Set Function \cite{findall, setFunctions}
The full code is given in figure \ref{fig:optFirst}

\begin{figure}
> fix :: (a -> a) -> a -> a
> fix f x
>  | f x == emptyset  = x
>  | otherwise        = fix f (f x)

> subExpr :: Expr -> Expr
> subExpr e               = e
> subExpr (Comb ct f vs)  = subExpr (foldr1 (?) es)
> subExpr (Let vs e)      = subExpr (foldr1 (?) (e : map snd es))
> subExpr (Free vs e)     = subExpr e
> subExpr (Or e_1 e_2)    = subExpr e_1 ? subExpr e_2
> subExpr (Case e bs)     = subExpr (e : map branchExpr bs)
>  where  branchExpr (Branch _ e) = e

> reduce :: Opt -> Expr -> Expr
> reduce opt e = opt (subExpr e)

> simplify :: Opt -> Expr -> Expr
> simplify opt e = fix (reduce opt) e
\caption{A first attempt at an optimization engine.
         Pick an arbitrary subexpression and try to optimize it.}
\label{fig:optFirst}
\end{figure}

While this attempt is short and readable,
there's a problem with it.
It is unusably slow.
While looking at the code, it's pretty clear to see what the problem is.
Every time we traverse the expression, we can only apply a single transformation.
This means that if we need to apply 100 transformations, which is not uncommon,
then we need to traverse the expression 100 times.

\subsection{A Second Attempt: Multiple Transformations Per Pass}

Our second attempt runs much faster.
Instead of picking an arbitrary subexpression,
we choose to traverse the expression manually.
Now, we can check at each node if an optimization applies.
We only need to make two changes.
The biggest is that we eliminate |subExpr| and change |reduce| to traverse the entire expression.
Now |reduce| can apply an optimization at every step.
We've also made |reduce| completely deterministic.
The second change is that since |reduce| is deterministic, we can change |fix|
to be a more traditional implementation of a fix point operator.
The new implementation is given in figure \ref{fig:optReduce}

\begin{figure}

> fix :: (a -> a) -> a -> a
> fix f x
>  | f x == x = x
>  | otherwise = fix f (f x)
> emptyspace
> reduce                                :: Opt -> Expr -> Expr
> reduce opt (Var v)             = runOpts opt (Var v)
> reduce opt (Lit l)             = runOpts opt (Lit l)
> reduce opt (Comb ct f es)      = runOpts opt (Comb ct f (map (reduce opt) es))
>  where  es' = map (reduce opt) es
> reduce opt (Let vs e)          = runOpts opt Let (map runLet vs) (reduce opt e)
>  where  runLet (v,e) = (v, reduce opt e)
> reduce opt (Free vs e)         = runOpts opt (Free vs (reduce opt e))
> reduce opt (Or a b)            = runOpts opt (Or (reduce opt a) (reduce opt a))
> reduce opt (Case e bs)         = runOpts opt (Case (reduce opt e) bs')
>  where  runBranch (Branch p e) = Branch p (reduce opt e)
>         bs' = map runBranch bs
> emptyspace
> runOpts :: Opt -> Expr -> Expr
> runOpts opt e =  case  oneValue (opt e) of
>                        Nothing  -> e
>                        Just e'  -> e'
> emptyspace
> simplify :: Opt -> Expr -> Expr
> simplify opt e = fix (reduce opt) e
\caption{A second attempt.
         Traverse the expression and, at each node, check if an optimization applies.}
\label{fig:optReduce}
\end{figure}

This approach is significantly better.
Aside from applying multiple rules in one pass,
we also limit our search space when applying our optimizations.
While there's still more we can do,
the new approach makes the GAS library usable on larger Curry programs,
like the standard Prelude.

\subsection{Adding More Information}

Rather surprisingly our current approach is actually sufficient for compiling FlatCurry.
However, to optimize Curry we're going to need more information when we apply a transformation.
Specifically, we'll be able to create new variables.
To simplify optimizations, we'll require that each variable name can only be used once.
Regardless, we need a way to know what is a safe variable name that we're allowed to use.
We may also need to know if we're rewriting the root of an expression.
Fortunately, all we need to change is to define |Opt| to accept more parameters.
For each optimization, we'll pass in an |n :: Int|
that represents the next variable |v_n| that is guaranteed to be fresh.
We'll also pass in a |top :: Bool| that tells us if we're at root of a function we're optimizing.
We also return a pair of |(Expr,Int)| to denote the optimized expression,
and the number of new variables we used.

> type Opt = (Int,Bool) -> Expr -> (Expr,Int)

If we later decide that we want to add more information, then we just update the first parameter.
The only problem is, how do we make sure we're calling each optimization with the correct |n| and |top|?
We just need to update |reduce| and |runOpt|.
In order to keep track of the next available free variable we use the |State| monad.
We do need to make minor changes to |fix| and |simplify|,
but this is just to make them compatible with |State|.
The full implementation is in figure \ref{fig:optAddInfo}.


\begin{figure}
> reduce :: Opt -> Bool -> Expr -> State Int Expr
> reduce opt top (Var v)          = runOpts opt top (Var v)
> reduce opt top (Lit l)          = runOpts opt top (Lit l)
> reduce opt top (Comb ct f es)   = do  es' <- mapM (reduce opt False es)
>                                       runOpts opt top (Comb ct f es')
> reduce opt top (Let vs in e)    = do  vs'  <- mapM runVar vs
>                                       e'   <- mapM reduce opt False e
>                                       runOpts opt top (Let vs' in e')
>  where  runVar (v, e) = do  e' <- reduce opt False e
>                             return (v, e')
> reduce opt top (Free vs e)      = do  e' <- reduce opt False e
>                                       runOpts opt top (Free vs e')
> reduce opt top (Or a b)         = do  a'  <- run opt False a
>                                       b'  <- run opt False b
>                                       runOpts opt (Or a' b')
> reduce opt top (Case e bs)      = do  e'    <- reduce opt False e
>                                       bs'   <- mapM runBranch bs
>                                       runOpts opt (Case e' bs')
>  where runBranch (Branch pat e)  = do  e' <- reduce opt False e
>                                        return (Branch pat e')
> emptyspace
> runOpts :: Opt -> Bool -> Expr -> State Int Expr
> runOpts opt top e =  do  v <- get
>                          case  opt (v,top) e of
>                                Nothing       -> return e
>                                Just (e',dv)  -> do  put (v+dv)
>                                                     return e'
> emptyspace
> fix :: (a -> State b a) -> a -> b -> a
> fix f x s =  let  (x',s') = runState (f x) s
>              in   if x == x' 
>                   then x
>                   else fix f x' s'
> emptyspace
> simplify :: Opt -> Expr -> Expr
> simplify opt e = fix (reduce opt True) e (maximum (vars e) + 1)
\caption{A third attempt.
         keep track of the next fresh variable, and if we're at the root.}
\label{fig:optAddInfo}
\end{figure}


\subsection{Reconstruction}

Right now we have everything we need to write all of our optimizations.
However, we've found it useful to be able to 
track which optimizations were applied and where they were applied.
This helps with testing, debugging, and designing optimizations,
as well as generating optimization derivations that we'll see later in this dissertation.
It is difficult to overstate just how helpful this addition was in building this compiler.

If we want to add this, then we need to make a few changes.
First, we need to decide on a representation for a rewrite derivation.
Traditionally a rewrite derivation is a sequence of rewrite steps,
where each step contains the rule and position of the rewrite.
We describe paths in figure \ref{fig:path}.
To make reconstruction easier, we also include the expression that is the result of the rewrite.
This gives us the type:
> type Path = [Int]
> type Step = (String, Path, Expr)
> type Derivation = [Step]


\begin{figure}
> ndpath e                 = []
> ndpath (Comb ct f es)    = anymap argPath es
>  where argPath (i,e) = i : ndpath e_i
> ndpath (Or e_0 e_1)      = 0 : ndpath e_0 ? 1 : ndpath e_1
> ndpath (Let vs e_n1)     = anymap letPath es
>                          ?  -1 : ndpath e_n1
>  where letPath (i,(_, e)) = i : ndpath e
> ndpath (Case e of alts)  = -1 : ndpath e
>                          ? anymap altPath alts
>  where altPath (i,Branch _ e) = i : ndpath e
> emptyspace
> anymap f = anyof . map f . zip [1..]
\caption{The definition of a path for Curry expressions.\\
         This function non-deterministically returns a path to a subexpression.}
\label{fig:path}
\end{figure}


This leads to the last change we need to make to our |Opt| type.
We need each optimization to also tell us its name.
This is good practice in general, 
because it forces us to come up with unique names for each optimization.

> type Opt = (Int,Bool) -> Expr -> (Expr, String, Int)

We only need to make a few change to the algorithm.
Instead of using the |State| monad, we use a combination of the |State| and |Writer| monads,
so we can keep track of the derivation.
We've elected to call this the |ReWriter| 
monad.
We add a function |update :: Expr -> Step -> Int -> ReWriter Expr| 
that is similar to |put| from |State|.  This updates the state variable, and creates a single step.
The |reduce| function requires few changes.
We change the Boolean variable |top| to a more general |Path|.
Because of this change, we need to add the correct subexpression position,
instead of just changing |top| to |False|.
The |RunOpts| function is similar.  We just change |top| to a |Path|,
and check if it's null.
Finally |fix| and |simplify| are modified to remember the rewrite steps we've already computed.
We change the return type of |simplify| so that we have the list of steps.
The full implementation is in figure \ref{fig:reconstruct}

\begin{figure}
> reduce :: Opt -> Path -> Expr -> ReWriter Expr
> reduce opt p (Var v)         = runOpts opt p (Var v)
> reduce opt p (Lit l)         = runOpts opt p (Lit l)
> reduce opt p (Comb ct f es)  = do  es' <- mapM runArg (zip [0..] es)
>                                    runOpts opt p (Comb ct f es)
>  where  runArg (n, e)        = do  e' <- reduce opt (n:p) e
>                                    return e'
> reduce opt p (Let vs e)      = do  vs'  <- mapM runVar (zip [0..] vs)
>                                    e'   <- mapM reduce opt (-1:p) e
>                                    runOpts opt p (Let vs' e')
>  where  runVar (n, (v, e))   = do  e' <- reduce opt (n:p) e
>                                    return (v, e')
> reduce opt p (Free vs in e)  = do  e' <- reduce opt (0:p) e
>                                    runOpts opt p (Free vs e')
> reduce opt p (Or a b)        = do  a  <- reduce opt (0:p) a
>                                    b  <- reduce opt (1:p) b
>                                    runOpts opt (Or (a' ? b'))
> reduce opt p (Case e bs))    = do  e'    <- reduce opt (-1:p) e
>                                    bs'   <- mapM runBranch (zip [0..] bs)
>                                    runOpts opt (Case e' bs')
>  where runBranch (n, (Branch pat e))  = do  e' <- reduce opt (n:p) e
>                                             return (Branch pat e')
>         
> emptyspace
> runOpts :: Opt -> Path -> Expr -> ReWriter Expr
> runOpts opt p e =  do  v <- get
>                        case  oneValue (opt (v, null p)) of
>                              Nothing            ->  return e
>                              Just (e',rule,dv)  ->  do update (e',rule,p) dv
>                                                     return e'
> emptyspace
> fix :: (a -> ReWriter a) -> a -> Int -> [Step] -> (a,[Step])
> fix f x n steps =  let  (x',n',steps') = runRewriter (f x) n
>                    in   if x == x' 
>                         then x
>                         else fix f x' n' (steps++steps')
> emptyspace
> simplify :: Opt -> Expr -> (Expr,[Step])
> simplify opt e = fix (reduce opt []) e (maximum (vars e) + 1) []
\caption{The final version of GAS with reconstruction.}
\label{fig:reconstruct}
\end{figure}

Now that we've computed the rewrite steps, it's a simple process to reconstruct them into a string.
The |pPrint| function comes from the FlatCurry Pretty Printing Library.

> reconstruct :: Expr -> [Step] -> String
> reconstruct _ [] = ""
> reconstruct e ((rule, p, rhs):steps) =  let e' = extend e p rhs
>                                         in  "=>_"++rule++" "++(show p)++"\n" ++
>                                             pPrint e' ++"\n" ++
>                                             reconstruct e' steps

\subsection{Optimizing the Optimizer}

Remember that our optimizing engine is going to run for every optimization,
so it's worth taking the time to tune it to be as efficient as possible.
There are a few tricks we can use to make the optimization process faster.
The first trick is really simple.  We add a Boolean variable |seen| to the ReWriter monad.
This variable starts as |False|, and we set it to |True| if we apply any optimization.
This avoids the linear time check for every call of |fix| to see if we actually ran any optimizations.
The second quick optimization is to notice that variables, literals, and type expressions 
are never going to run an optimization, 
so we can immediately return in each of those cases without calling |runOpt|.
This is actually a much bigger deal than it might first appear.
All of the leaves are going to either be variables, literals, or constructors applied to no arguments.
For expression trees the leaves are often the majority of the nodes in the tree.
Finally, we can put a limit on the number of optimizations to apply.
If we ever reach that number, then we can immediately return.
This can stop our optimizer from taking too much time.

Now that the GAS system is in place, we can move onto compiling FlatCurry programs.
In the next section we discus the compiler pipeline, and how to transform the FlatCurry into C.

\section{The Compiler Pipeline}

This compiler, unsurprisingly, follows a traditional compiler pipeline.
While we start with an AST, there are still five phases left before we can generate C code.
First, we normalize FlatCurry to a canonical form. 
Second we optimize the FlatCurry. 
Third, we sanitize the FlatCurry to simplify the process of generating C code.
Fourth, we compile the FlatCurry to ICurry, an intermediate representation that is closer to C.
Finally, we compile the ICurry to C.
These steps are referred to as pre-process, optimize, post-process, toICurry, and toC within the compiler.
We give an example of the |<=| function as it passes through each of the stages of the compiler in figure \ref{fig:stages}.
While there are several small details that are important to constructing a working Curry compiler,
we'll concern ourselves with the big picture here.

\begin{figure}
Curry function
> f True = False
> f x = not (let y = not x in not y)

FlatCurry representation
> f v_1 =  (case v_1 of True -> False) ? 
>          (not (let v_2 = not v_1 in not v_2))

After Pre-processing
> f v_1 =  let v_2 = not v1_ 
>          in  (case  v_1 of 
>                     True -> False 
>                     False -> EXEMPT) 
>              ? (not (not v2))

After Optimization
> let  v_4 = case  v_1 of
>                  True -> False
>                  False -> EXEMPT
> in   let  v_5 = case  v_1 of 
>                       True -> False 
>                       False -> True 
>      in   v4 ? v5

After Post-processing
> f v_1 =  let  v_4 = f_0 v_1
>          in   let  v_5 = f_1 v_1
>               in   v_4 ? v_5
> f_0 v_1 = fcase  v_1 of
>                  True -> False
>                  False -> EXEMPT
> f_1 v_1 = fcase  v_1 of
>                  True -> False
>                  False -> True
\caption{|f| at each stage of the compiler.\\
         After pre-processing the let expression has been floated to the root,
         and the missing branch has been filled in.
         After optimization, code is organized into blocks, and functions have been reduced.
         After post processing, let bound variables with a case expression have been factored
         out into functions, then the code is translated into ICurry and C.}
\label{fig:stages}
\end{figure}

\subsection{Canonical FlatCurry}

The preproces and post-process steps of the compiler make heavy use the of GAS system,
and transform the FlatCurry program in to a form that is more amenable to C,
such as removing case and let expression from inside function applications.
We will discuss the optimization phase in the next section,
but for now we can see how transformations work.

\begin{figure}
> f v_1 v_2 ... v_n = b
>
> s  =  case  e of {C v_1 ... v_n -> s}
>    |  let {v = e} in s
>    |  let {v} free in s
>    |  e
>
> e  =  v
>    |  l
>    |  f_k e_1 e_2 ... e_n
>    |  C_k e_1 e_2 ... e_n
>    |  e_1 ? e_2

\caption{Canonical FlatCurry.\\
         We split expressions into statement like expressions |s|, and expressions |e|.
         Statement like expressions roughly correspond to control flow, and are translated
         to variable declaration and control flow statements in C.  }
\label{fig:flatCanonical}
\end{figure}

Let's start with an example:
> 1 + let x = 3 in x

This is a perfectly fine Curry program, 
but C does not allow variable declarations in an expression,
so we need to rewrite this Curry expression to:
> let x = 3 in 1 + x

We don't reduce |let x = 3 in x| yet, because that would be an optimization.
However, this will be reduced later.
We can translate the new expression to C in a direct manner.
This is the purpose of the pre-process and post-process steps.
We rewrite a Curry expression that doesn't make sense in C to an equivalent Curry expression
that we can translate directly to C.
Most of the transformations consist of disallowing certain syntactic constructs.
Canonical FlatCurry is defined in figure \ref{fig:flatCanonical}.

The implementation of the preprocessing transformations are presented in figures.
\ref{fig:canonical} and \ref{fig:canonical2}.
The implementation is presented in figure \ref{fig:code}.
We only show the initial implementation of an optimiation
that excludes the name and path, but it can be extended to the full optimization
in a straightforward manner.
The full implementation can be found in 
the \texttt{src/Optimize/Preprocess.curry} file at \cite{git}.

In practice several of these rules are generalized and optimized.
For example let-expressions may have many mutually recursive variables,
and when floating a let bound variable inward, we may want to recursively traverse
the expression to find the innermost declaration possible.
However, these extensions to the rules are also included in the repository.

\begin{figure}
> float (Let (as++[(x,Let vs e_1)]++bs) e_2)   = Let ((x,e_1):vs++as++bs) e_2
> float (Let (as++[(x,Free vs e_1)]++bs) e_2)  = Free vs (Let ((x,e_1):as++bs) e_2)
> float (Or (Let vs e_1) e_2)                  = Let vs (Or e_1 e_2)
> float (Or e_1 (Let vs e_2))                  = Let vs (Or e_1 e_2)
> float (Or (Free vs e_1) e_2)                 = Free vs (Or e_1 e_2)
> float (Or e_1 (Free vs e_2))                 = Free vs (Or e_1 e_2)
> float (Comb ct n (as++[Let vs e]++bs))       = Let vs (Comb ct n (as++[e]++bs))
> float (Comb ct n (as++[Free vs e]++bs))      = Free vs (Comb ct n (as++[e]++bs))
> float (Case (Let vs e) alts)                 = Let vs (Case e alts)
> float (Case (Free vs e) alts)                = Free vs (Case e alts)
> emptyspace
> flatten (apply (apply f as) bs)    = applyf f (as++bs)
> flatten (apply (Case e bs) xs)     = Case e bs'
>   where bs' = [Branch p (applyf e' xs) | (Branch p e') <- bs]
> flatten (Case (Case e alt2) alt1)  = Case e bs (map addCase alt2)
>   where addCase (Branch p e') = Branch p (Case e' b1)
> emptyspace
> blocks _ (Let vs e) | changed = e'
>   where (e', changed) = makeBlocks vs e
> emptyspace
> alias _ (Let (as++[(v, Var y)]++bs) e)
>  | v == y       = Let (as++[(v,loop)]++bs) e
>  | otherwise    = suby (Let (as++bs) e)
>   where  loop = Comb FuncCall ("Prelude","loop") []
>          suby = sub (\x -> x == v then Var y else Var x)
> emptyspace
> fillCases dt _ (Case e bs)
>   | not (null exempts) = Case e (bs++exempts)
>   where exempts = [Branch (Pattern b []) exempt | b <- missingBranches dt bs]
\caption{In |fillCases|, |dt| is a |DataTable|, which holds information about data types.
         The |missingBranches| takes a list of branches and a |DataTable| and returns the names
         of the branches that aren't present.
         In |alias| the |sub| function applies a substitution to an expression.}
\label{fig:code}
\end{figure}

\begin{figure}
>FLOAT
\begin{minipage}{.40\textwidth}
> let x =  let y = e_1 
>          in e_2 
> in e_3
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let  y = e_1
>      in  let  x = e_2
>          in   e_3
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> let x =  let y free 
>          in e_1
> in e_2
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let y free
> in  let x = e_2
>     in e_3
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> (let x = e_1 in e_2) ? e_3
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x = e_1 in (e_2 ? e_3)
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> (let x free in e_1) ? e_2
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x free in (e_1 ? e_2)
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> f (let x = e_1 in e_2)
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x = e_1
> in f e_2
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> f (let x free in e)
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x free
> in f e
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> case  let x = e_1 
>       in e_2  of 
>       alts
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x = e_1
> in case e_2 of alts
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> case  let x free 
>       in e  of 
>       alts
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x free
> in case e of alts
\end{minipage}
\caption{GAS rules for putting FlatCurry programs into canonical form}
\label{fig:canonical}
\end{figure}

\begin{figure}
\vspace{-4ex}
\textbf{Case in Case}\\
\begin{minipage}{.40\textwidth}
> case  (case  e of 
>              {b_2 -> e_2}) of
>       {b_1 -> e_1}
>
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> case e of  {  b_2 -> 
>               case  e_2 of  
>                     {b_1 -> e_1} }
>
\end{minipage}\\
\textbf{Double Apply}\\
\begin{minipage}{.40\textwidth}
> apply (apply f [x]) [y]
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> apply f [x,y]
\end{minipage}\\
\textbf{Case Apply}\\
\begin{minipage}{.40\textwidth}
> apply (case  e of 
>              {pat -> f}) x
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> case  e of 
>       {pat -> f x}
\end{minipage}\\
\textbf{Blocks}\\
\begin{minipage}{.40\textwidth}
> let  a = b
>      b = c
>      c = d + e
>      d = b
>      e = 1
> in   a
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let  e = 1
> in   let  b = c
>           c = d + e
>           d = b
>      in   let  a = b
>           in   a
\end{minipage}\\
\textbf{Alias}\\
\begin{minipage}{.40\textwidth}
> let x = y in e
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> e[x `to` y]
\end{minipage}\vspace{-4ex}\\
\begin{minipage}{.40\textwidth}
> let x = x in e
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x = loop in e
\end{minipage}\\
\textbf{Case Fill}\\
\begin{minipage}{.40\textwidth}
> case  e of  
>       True -> e
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> case  e of 
>       True   -> e
>       False  -> EXEMPT
\end{minipage}\\

\caption{GAS rules for putting FlatCurry programs into canonical form (continued)}
\label{fig:canonical2}
\end{figure}

While most of these transformations are simple,
a few require some explanation.
The \textbf{blocks} transformation takes a let block with multiple variable definitions,
and rewrites it to a series of let blocks where all variables are split into
strongly connected components.
This isn't strictly necessary,
but it removes the need to check for mutual recursion during the optimization phase.
It will often transform a block of mutually defined variables into a cascading 
series of let expressions with a single variable,
which will allow more optimizations to run throughout the compiler.

The \textbf{alias} transformation will remove any aliased variables.
If one variables is aliased to another, then it will do the substitution,
but if a variable is aliased to itself, then it cannot be reduced to a normal form,
so we can replace it with an infinite loop.


Finally the \textbf{Fill cases} transformation completes the definitional tree.
If we have a case with branches for constructors |C_1, C_2 ... C_k|,
then we look up the type |T| that all of these constructor belong to.
Then we get the list |Ctrs| of all constructors that belonging to |T|.
This list will contain |C_1, C_2, ... C_n|, but it may contain more.
For each constructor not represented in the case-expression,
we create a new branch |C_i -> EXEMPT|.

After running all of these transformations, our program is in canonical form
and we may choose to optimize it, or we may skip straight to the post-processing phase.
At this point we only need two transformations for post processing,
however we will need to add more to support some of the optimizations.
If we ever have an expression of the form |let x = case ...|, then we need to transform the
case-expression into a function call.
We don't do this transformation in pre-processing
because we don't want to split functions apart during optimizations.
The \textbf{Let-Case} transformation has a single rule given in figure \ref{fig:letcase}.

\begin{figure}
\textbf{Let Case}\\
\begin{minipage}{.40\textwidth}
> f v_1 ... v_n 
>    =  let x = case   e of  
>                      {p_i -> e_i}
>       in e'
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> f v_1 ... v_n  =  let x = f_1 x_1 ... x_k
>                   in e'
> f_1 x_1 ... x_k  =  case  e of  
>                           {p_i -> e_i}
>  where set (x_1 ... x_k) == freeVars [e,e_1,e_2...e_n]
\end{minipage}\\
\textbf{Var Case}\\
\begin{minipage}{.40\textwidth}
> case e of alts
\end{minipage} |=>|
\begin{minipage}{.40\textwidth}
> let x = e in case x of alts
\end{minipage}\\
\caption{Rule for moving a let bound case out of a function, 
         and eliminating compound expressions in case-expressions.}
\label{fig:letcase}
\end{figure}

Every let with a case-expression creates a new function |outline f n|
where |n| is incremented every time.

Finally, in our post-processing phase we simply factor out 
the scrutinee of a case-expression into a variable.
The transformation is straightforward.
An example of a preprecess derivation is given in \ref{fig:preprocess_example}.
At this point point we are ready to transform the cononicalized FlatCurry into ICurry.

\begin{figure}
> poweraux v1 v2 v3 = case  (==) v3 0 of
>                           True   ->  v1
>                           False  ->  let  v4 = square v2
>                                           v5 = halve v3
>                                      in   case  (==) (appOpt ((apply (apply mod v3) 2) 1)) of
>                                                 True   -> powaux ((*) v1 v2) v4 v5
>                                                 False  -> powaux v1 v4 v5
> => DOUBLE_APPLY [1,-1,-1,0]
> poweraux v1 v2 v3 = case  (==) v3 0 of
>                           True   ->  v1
>                           False  ->  let  v4 = (appOpt (square v2))
>                                           v5 = (appOpt (halve v3))
>                                      in   case  (==) (apply mod v3 2) 1 of
>                                                 True   -> powaux ((*) v1 v2) v4 v5
>                                                 False  -> powaux v1 v4 v5
> => BLOCKS [1]
> poweraux v1 v2 v3 = case  ((==) v3 0) of
>                           True   ->  v1
>                           False  ->  let  v4 = square v2
>                                      in   let  v5 = halve v3
>                                           in   case  (==) (apply mod v3 2) 1 of
>                                                      True   -> powaux ((*) v1 v2) v4 v5
>                                                      False  -> powaux v1 v4 v5
    \caption{Reducing the |powaux| function defined in the standard Float library.
             The first reduction occurs in the |False| branch |[1]| or the in expression [-1]
             in the scrutinee of the case [-1] in the first argument of the apply node [0],
             so it has a path of [1,-1,-1,0].}
    \label{fig:preprocess_example}
\end{figure}

\subsection{ICurry}

ICurry is meant to be a bridge between Curry code and imperative languages like C, Python, and Assembly.
The let and case-expressions have been transformed into statements,
and variables have been explicitly declared.
All mutually recursive declarations are broken here into two steps.
Declare memory for each node, 
then fill in the pointers.
This allows us to create expression graphs with loops in them.
Each function is organized into a sequence of blocks,
and each block is broken up into declarations, assignments, and a single statement.
A statement can either fail, return a new expression graph, 
or inspect a single variable to choose a case.
It should be noted that restricting the scrutinee of the case statement
to a single variable will cause efficiency problems, but
we'll address this later.


\begin{figure}[t]
> p         =>  t^* f^*                       program
> t         =>  C_1 C_2 ... C_n               datatype
> f         =>  f = b                         function
> b         =>  d_1                           block
>               ... 
>               d_k 
>               a_1 
>               ... 
>               a_n 
>               s  
> d         =>  declare x                     variable declaration
>           |   declfree x                    free variable declaration
> a         =>  v = e
> s         =>  return e                      return statement
>           |   EXEMPT                        failure
>           |   case x of                     case statement
>                    C_1 -> b_1 
>                    ...
>                    C_n -> b_n
> e         =>  v                             variable expression
>           |   NODE(l,e_1,...,e_n)           node creation
>           |   e_1 ? e_2                     choice expression
> v         =>  x                             local variable
>           |   v[i]                          variable access
>           |   ROOT                          root variable
> l         =>  C                             constructor label
>           |   f                             function label
>           |   LABEL(v)                      variable label
\caption{Abstract syntax of function definitions in ICurry}
\label{fig:iCurrySyntax}
\end{figure}

After we've finished transforming the FlatCurry,
the transformation to ICurry is much easier to implement.
The algorithm from \cite{icurry}, given in figure \ref{fig:flatCurryiCurry}, 
can be applied directly to the translated program.
We show an example of translating the function |f| from figure \ref{fig:stages}
into ICurry in figure \ref{fig:icurryStage}.



\begin{figure}
>ifunction(f(x_1,...,x_n) = e) := f = iblock(x_1,..., x_n, e, ROOT)
>
>iblock(x_1,...,x_n, EXEMPT, root)  := EXEMPT 
>iblock(x_1,...,x_n, e, root)       := 
>    declare x_1
>    ...
>    declare x_n
>    decl(e)
>    x_1 = root[1]
>    ...
>    x_n = root[n]
>    asgn(e)
>    ret(e)
>
>decl(let x_1,...,x_n free in e) :=
>    decfree x_1
>    ...
>    decfree x_n
>decl(let {x_1 = e_1;...;x_n = e_n} in e) :=
>    declare x_1
>    ...
>    declare x_n
>decl(case e of {p_1 -> e_1; ...; p_n -> e_n}) := declare x_e
>
>asgn(let {x_1 = e_1;...;x_n = e_n} in e) :=
>    x_1 = expr(e_1)
>    ...
>    x_n = expr(e_n)
>    [x_i[p] = x_j | x_j `in` {x_1,...,x_n}, e_i|_p = x_j]
>
>asgn(case e of _) := x_e = expr(e)
>
>
>ret(case e of  C_1(x_11,...,x_1m) -> e_1   := case E(e) of  iblock(x_11,...,x_1m, e_1, x_e)
>               ...                                          ...
>               C_n(x_n1,...,x_nk) -> e_n)                   iblock(x_n1,...,x_nk, e_n, x_e)
>ret(e)                                     := return E(e)
>
>expr(x)                                     := x
>expr(c e_1 ... e_n)                         := NODE(c, expr(e_1),...,expr(e_n))
>expr(f e_1 ... e_n)                         := NODE(f, expr(e_1),...,expr(e_n))
>expr(e_1 ? e_2)                             := expr(e_1) ? expr(e_2)
>expr(let {x_1 = e_1;...;x_n = e_n} in e)    := expr(e)
>expr(let x_1,...,x_n free in e)             := expr(e)

\caption{Algorithm for translating FlatCurry into ICurry}
\label{fig:flatCurryiCurry}
\end{figure}

\begin{figure}
After Post-processing
> f v_1 =  let  v_4 = f_0 v_1
>          in   let  v_5 = f_1 v_1
>               in   v_4 ? v_5
> f_0 v_1 = fcase  v_1 of
>                  True -> False
>                  False -> EXEMPT
> f_1 v_1 = fcase  v_1 of
>                  True -> False
>                  False -> True
ICurry
> f/1 : {
>   declare x_2
>   declare x_3
>   declare x_4
>   x_2 = ROOT[0]
>   x_3 = f_0(x2)
>   x_4 = f_1(x2)
>   return (x_3 ? x_4)
> }
> f_0/1 : {
>   declare x_2
>   x_2 = ROOT[0]
>   case x_2 of
>     True / 0 -> {
>       return False()
>     }
>     False / 0 -> {
>       exempt
>     }
> }
> 
> f_1/1 : {
>   declare x_2
>   x_2 = ROOT[0]
>   case x_2 of
>     True / 0 -> {
>       return False()
>     }
>     False / 0 -> {
>       return True()
>     }
> }
\caption{translating |f| |f_0| and |f_1| into icurry}
\label{fig:icurryStage}
\end{figure}


\subsection{C}

We already have a good idea of what the C code should look like,
and our ICurry structure fits closely with this.
This major difference is that we need to be sure to declare and allocate memory for all variables,
which leads to a split in the structure of the generated code.
The code responsible for creating expression graphs and declaring memory will
go in the *.h file, and the code for executing the hnf function will go in the *.c file.
This is a common pattern for structuring C and C++ code,
so it's not surprising that we take the same approach.

For each Data type |D|, we generate both a \texttt{make\_D} function and a \texttt{set\_D}.
The difference is that \texttt{make\_D} will allocate memory for a new node, while \texttt{set\_D}
takes an existing node as a parameter, and transforms it to the given type of node.
We do the same thing for every ICurry function |f|,
and produce a \texttt{make\_f} and \texttt{set\_f} function in C.
Each node contains a \texttt{symbol}, that denotes the type of node, and holds information
such as the name, arity, and hnf function of the node.
Along with setting the \texttt{symbol} from section \ref{Generated Code},
the make and set functions reset the nondet flag to \texttt{false},
and set any children that were passed into the node.

As mentioned in the last chapter, for every function |f|, 
we need to generate a function for every case expression in |f|.
|path f p| where
|p| is a path to a case statement.
The translation to C code is where we finally generate these functions.
Surprisingly, this doesn't have too much of an effect on the code generator.
Instead of generating code for a function |f|,
we collect a list of pairs |(f,p)|, where |p| is a path to a case statement,
then we generate code for each |(f,p)| pair.
In practice, it is useful to keep track of the declared variables,
so we actually generate code for a triple |(f,vars,p)|.
While generating code for |(f,p)|,
we track the current position |p'| of the code we're generating.
When we need to generate code to push |f| onto the stack,
such as when the code evaluates a non-deterministic expression,
then we push \texttt{f\_p'} onto the stack.

Aside from that change, generating the C code proceeds exactly as you would expect.
Declarations are turned into C declarations;
assignments are turned into calls to make functions;
return statements are turned into calls to set functions;
and case statements are turned into the while/switch construct from the last chapter.

At this point we can now produce a running program.
In fact, the code produced by this compiler is already relatively efficient,
As we will see later on,
in many cases it outperforms the current state-of-the-art Curry compilers.

In this chapter, we've built up the GAS tool for performing transformations
on FlatCurry code in a simple declarative way.
We've already seen how this transformation tool eases the process of compiling programs,
but it turns out to be much more powerful.
As we'll see in the next chapter,
when we apply GAS to optimizations, we can produce some complex optimizations,
such as inlining, with rather minimal effort.
This allows us to implement new optimizations faster, and test them more thoroughly,
than if we had written each optimization by hand.
In fact, it's often faster to implement an optimization, and just see what it does to the code,
than to try to reason it out ourselves.
This lets us iterate quickly on designing new optimizations,
and produce more complex and powerful optimizations.
Now we're cooking with GAS!





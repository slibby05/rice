
When cooking, it is very important to follow the rules.
You do not need to stick to an exact recipe, 
but you do need to know the how ingredients will react to temperature
and how different combinations will taste.
Otherwise you might get some unexpected reactions.

Similarly, there is not a single way to compile Curry programs,
however we do need to know the rules of the game.
Throughout this compiler, we will be transforming Curry programs
in many different ways, and it is important to make sure that all
of these transformations respect the rules of Curry.
As we will see, if we break these rules, 
then we may get some unexpected results.

We introduce the concept of Rewriting, 
along with the more specific Term and Graph Rewriting.
We give a basic intuition about how to apply these topics,
and show several examples using a small, but not trivial,
example of a rewrite system for Peano Arithmetic \ref{fig:peano}.

\section{Rewriting}
In programming language terms, the rules of Curry are its semantics.
The semantics of Curry are generally given in terms of rewriting.
\cite{IntegrationFunLog, FunLog, Needed}
While there are other semantics \cite{currySemantics, crwl, monadSemantics}, 
rewriting is a common formalism for many functional languages,
and the general theory of Curry grew out of this discipline \cite{Needed},
a good fit for Curry \cite{CurryReport}.
We will give a definition of rewrite systems,
then we will look at two distinct types of rewrite systems:
Term Rewrite Systems, which are used to implement transformations and optimizations
on the Curry syntax trees;
and Graph Rewrite Systems, which define the operational semantics for Curry programs.
This mathematical foundation will help us justify the correctness of our transformations
even in the presence of laziness, non-determinism, and free variables.

An Abstract Rewriting System (ARS)\index{Abstract Rewriting System}
is a set $A$ along with a relation $\to$\index{$\to$}.
We use $a \to b$ as a shorthand for $(a,b) \in \to$, and we have several modifiers on our relation.
\begin{itemize}
    \item $a \to^n b$\index{$\to^n$} iff $a = x_0 \to x_1 \to \ldots x_n = b$.
    \item $a \to^{\le n} b$\index{$\to^{\le n}$} b iff $a \to^i b$ and $i \leq n$.
    \item reflexive closure: $a \to^= b$\index{$\to^=$} iff $a = b$ or $a \to b$.
    \item symmetric closure: $a \leftrightarrow b$\index{$\leftrightarrow$} iff $a \to b$ or $b \to a$.
    \item transitive closure: $a \to^+ b$\index{$\to^+$} iff $\exists n\in \N. a \to^{\le n} b$.
    \item reflexive transitive closure: $a \to^* b$\index{$\to^*b$} iff $a \to^= b$ or $a \to^+ b$.
    \item rewrite derivation\index{rewrite derivation}: 
          a sequence of rewrite steps $a_0 \to a_1 \to \ldots a_n$.
    \item $a$ is in Normal Form (NF)\index{normal form} if no rewrite rules can apply.
\end{itemize}

A rewrite system is meant to invoke the feeling of algebra.
In fact, rewrite system are much more general, but they can still retain the feeling.
If we have an expression $(x\cdot x + 1)(2 + x)$, we might reduce this with the reduction in figure \ref{fig:reduce}.

\begin{boxfigure}
\begin{tabular}{rll}
          & $(x\cdot x + 1)(2 + x)$                          & \\
    $\to$ & $(x\cdot x + 1)(x + 2)$                          & by commutativity of addition \\
    $\to$ & $(x^2 + 1)(x + 2)$                               & by definition of $x^2$\\
    $\to$ & $x^2\cdot x + 2\cdot x^2 + 1\cdot x + 1 \cdot 2$ & by FOIL\\
    $\to$ & $x^2\cdot x + 2x^2 + x + 2$                      & by identity of multiplication\\ 
    $\to$ & $x^3 + 2x^2 + x + 2$                             & by definition of $x^3$\\
\end{tabular}\\
    \caption{reducing $(x\cdot x + 1)(2 + x)$ using the standard rules of algebra}
    \label{fig:reduce}
\end{boxfigure}

We can conclude that $(x\cdot x + 1)(x + 2) \to^+ x^3 + 2x^2 + x + 2$.
This idea of rewriting invokes the feel of algebraic rules.
The mechanical process of rewriting allows for a simple implementation on a computer.

It is worth understanding the properties and limitations of these rewrite systems.
Traditionally there are two important questions to answer about any rewrite system.
Is it \emph{confluent}? Is it \emph{terminating}?

A \emph{confluent}\index{confluent} system is a system 
where the order of the rewrites does not change the final result.
For example, consider the distributive rule.
When evaluating $3\cdot(4 + 5)$ we could either evaluate the addition or multiplication first.
Both of these reductions arrived at the same answer as can be seen in figure \ref{fig:confluent}.

\begin{boxfigure}
  \begin{subfigure}{.5\textwidth}
      \centering
    \fbox{
      \begin{tabular}{rl}
              & $3\cdot(4 + 5)$       \\
        $\to$ & $3\cdot 4 + 3\cdot 5$ \\
        $\to$ & $12 + 15$             \\
        $\to$ & $27$                  \\
      \end{tabular}
    }
      \caption{distributing first}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
      \centering
    \fbox{
      \begin{tabular}{rl}
              & $3\cdot(4 + 5)$       \\
        $\to$ & $3\cdot 9$            \\
        $\to$ & $27$                  \\
      \end{tabular}
    }
      \caption{reducing $4 + 5$ first}
  \end{subfigure}
    \caption{Two possible reductions of $3\cdot(4 + 5)$.  
             Since this is a confluent system, they both can rewrite to 27.}
    \label{fig:confluent}
\end{boxfigure}

In a \emph{terminating}\index{terminating} system every derivation is finite.
That means that eventually there are no rules that can be applied.
The distributive rule is terminating, 
whereas the commutative rule is not terminating.  See figure \ref{fig:terminate}.

\begin{boxfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \fbox{
      \begin{tabular}{rl}
              & $a\cdot (b + c)$\\
        $\to$ & $a\cdot b + a\cdot c$ \\
      \end{tabular}
    }
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \centering
    \fbox{
      \begin{tabular}{rl}
              & $x + y$\\
        $\to$ & $y + x$\\
        $\to$ & $x + y$\\
        $\ldots$      & \\
      \end{tabular}
    }
  \end{subfigure}
    \caption{A system with a single rule for distribution is terminating,
             but any system with a commutative rule is not.
             Note that $x + y \to^2 x + y$}
    \label{fig:terminate}
\end{boxfigure}

Confluence and termination are important topics in rewriting, but we will largely ignore them.
After all, Curry programs are neither confluent nor terminating.
However, there will be a few cases where these concepts will be important.
For example, if our optimizer is not terminating, then we will never actually compile a program.

Now that we have a general notation for rewriting, we can introduce two important rewriting frameworks:
term rewriting and graph rewriting, where we are transforming trees and graphs respectively.

\section{Term Rewriting}

As mentioned previously, one application of term rewriting
is to transform terms representing syntax trees.
This will be useful in optimizing the Abstract Syntax Trees (ASTs) of Curry programs.
Term rewriting is a special case of abstract rewriting.
Therefore everything from abstract rewriting will apply to term rewriting.

A term is made up of signatures and variables. \cite{AdvancedTRS}[Def 3.1.2]
We let $\Sigma$ and $V$ be two arbitrary alphabets, 
but we require that $V$ be countably infinite, and $\Sigma \cap V = \emptyset$ to avoid name conflicts.
A \emph{signature}\index{signature} $f^{(n)}$ consists of a name $f \in \Sigma$ and an arity $n\in \mathbb{N}$.
A \emph{variable}\index{variable} $v\in V$ is just a name.
Finally a \emph{term}\index{term} is defined inductively.
The term $t$ is either a variable $v$, or it is a signature $f^{(n)}$ with children $t_1,t_2, \ldots t_n$,
where $t_1,t_2, \ldots t_n$ are all terms.
We write the set of terms all as $T(\Sigma,V)$\index{$T(\Sigma,V)$}.
If $t \in T(\Sigma,V)$ then we write $Var(t)$ to denote the set of variables in $t$.
By definition $Var(t) \subseteq V$.
We say that a term is \emph{linear}\index{linear} if no variable appears twice in the term \cite{AdvancedTRS}[Def. 3.2.4].

This inductive definition gives us a tree structure for terms.
As an example consider Peano arithmetic $\Sigma = \{+^2, *^2, -^2, <^2, 0^0, S^1, True^0, False^0\}$.
We can define the term $*(+(0, S(0)), +(S(0), 0))$.
This gives us the tree in figure \ref{fig:tree}.
Every term can be converted into a tree like this and vice versa.
The symbol at the top of the tree is called the root of the term.


\begin{boxfigure}
    \Tree[.$*$ [.$+$ $0$ [.$S$ $0$ ] ] [.$+$ [.$S$ $0$ ] $0$ ] ]\\
    \caption{Tree representation of the term $*(+(0, S(0)), +(S(0), 0))$.}
    \label{fig:tree}
\end{boxfigure}

A \emph{child}\index{child} $c$ of term $f(t_1, t_2, \ldots t_n)$ is one of $t_1, t_2, \ldots t_n$.
A \emph{subterm}\index{subterm} $s$ of $t$ is either $t$ itself, or it is a subterm of a child of $t$.
We write $s = t||_p$\index{$t||_p$} where $p = [i_1,i_2,\ldots i_n]$ to denote that 
$t$ has child $t_{i_1}$ which has child $t_{i_2}$ and so on until $t_{i_n} = s$.
Note that we can define this recursively as
$t\vert_{[i_1,i_2,\ldots i_n]} = t_{i_1}\vert_{[i_2,\ldots i_n]}$, which matches our definition for subterm.
We call $[i_1,i_2,\ldots i_n]$ the \emph{path}\index{path} from $t$ to $s$ \cite{AdvancedTRS}[Def 3.1.5].
We write $\epsilon$ for the empty path,
and $i:p$ for the path starting with the number $i$ and followed by the path $p$,
and $p\cdot q$ for concatenation of paths $p$ and $q$.

In our previous term $S(0)$ is a subterm in two different places.
One occurrence is at path $[0,1]$, and the other is at path $[1,0]$.

We write $t[p \to r]$\index{$t[p \to r]$} to denote replacing subterm $t||_p$ with $r$.
We define the algorithm for this in figure \ref{fig:subterm}.

\begin{boxfigure}
    $t[\epsilon \to r] = r$\\
    $f(t_1,\ldots t_i,\ldots t_n)[i:p \to r] = f(t_1,\ldots t_i[p\to r],\ldots t_n) $\\
    \caption{algorithm for finding a subterm of $t$.}
    \label{fig:subterm}
\end{boxfigure}

In our above example $t =*(+(0, S(0), +(S(0), 0)))$,
We can compute the rewrite  $t[[0,1] \to *(S(0),S(0))]$, and we get the term
$*(+(0,*(S(0),S(0))), +(S(0), 0))$, with the tree in figure \ref{fig:subtree}.

\begin{boxfigure}
    \begin{center}
    \begin{tabular}{>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}} 
        \Tree[.$*$ [.$+$ $0$ [.$S$ $0$ ] ] [.$+$ [.$S$ $0$ ] $0$ ] ] &
        {\huge $\Rightarrow$} &
        \Tree[.$*$ [.$+$ $0$ [.$*$ [.$S$ $0$ ] [.$S$ $0$ ] ] ] [.$+$ [.$S$ $0$ ] $0$ ] ] \\
    \end{tabular}
    \end{center}
    \caption{The result of the computation $t[[0,1] \to S(0)]$}
    \label{fig:subtree}
\end{boxfigure}

A substitution replaces variables with terms.
Formally, a \emph{substitution}\index{substitution} is a mapping from 
$\sigma : V \to T(\Sigma,V)$\index{$\sigma$},
such that $\sigma(x) \ne x$ \cite{AdvancedTRS}[Def. 3.1.7].
We write $\sigma = \{v_1 \mapsto t_1, \ldots v_n \mapsto t_n\}$ to denote the substitution
where $s(v_i) = t_i$ for $i \in \{1\ldots n\}$, and $s(v) = v$ otherwise.
We can uniquely extend $\sigma$ to a function on terms by figure \ref{fig:substitute}

\begin{boxfigure}
    $\sigma'(v) = \sigma(v)$\\
    $\sigma'(f(t_1,\ldots t_n) = f(\sigma'(t_1) \ldots \sigma'(t_n))$\\
    \caption{Algorithm for applying a substitution.}
    \label{fig:substitute}
\end{boxfigure}

Since this extension is unique, we will just write $\sigma$ instead of $\sigma'$.
Term $t_1$ \emph{matches} term $t_2$
if there exists some substitution $\sigma$ such that $t_1 = \sigma(t_2)$ \cite{AdvancedTRS}[3.1.8],.
We call $\sigma$ a \emph{matcher}\index{matcher}.
Two terms $t_1$ and $t_2$ \emph{unify}
if there exists some substitution $\sigma$ such that $\sigma(t_1) = \sigma(t_2)$ \cite{AdvancedTRS}[3.1.8],.
In this case $\sigma$ is called a \emph{unifier}\index{unifier} for $t_1$ and $t_2$.

We can order substitutions based on what variables they define.
A substitution $\sigma \leq \tau$, iff,
there is some substitution $\nu$ such that $\tau = \nu \circ \sigma$.
The relation $\sigma \leq \tau$ should be read as $\sigma$ is more general than $\tau$,
and it is a quasi-order on the set of substitutions.
A unifier $u$ for two terms is \emph{most general} (or an mgu), iff, for all unifiers $v$, $v \le u$.
Mgus are unique up to renaming of variables.
That is, if $u_1$ and $u_2$ are mgus for two terms, then $u_1 = \sigma_1 \circ u_2$
and $u_2 = \sigma_2 \circ u_1$.
This can only happen if $\sigma_1$ and $\sigma_2$ just rename the variables in their terms.

As an example $+(x,y)$ matches $+(0, S(0))$ with $\sigma = \{x \mapsto 0, y \mapsto S(0)\}$.
The term $+(x, S(0))$ unifies with term $+(0, y)$ with unifier
$\sigma = \{x \mapsto 0, y \mapsto S(0)\}$.
If $\tau = \{x \mapsto 0, y \mapsto S(z)\}$, then $\tau \le \sigma$.  We can define $\nu = \{z \mapsto 0\}$,
and $\{\sigma = \nu \circ \tau\}$

Now that we have a definition for a term, we need to be able to rewrite it.
A \emph{rewrite rule}\index{rewrite rule} $l \to r$ is a pair of terms.
However this time we require that $Var(r) \subseteq Var(l)$, and that $l \not \in V$.
A \emph{Term Rewriting System (TRS)}\index{Term Rewriting System (TRS)}
is the pair $(T(\Sigma,V),R)$ where $R$ is a set of rewrite rules.


\theoremstyle{definition}
\begin{definition}{Rewriting\index{Rewriting}:}
Given terms $t,s$, path $p$, and rule $l \to r$, we say that $t$ rewrites to $s$ if, 
$l$ matches $t\vert_p$ with matcher $\sigma$, and $t[p \to \sigma(r)] = s$.
The term $\sigma(l)$ is the \emph{redex}\index{redex}, and the term $\sigma(r)$ is the \emph{contractum}\index{contractum}
of the rewrite.
\end{definition}

There are a few important properties of rewrite rules $l \to r$.
A rule is left or right linear if $l$ or $r$ is linear respectively
\cite{AdvancedTRS}[Def. 3.2.4].
A rule is \emph{collapsing}\index{collapsing} if $r \in V$.
A rule is \emph{duplicating}\index{duplicating} if there is an $x \in V$ that occurs more often in $r$ than in $l$
\cite{AdvancedTRS}[Def. 3.2.5].

Two terms $s$ and $t$ are \emph{overlapping}\index{overlapping} if $t$ unifies with a subterm of $s$,
or $s$ unifies with a subterm of $t$ at a non-variable position \cite{AdvancedTRS}[Def. 4.3.3].
Two rules $l_1 \to r_1$ and $l_2 \to r_2$ if $l_1$ and $l_2$ overlap.
A rewrite system is \emph{overlapping} if, and only if, any two rules overlap.
Otherwise it is non-overlapping.
Any non-overlapping left linear system is \emph{orthogonal}\index{orthogonal} \cite{AdvancedTRS}[Def.4.3.4].
Orthogonal systems have several nice properties, such as the following theorem \cite{AdvancedTRS}[Thm. 4.3.11].

\begin{theorem}
Every orthogonal TRS is confluent.
\end{theorem}

As an example, in figure \ref{fig:overlap} examples (b) and (c) both overlap.
It is clear that these systems are not confluent,
but non-confluence can arise in more subtle ways.
The converse to theorem 2.1 is not true. There can be overlapping systems which are confluent.

\begin{boxfigure}
    \begin{subfigure}{.26\textwidth}
    \fbox{
    \begin{tabular}{c}
    $g(0,y) \to 0$\\
    $g(1,y) \to 1$\\
    \end{tabular}
    }
    \caption{A non-overlapping system}
    \end{subfigure}
    \hspace{.06\textwidth}
    \begin{subfigure}{.26\textwidth}
    \fbox{
    \begin{tabular}{c}
    $g(0,y) \to 0$\\
    $g(x,1) \to 1$\\
    \end{tabular}
    }
    \caption{A system that overlaps at the root}
    \end{subfigure}
    \hspace{.06\textwidth}
    \begin{subfigure}{.26\textwidth}
    \fbox{
    \begin{tabular}{c}
    $f(g(x,y)) \to 0$\\
    $g(x,y)    \to 1$\\
    \end{tabular}
    }
    \caption{A system that overlaps at a subterm}
    \end{subfigure}
    \caption{Three TRSs demonstrating how rules can overlap.
            In (a) they do not overlap at all,
            In (b) both rules overlap at the root,
            and in (c) rule 2 overlaps with a subterm of rule 1.}
    \label{fig:overlap}
\end{boxfigure}

When defining rewrite systems we usually follow the constructor discipline;
we separate the set $\Sigma = C \uplus F$.
$C$ is the set of \textit{constructors},
and $F$ is the set of \textit{function symbols}.
Furthermore, for every rule $l \to r$, the root of $l$ is a function symbol, 
and every other symbol is a constructor or variable.
We call such systems \textit{constructor systems}.
As an example, the rewrite system for Peano arithmetic is a constructor system.

\begin{boxfigure}
    \begin{tabular}{lclcl}
        $R_1$ &    : & $0    + y$      & $\to$ & $y$       \\
        $R_2$ &    : & $S(x) + y$      & $\to$ & $S(x+y)$  \\
        $R_3$ &    : & $0    * y$      & $\to$ & $0$       \\
        $R_4$ &    : & $S(x) * y$      & $\to$ & $y+(x*y)$ \\
        $R_5$ &    : & $0    - y$      & $\to$ & $0$       \\
        $R_6$ &    : & $S(x) - 0$      & $\to$ & $S(x)$    \\
        $R_7$ &    : & $S(x) - S(y)$   & $\to$ & $x - y$   \\
        $R_8$ &    : & $0    \le y$    & $\to$ & $True$    \\
        $R_9$ &    : & $S(x) \le 0$    & $\to$ & $False$    \\
        $R_{10}$ & : & $S(x) \le S(y)$ & $\to$ & $x < y$   \\
        $R_{11}$ & : & $0    = 0   $   & $\to$ & $True$    \\
        $R_{12}$ & : & $S(x) = 0   $   & $\to$ & $False$    \\
        $R_{13}$ & : & $0    = S(y)$   & $\to$ & $False$    \\
        $R_{14}$ & : & $S(x) = S(y)$   & $\to$ & $x = y$   \\
    \end{tabular}
    \caption{The rewrite rules for Peano Arithmetic with addition, multiplicaton,
             subtraction, and comparison.  All operators use infix notation.}
    \label{fig:peano}
\end{boxfigure}

The two sets are $C = \{0, S, True, False\}$ and $F = \{+,*,-,\le\}$,
and the root of the left hand side of each rule is a function symbol.
In contrast, the SKI system is not a constructor system.
While $S,K,I$ can all be constructors, the $Ap$ symbol appears in both root
and non-root positions of the left hand side of rules.
This example will become important for us in Curry.
We will do something similar to implement higher order functions.
This means that Curry programs will not directly follow the constructor discipline.
Therefore, we must be careful when specifying the semantics of function application.

\begin{boxfigure}
    \begin{tabular}{lcl}
        $Ap(I,x)$             & $\to$ & $x$\\
        $Ap(Ap(K,x),y)$       & $\to$ & $x$\\
        $Ap(Ap(Ap(S,x),y),z)$ & $\to$ & $Ap(Ap(x,z),Ap(y,z))x$\\
    \end{tabular}
    \caption{The SKI system from combinatorial logic.}
    \label{fig:SKI}
\end{boxfigure}


Constructor systems have several nice properties.
They are usually easy to analyze for confluence and termination.
For example, if the left hand side of two rules do not unify, then they cannot overlap.
We do not need to check if subterms overlap.
Furthermore, any term that consists entirely of constructors and variables is in normal form.
For this reason, it is not surprising that most functional languages are based on constructor systems.

%Finally, we can introduce conditions to rewriting systems.
%We introduce a new symbol $True$ to the rewrite system's alphabet $\Sigma$.
%A conditional rewrite rule is a rule $l \vert c \to r$ where $l,c,$ and $r$ are terms.
%A term $t$ conditionally rewrites to $s$ with rule $l \vert c \to r$ if
%there exists a path $p$ and substitution $\sigma$ such that 
%$t_p = \sigma(l)$, $\sigma(c) \to^* True$, and $s = \sigma(r)$.
%
%The idea is actually a pretty simple extension.
%In order to rewrite a term, we must satisfy a condition.
%If the condition is true, then the rule is applied.
%In order to simplify the semantics of this system,
%we determine if a condition is true by rewriting it to the value $True$.
%Figure \ref{fig:gcd} gives an example of a conditional rewrite system for computing
%greatest common divisor.  It uses the rule defined in \ref{fig:peano}.
%
%\begin{boxfigure}
%    \begin{tabular}{lllcl}
%        $gcd(x,x)$ &         &           & $\to$ & $x$ \\
%        $gcd(x,y)$ & $\vert$ & $y \le x$ & $\to$ & $gcd(x-y,y)$ \\
%        $gcd(x,y)$ & $\vert$ & $x \le y$ & $\to$ & $gcd(x,y-x)$ \\
%    \end{tabular}
%    \caption{Conditional rewrite system for computing greatest common divisor.}
%    \label{fig:gcd}
%\end{boxfigure}
%
%While most treatments of conditional rewriting \cite{IntegrationFunLog,condKaplan}
%define a condition as a pair $s = t$ where $s$ and $t$ mutually rewrite to each other,
%We chose this definition because it is closer to the definition of Curry,
%where the condition must reduce to the boolean value \texttt{True} for the rule to apply.
%
%Curry uses conditional rewriting extensively,
%and efficiently evaluating conditional rewrite systems 
%is the core problem in most functional logic languages.
%The solution to this problem comes from the theory of narrowing.


\section{Narrowing}

Narrowing was originally developed to solve the problem of semantic unification.
The goal was, given a set of equations $E = \{a_1 = b_2, a_2 = b_2, \ldots a_n = b_n\}$,
to solve the equation $t_1 = t_2$ for arbitrary terms $t_1$ and $t_2$.
Here a solution to $t_1 = t_2$ is a substitution $\sigma$ such that $\sigma(t_1)$
can be transformed into $\sigma(t_2)$ by the equations in $E$.

As an example let $E = \{*(x +(y, z)) = +(*(x,y), *(x,z))\}$
Then the equation $*(1,+(x,3)) = +(+(*(1,4), *(y,5)), *(z,3))$
is solved by $\sigma = \{x \mapsto +(4,5), y \mapsto 1, z \mapsto 1\}$.
The derivation is in figure \ref{fig:narrow}.

\begin{boxfigure}
\begin{tabular}{ll}
    $\sigma(*(1,+(x,3)))$                & = \\
    $*(1,+(+(4,5),3))$                   & = \\
    $+(*(1,+(4,5)),*(1,3))$              & = \\
    $+(+(*(1,4),*(1,5)),*(1,3))$         & = \\
    $\sigma(+(+(*(1,4),*(y,5)),*(z,3)))$ &
\end{tabular}
    \caption{Derivation of $*(1,+(x,3)) = +(+(*(1,4), *(y,5)), *(z,3))$ with
             $\sigma = \{x \mapsto +(4,5), y \mapsto 1, z \mapsto 1\}$.}
    \label{fig:narrow}
\end{boxfigure}

Unsurprisingly, there is a lot of overlap with rewriting.
One of the earlier solutions to this problem was to convert 
the equations into a confluent, terminating rewrite system. \cite{KnuthBendix}
Unfortunately, this only works for ground terms, that is, terms without variables.
However, this idea still has merit.
So we want to extend it to terms with variables.

Before, when we rewrote a term $t$ with rule $l \to r$, we assumed it was a ground term,
then we could find a substitution $\sigma$ that would match a subterm $t\vert_p$ with $l$,
so that $\sigma(l) = t\vert_p$.
To extend this idea to terms with variables in them, 
we look for a unifier $\sigma$ that unifies $t\vert_p$ with $l$.
This is really the only change we need to make \cite{AdvancedTRS}.
However, now we record $\sigma$, because it is part of our solution.

\theoremstyle{definition}
\begin{definition}{Narrowing\index{Narrowing}:}
Given terms $t,s$, path $p$, and rule $l \to r$, we say that $t$ narrows to $s$ if, 
$l$ unifies with $t\vert_p$ with unifier $\sigma$, and $t[p \to \sigma(r)] = s$.
We write $t \rightsquigarrow_{p,l\to r,\sigma} s$.
We may write $t \rightsquigarrow_\sigma s$ if $p$ and $l \to r$ are clear.
\end{definition}

Notice that this is almost identical to the definition of rewriting.
The only difference is that $\sigma$ is a unifier instead of a matcher.

Narrowing was first developed to solve equations for automated theorem provers \cite{narrowing}.
However, for our purposes it is more important that narrowing allows us to
rewrite terms with free variables. \cite{multiparadigm}

At this point, rewrite systems are a nice curiosity,
but they are completely impractical. 
This is because we do not have a plan for solving equations in them.
In the definition for both rewriting and narrowing,
we did not specify how to find $\sigma$ the correct rule to apply, or even
what subterm to apply the rule.

In confluent terminating rewrite systems, we could simply try every possible rule
at every possible position with every possible substitution.
Since the system is confluent, we could choose the first rule that could be successfully applied,
and since the system is terminating, we would be sure to find a normal form.
In a narrowing system, this is still not guaranteed to halt, because there could be
an infinite number of substitutions.
This is the best possible case for rewrite systems, 
and we still cannot ensure that our algorithm will finish.
We need a systematic method for deciding what rule should be applied,
what subterm to apply it to,
and what substitution to use.
This is the role of a strategy.

\section{Rewriting Strategies}

Our goal with a rewriting strategy is to be able to find a normal form for any term.
Similarly our goal for narrowing will be to find a normal form and substitution.
However, we want to be efficient when rewriting.
We would like to use only local information when deciding what rule to select.
We would also like to avoid unnecessary rewrites.
Consider the following term from the SKI system defined in figure \ref{fig:SKI}
$Ap(Ap(K, I), Ap(Ap(S,Ap(I,I)),Ap(S,Ap(I,I))))$.
It would be pointless to reduce $Ap(Ap(S,Ap(I,I)),Ap(S,Ap(I,I))))$ since $Ap(Ap(K,I,z)$ rewrites to $I$
no matter what $z$ is.
In this particular case, since $Ap(Ap(S,Ap(I,I)),Ap(S,Ap(I,I))))$ reduces to itself,
we have turned a potentially non-terminating reduction to a terminating one.

A \emph{Rewriting Strategy}\index{Rewrite Strategy} $\mathcal{S} : T(\Sigma, V) \to Pos\times R$ 
is a function that takes a term, and returns a position to rewrite, and a rule to rewrite with
\cite{termRewriting}.
Furthermore we require that if $(p,l \to r) = \mathcal{S}(t)$, then $t\vert_p$ is a redex that matches $l$.
The idea is that $S(t)$ should give us a position to rewrite, and the rule to rewrite with.

For orthogonal rewriting systems, 
there are two common rewriting strategies that do not run in 
parallel,\footnote{we avoid discussing parallel strategies,
                   because our work is focused on sequential execution of Curry programs.
                   That has been a lot of work done on parallel execution of Curry programs 
                   elsewhere \cite{concurrentCurry,abstractConcurrentCurry}.}
innermost and outermost rewriting\cite{termRewriting,rewriteStrategies}.
Innermost rewriting corresponds to eager evaluation in functional programming.
We rewrite the term that matches a rule that is the furthest down the tree.
Outermost rewriting correspond roughly to lazy evaluation.
We rewrite the highest possible term that matches a rewrite rule.

A strategy is \emph{normalizing}\index{normalizing} if, when a term $t$ has a normal form, 
then the strategy will eventually find it.
While outermost rewriting is not normalizing in general,
it is for left-normal systems, 
which is a large subclass of orthogonal rewrite systems \cite{termRewriting}.
This matches the intuition from programming languages.
Lazy languages can perform computations that would run forever with an eager language.

While both of these strategies are well understood, we can actually make a stronger guarantee.
We want to reduce only the redexes that are necessary to find a normal form.
To formalize this we need to understand what can happen when we rewrite a term.
Specifically for a redex $s$ that is a subterm of $t$, how can $s$ change as we rewrite $t$.
If we were rewriting at position $p$ with rule $l \to r$, then there are 3 cases to consider.\\
Case 1: we are rewriting $s$ itself.  That is, $s$ is the subterm $t\vert_p$.
Then $s$ disappears entirely.\\
Case 2: $s$ is either above $t\vert_p$, or they are completely disjoint.
In this case $s$ does not change.\\
Case 3: $s$ is a subterm of $t\vert_p$.
In this case $s$ may be duplicated, or erased, moved, or left unchanged.
It depends on whether the rule is duplicating, erasing, or right linear.\\
These cases can be seen in figure \ref{fig:descendant}
We can formalize this with the notion of descendants with the following definition from 
\cite{AdvancedTRS}[Def. 4.3.6].


\begin{boxfigure}
  \begin{subfigure}{.6\textwidth}
    \begin{tabular}{>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}} 
        \Tree[.$S$ [.\fbox{$+$} [.$S$ $0$ ] [.$+$ [.$S$ $0$ ] $0$ ] ] ] &
        {\huge $\Rightarrow$} &
        \Tree[.$S$ [.\fbox{$+$} [.$S$ $0$ ] [.$S$ [.$+$ $0$ $0$ ] ] ] ] \\
    \end{tabular}
    \caption{rewrite $R_1$ at position $[0,1]$ does not affect $t\vert_{[0]}$.}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \begin{tabular}{>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1.5cm}} 
        \Tree[.$*$ [.\fbox{.$+$} $0$ $0$ ] [.$*$ $0$ $0$ ] ] &
        {\huge $\Rightarrow$} &
        \Tree[.$*$ [.\fbox{.$+$} $0$ $0$ ] $0$ ] \\
    \end{tabular}
    \caption{rewrite $R_4$ at position $[1]$ does not affect $t\vert_{[0]}$.}
  \end{subfigure}
  \begin{subfigure}{.6\textwidth}
    \begin{tabular}{>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{2cm}} 
        \Tree[.$S$ [.$*$ [.$S$ $0$ ] [.\fbox{$+$} $0$ $0$ ] ] ] &
        {\huge $\Rightarrow$} &
        \Tree[.$S$ [.$+$ [.\fbox{$+$} $0$ $0$ ] [.$*$ $0$ [.\fbox{$+$} $0$ $0$ ] ] ] ] \\
    \end{tabular}
    \caption{rewrite $R_3$ at position $[0]$ duplicates $t\vert_{[0,1]}$.}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \begin{tabular}{>{\centering\arraybackslash}m{1.5cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}} 
        \Tree[.$S$ [.$-$ $0$ [.\fbox{$+$} $0$ $0$ ] ] ] &
        {\huge $\Rightarrow$} &
        \Tree[.$S$ $0$ ] \\
    \end{tabular}
    \caption{rewrite $R_5$ at position $[0]$ erases term at $t\vert_{[0,1]}$.}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
    \begin{tabular}{>{\centering\arraybackslash}m{2cm}>{\centering\arraybackslash}m{1cm}>{\centering\arraybackslash}m{1cm}} 
        \Tree[.$S$ [.$+$ [$S$ $0$ ] .\fbox{$0$} ] ] &
        {\huge $\Rightarrow$} &
        \Tree[.$S$ [.$S$ [.$+$ $0$ .\fbox{$0$} ] ] ] \\
    \end{tabular}
    \caption{rewrite $R_2$ at position $[0]$ moves $t\vert_{[0,1]}$ to position $[0,0,1]$.}
  \end{subfigure}
    \caption{four cases for the descendants for a term after a single rewrite.
             The boxed term is either left alone, duplicated, or erased, or moved.
             The rules are defined in figure \ref{fig:peano}}
    \label{fig:descendant}
\end{boxfigure}


\theoremstyle{definition}
\begin{definition}{Descendant\index{descendant}:}
    Let $s = t\vert_v$, and $A = l \rightarrow_{p,\sigma,R} r$ be a rewrite step in $t$.
    The set of descendants of $s$ is given by $Des(s,A)$\\
    $$
    Des(s,A) = 
    \begin{cases}
        \emptyset & \text{if}\ v = u \\
        \{s\}     & \text{if}\ p \not \leq v \\
        \{t\vert_{u\cdot w\cdot q}\ :\ r\vert_w = x \}
                  & \text{if}\ p = u\cdot v \cdot q\ \text{and}\ t\vert_v = x\ \text{and}\ x\in V
    \end{cases}
    $$\\
    This definition extends to derivation $t \to_{A_1} t_1 \to_{A_2} t_2 \to_{A_2} \ldots \to_{A_n}, t_{n+1}$.
    $Des(s,A_1,A_2\ldots A_n) = \bigcup_{s' \in Des(s,A_1)} Des(s', A_2,\ldots A_n)$.
\end{definition}

The first part of the definition is formalizing the notion of descendant.
The second part is extending it to a rewrite derivation.
The extension is straightforward. Calculate the descendants for the first rewrite,
then for each descendant, calculate the descendants for the rest of the rewrites.
With the idea of a descendant, we can talk about what happens to a term in the future.
This is necessary to describing our rewriting strategy.
Now we can formally define what it means for a redex to be necessary for computing
a normal form.

\theoremstyle{definition}
\begin{definition}{Needed\index{needed}:}
    A redex $s$ that is a subterm of $t$ is \emph{needed} in $t$ if,
    for every derivation of $t$ to a normal form,
    a descendant of $s$ is the root of a rewrite.
\end{definition}

This definition is good because it is immediately clear that, if we were going
to rewrite a term to normal form, we need to rewrite all of the needed redexes.
In fact, we can guarantee more than that with the following theorem \cite{condKaplan}.

\begin{theorem}
    For an orthogonal TRS, any term that is not in normal form contains a needed redex.
    Furthermore, a rewrite strategy that rewrites only needed redexes is normalizing.
\end{theorem}

This is a very powerful result.
We can compute normal forms by rewriting needed redexes.
This is also, in some sense, the best possible strategy.
Every needed redex needs to be rewritten.
Now we just need to make sure our strategy only rewrites needed redexes.
There is only one problem with this plan.
Determining if a redex is needed is undecidable in general.
However, with some restrictions, there are rewrite systems where this is possible
\cite{termRewriting}[def. 3.3.7].\footnote{The original definition used the notion of a
                                           context in normal form.}

\theoremstyle{definition}
\begin{definition}{Sequential\index{sequential}}
    A rewrite system is \emph{sequential} if, given a term $t$ with $n$ variables $v_1,v_2\ldots v_n$,
    such that $t$ is in normal form,
    then there is an $i$ such that for every substitution $\sigma$ from variables to redexes,
    $\sigma(v_i)$ is needed in $\sigma(t)$.
\end{definition}

If we have a sequential rewrite system,
then this leads to an efficient algorithm for reducing terms to normal form.
Unfortunately, sequential is also an undecidable property.
There is still hope.
As we will see in the next section,
with certain restrictions we can ensure the our rewrite systems are sequential.
Actually we can make a stronger guarantee.
The rewrite system will admit a narrowing strategy that only narrows needed redexes.

\section{Narrowing Strategies}

Similar to rewriting strategies, narrowing strategies attempt to compute a normal form
for a term using narrowing steps.
However, a narrowing strategy must also compute a substitution for that term.
There have been many narrowing strategies including basic \cite{basicNarrowing},
innermost \cite{slog}, outermost \cite{outerNarrowing},
standard \cite{standardNarrowing}, and lazy \cite{lazyNarrowing}.
Unfortunately, each of these strategies are too restrictive on the rewrite systems they allow.


\begin{boxfigure}
  \begin{subfigure}{.45\textwidth}
      $(x + x) + x = 0$
    \caption{This fails for eager narrowing, because evaluating $x + x$ can produce infinitely many answers.
             However This is fine for lazy narrowing. We will get
             $(0 + 0) + 0 = 0, \{x = 0\}$
             or $S(S(y + S(y)) + S(y)) = 0 \{x = S(y)\}$
             and the second one will fail.}
  \end{subfigure}
  \hspace{.05\textwidth}
  \begin{subfigure}{.45\textwidth}
      $x \le y + y$
    \caption{With a lazy narrowing strategy we may end up computing more than is necessary.
             If $x$ is instantiated to $0$, then we do not need to evaluate $y + y$ at all.}
  \end{subfigure}
    \caption{examples of where eager and lazy narrowing can fail using the rewrite system
             if figure \ref{fig:peano}.}
    \label{NarrowingComp}
\end{boxfigure}


Fortunately there exists a narrowing strategy that is defined on a large class of rewrite systems,
only narrows needed expressions, and is sound and complete.
However this strategy requires a new construct called a definitional tree.

The idea is that since we are working with constructor rewrite systems,
we can group all of the rules defined for the same function symbol together.
We will put them together in a tree structure defined below, and 
then we can compute a narrowing step by traversing the tree for the defined symbol.


\theoremstyle{definition}
\begin{definition}
    $T$ is a \emph{partial definitional tree} if $T$ is one of the following.\\
    $T = exempt(\pi)$ where $\pi$ is a pattern.\\
    $T = leaf(\pi \to r)$ where $\pi$ is a pattern, and $\pi \to r$ is a rewrite rule.\\
    $T = branch(\pi, o, T_1, \ldots T_k)$, where $\pi$ is a pattern,
    $o$ is a path,
    $\pi\vert_o$ is a variable,
    $c_1,\ldots c_k$ are constructors,
    and $T_i$ is a pdt with pattern $\pi[c_i(X_1,\ldots X_n)]_o$ where $n$ is the arity of $c_i$,
    and $X_1,\ldots X_n$ are fresh variables.\\
    $\ $\\
    Given a constructor rewrite system $R$,
    $T$ is a \emph{definitional tree}\index{definitional tree} for function symbol $f$ if
    $T$ is a partial definitional tree, and each leaf in $T$
    corresponds to exactly one rule rooted by $f$.
    A rewrite system is \emph{inductively sequential}\index{inductively sequential}
    if there exists a definitional tree for every function symbol.
\end{definition}

The name ``inductively sequential'' is justified because there 
is a narrowing strategy that only reduces needed redexes for any of these systems.
We show an example to clarify the definition.
In figure \ref{fig:defTree} we show the definitional tree for the $+, \le,$ and $=$ rules.
The idea is that, at each branch, we decide which variable to inspect.
Then we decide what child to follow based on the constructor of that branch.
This gives us a simple algorithm for outermost rewriting with definitional trees.
However, we need to extend this to narrowing.


\begin{boxfigure}
  \begin{subfigure}{.4\textwidth}
    \begin{tabular}{lcl}
        $0    + y$      & $\to$ & $y$       \\
        $S(x) + y$      & $\to$ & $S(x+y)$  \\
  \end{tabular}
  \end{subfigure}
  \begin{subfigure}{.6\textwidth}
  \Tree[.$x+y$ [.$0+y$ $y$ ]
               [.$S(x)+y$ $S(x+y)$ ] ]
  \end{subfigure} \\
  \begin{subfigure}{.4\textwidth}
  \begin{tabular}{lcl}
        $0    \le y$    & $\to$ & $True$    \\
        $S(x) \le 0$    & $\to$ & $False$    \\
        $S(x) \le S(y)$ & $\to$ & $x < y$   \\
  \end{tabular}
  \end{subfigure}
  \begin{subfigure}{.6\textwidth}
  \Tree[.$x\le y$ [.$0\le y$ $True$ ] 
                   [.$S(x)\le y$ [.$S(x)\le 0$ $False$ ]
                                 [.$S(x)\le S(y)$ $x\le y$ ] ] ] 
  \end{subfigure} \\
  \begin{subfigure}{.4\textwidth}
  \begin{tabular}{lcl}
        $0    = 0   $   & $\to$ & $True$  \\
        $S(x) = 0   $   & $\to$ & $False$ \\
        $0    = S(y)$   & $\to$ & $False$ \\
        $S(x) = S(y)$   & $\to$ & $x = y$ \\
  \end{tabular}
  \end{subfigure}
  \begin{subfigure}{.6\textwidth}
  \Tree[.$x=y$ [.$0=y$ [.$0=0$ $True$ ]
                       [.$0=S(y)$ $False$ ] ]
               [.$S(x)=y$ [.$S(x)=0$ $False$ ]
                          [.$S(x)=S(y)$ $x=y$ ] ] ]
  \end{subfigure} 
  \caption{Definitional trees for $ + $, $ \le $, and $ = $.}
  \label{fig:defTree}
\end{boxfigure}


In order to extend the strategy from rewriting to narrowing we need
to figure out how to compute a substitution,
and we need to define what it means for a narrowing step to be needed.
The earliest definition involved finding a most general unifier for the substitution.
This has some nice properties.
There is a well known algorithm for computing mgus, which are unique up to renaming of variables.
However, this turned out to be the wrong approach.
Computing mgus is too restrictive.
Consider the step $x \le y + z \rightsquigarrow_{2\cdot \epsilon,R_1,\{y \mapsto 0\}} x \le z$.  
Without further substitutions $x \le z$ is a normal form, and $\{y \mapsto 0\}$ is an mgu.
Therefore this should be a needed step.
But if we were to instead narrow $x$, 
we have $x \le y + z \rightsquigarrow_{\epsilon,R_8,\{x \mapsto 0\}} True$.
This step never needs to compute a substitution for $y$.
Therefore we need a definition that is not dependent on substitutions that might be computed later.


\theoremstyle{definition}
\begin{definition}
    A narrowing step $t \rightsquigarrow_{p, R, \sigma} s$ is needed, 
    iff, for every $\eta \ge \sigma$,
    there is a needed redex at $p$ in $\eta(t)$.
\end{definition}

Here we do not require that $\sigma$ be an mgu, but, for any less general substitution,
it must be the case that we were rewriting a needed redex.
So our example, $x \le y + z \rightsquigarrow_{2\dot \epsilon,R_1,\{y \mapsto 0\}} x \le z$,
is not a needed narrowing step because $x \le y + z \rightsquigarrow_{2\dot \epsilon,R_1,\{x \mapsto 0, y \mapsto 0\}} 0 \le z$,
Is not a needed rewriting step.

Unfortunately, this definition raises a new problem.
Since we are no longer using mgus for our unifiers, we may not have a unique step for an expression.
For example, $x < y \rightsquigarrow_{\epsilon, R_8, \{x\mapsto 0\}} True $, and
$x < y \rightsquigarrow_{\epsilon, R_9, \{x\rightarrow S(u), t \mapsto S(v)\}} u \le v $
are both possible needed narrowing steps.

Therefore we define a \emph{Narrowing Strategy}\index{narrowing strategy} 
$\mathcal{S}$ as a function from terms
to a set of triples of a position, rule, and substitution, such that if $(p, R, \sigma) \in \mathcal{S}(t)$,
then $\sigma(t)\vert_p$ is a redex for rule $R$.

At this point we have everything we need to define a needed narrowing strategy.

\theoremstyle{definition}
\begin{definition}
    Let $t$ be a term rooted by function symbol $f$,
    $T$ be the definitional tree for $f$,
    and ``$?$'' be a distinguished symbol to denote that no rule could be found.
    $$\lambda(t,T) \in  
    \begin{cases}
        (\epsilon, R, mgu(t, \pi))   & \text{if}\ T = rule(\pi, R) \\
        (\epsilon, ?, mgu(t, \pi))   & \text{if}\ T = exempt(\pi) \\
        (p, R, \sigma)               & \text{if}\ T = branch(\pi, o, T_1, \ldots T_n) \\
                                     & t\ \text{unifies with}\ T_i \\
                                     & (p, R, \sigma) \in \lambda(t, T_i) \\
        (o:p, R, \sigma \circ \tau)  & \text{if}\ T = branch(\pi, o, T_1, \ldots T_n) \\
                                     & t\ \text{does not unify with any}\ T_i \\
                                     & \tau = mgu(t, \pi) \\
                                     & T'\ \text{is the definitional tree for}\ t\vert_o \\
                                     & (p, R, \sigma) \in \lambda(t\vert_o, T') \\
    \end{cases}
    $$
\end{definition}

The function $\lambda$ is a narrowing strategy.
It takes an expression rooted by $f$, and the definition tree for $f$,
and it returns a position, rule and substitution for a narrowing step.
If we reach a rule node, then we can just rewrite;
if we reach an exempt node, then there is no possible rewrite;
if we reach a branch node, then we match a constructor;
but if the subterm we were looking at is not a constructor, then we need to narrow that subterm first.


\begin{theorem}
    $\lambda$ is a needed narrowing strategy.
    Furthermore, $\lambda$ is sound and complete.
\end{theorem}

It should be noted that while $\lambda$ is complete with respect to finding substitutions
and selecting rewrite rules \cite{Needed},
this says nothing about the underlying completeness of the rewrite system we were narrowing.
We may still have non-terminating derivations.

This needed narrowing strategy is important in developing the evaluation strategy for Curry programs.
In fact, one of the early stages of a Curry compiler is to construct definitional trees
for each function defined.
However, if we were to implement our compiler using terms, it would be needlessly inefficient.
We solve this problem with graph rewriting.

\section{Graph Rewriting}
As mentioned above term rewriting is too inefficient to implement Curry.
Consider the rule $double(x) = x + x$.
Term rewriting requires this rule to make a copy of $x$, no matter how large it is,
whereas we can share the variable if we use a graph.
In programming languages, this distinction moves the evaluation strategy from
``call by name'' to ``call by need'', and it is what we mean when we refer to ``lazy evaluation''.


As a brief review of relevant graph theory:
A \emph{graph}\index{graph} $G = (V,E)$ is a pair of vertices $V$ and edges $E \subseteq V \times V$.
We will only deal with directed graphs, so the order of the edge matters.
A \emph{rooted graph}\index{rooted graph} is a graph with a specific vertex $r$ designated as the \emph{root}\index{root}.
The \emph{neighborhood}\index{neighborhood} of $v$, written $N(v)$ is the set of vertices adjacent to $v$.
That is, $N(v) = \{u\ \vert\ (v,u) \in E\}$.
A \emph{path}\index{path} $p$ from vertex $u$ to vertex $v$ is a sequence 
$u = p_1, p_2 \ldots p_n = v$ where $(p_i,p_{i+1}) \in E$.
A rooted graph is \emph{connected}\index{connected} 
if there is a path from the root to every other vertex in the graph.
A graph is \emph{strongly connected}\index{strongly connected} if, 
for each pair of vertices $(u,v)$, there is a path from $u$ to $v$
and a path form $v$ to $u$.
A path $p$ is a cycle
\footnote{Some authors will use walk and tour and
          reserve path and cycle for the cases where there are no repeated vertices.
          This distinction is not relevant for our work.}
if its endpoints are the same.
A graph is acyclic if it contains no cycles.
Such graphs are referred to as Directed Acyclic Graphs, or DAGs.
A graph $H$ is a \emph{subgraph}\index{subgraph} of $G$, $H \subseteq G$ if, and only if, $V_H \subseteq V_G$
and $E_H \subseteq E_G$.
A strongly connected component $S$ of $G$ is a subgraph that is strongly connected.
We will use the well-known facts that strongly connected components partition a graph.
The component graph, which is obtained by shrinking the strongly connected components 
to a single vertex, is a DAG.
To avoid confusion with variables, we will refer to vertices of graphs as nodes.


We define term graphs in a similar way to terms.
Let $\Sigma = C \uplus F$ be an alphabet of constructor and function names respectively,
and $V$ be a countably infinite set of variables.
A \emph{term graph}\index{term graph} is a rooted graph $G$ with nodes in
$N$ where each node $n$ has a label in $\Sigma \cup V$.
We will write $L(n)$ to refer to the label of a node.
If $(n, s) \in E$ is an edge, then $s$ is a successor of $n$.
In most applications the order of the outgoing edges does not matter, 
however it is very important in term graphs.
So, we will refer to the first successor, second successor and so on.
We denote this the same way we did with terms $n_i$ is the $i$th successor of $n$.
The arity of a node is the number of successors.
Finally, no two nodes can be labeled by the same variable.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% EXAMPLES (\label{termGraphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{boxfigure}
\begin{enumerate}
    \item $
          \begin{tikzcd}
              & +_1 \ar[ld] \ar[rd] & \\
              /_2 \ar[rd, bend right=50] \ar[rd] & & /_4 \ar[ld] \ar[ld, bend left=50]\\
              & x_3 & \\
          \end{tikzcd}
          $
    \item $
          \begin{tikzcd}
              double_1 \ar[d] \\
              x_2
          \end{tikzcd} \ \ \ \Rightarrow \ \ \ 
          \begin{tikzcd}
              +_3 \ar[d, bend right=50] \ar[d, bend left=50] \\
              x_2        
          \end{tikzcd}
          $
    \item $
          \begin{tikzcd}
              & +_1 \ar[ld] \ar[loop, out = -50, in = 50, distance = 5em] \\
            4_2 & \\
          \end{tikzcd}
          $
    \item $
          \begin{tikzcd}
              +_1 \ar[rd] \ar[dd] &             \\
                                  & S_4 \ar[ld] \\
              S_2 \ar[d]          &             \\
              0_3                 &
          \end{tikzcd}
          \ \ \ \ \Rightarrow\ \ \ \ 
          \begin{tikzcd}
              S_5 \ar[d]                          &             \\
              +_6 \ar[ddd, bend right=20] \ar[rd] &             \\
                                                  & S_4 \ar[ld] \\
              S_2 \ar[d]                          &             \\
              0_3                                 &
          \end{tikzcd}
          $
\end{enumerate}
\caption{1. $1:+(2:/(3:x,3), 4:/(3,3))$,\\
         2. $1:double(2:x) \Rightarrow 3:+(2:x, 2)$\\
         3. $1:+(2:4, 1)$\\
         4. $1:+(2:S(3:0), 4:S(2)) \Rightarrow 5:S(6:+(3:0), 4:S(2:S(3)))$}
\label{fig:termGraph}
\end{boxfigure}



While the nodes in a term graph are abstract, 
in reality, they connected using pointers in the implementation.
It can be helpful to keep this in mind. 
As we define more operations on our term graphs, 
there exists a natural implementation using pointers.

We will often use a linear notation to represent graphs.
This has two advantages.
The first is that it is exact.
There are many different ways to draw the same graph,
but there is only one way to write it out a linear representation\cite{graphRewriting}
The second is that this representation corresponds closely to the representation in a computer.
The notation these graphs is given by the following grammar,
where the set of nodes and the set of labels are disjoint.

> Graph  ->  Node 
> Node   ->  n : L(Node, ... Node) 
>        |   n

We start with the root node, and for each node in the graph, If we have not encountered
it yet, then we write down the node, the label, and the list of successors.
If we have seen it, then we just write down the node.
If a node does not have any successors, then we will omit the parentheses entirely,
and just write down the label.

A few examples are shown in figure \ref{fig:termGraph}.
Example 1 shows an expression where a single variable is shared several times.
Example 2 shows how a rewrite can introduce sharing.
Example 3 shows an example of an expression with a loop.
These examples would require an infinitely large term, so they cannot be represented
in term rewrite systems.
Example 4 shows how reduction changes from terms to graphs.
In a term rewrite system, if a node is in the pattern of a redex, then it can safely be discarded.
However, in graph rewriting this is no longer true.

\theoremstyle{definition}
\begin{definition}
Let $p$ be a node in $G$, then the \emph{subgraph} $G\vert_p$ is a new graph rooted by $p$.
The nodes are restricted to only those reachable from $p$.
\end{definition}

Notice that we do not define subgraphs by paths like we did with subterms.
This is because there may be more than one path to the node $p$.
It may be the case that $G\vert_p$ and $G$ have the same nodes, such as if the root of $G$ is in a loop.

\theoremstyle{definition}
\begin{definition}
A \emph{replacement}\index{replacement} of node $p$ by graph $u$ in $g$ 
(written $g[p \to u]$) is given by the following procedure.
For each edge $(n,p) \in E_g$ replace it with an edge $(n, root_u)$.
Add all other edges from $E_g$ and $E_u$.
If $p$ is the root of $g$, then $root_u$ is now the root.
\end{definition}

It should be noted that when implementing Curry, we do not actually change
any of the pointers when doing a replacement.
Traversing the graph to find all of the pointers to $p$ would be horribly inefficient.
Instead we change the contents of $p$ to be the contents of $u$.

We can define matching in a similar way to terms, but we need to be more careful.
When matching terms the structure of the term must to be the same.
That is, both terms must have exactly the same tree.
However, when matching graphs the structure can be wildly different.
Consider the following graph.

$$
\begin{tikzcd}
    and \ar[d, bend right=50] \ar[d, bend left=50] \\
    True
\end{tikzcd}
$$

Here the graph should match the rule $and(True,True) \rightarrow True$.
But $and(True,True)$ is a term, so they no longer have the same structure.
Therefore we must be more careful about what we mean by matching.
We define matching inductively on the structure of the term.


\theoremstyle{definition}
\begin{definition}
A graph $K$ \emph{matches} a term $T$ if, and only if,
$T$ is a variable,
or $T = l(T_1,T_2\ldots T_n)$, the root of $K$ is labeled with $l$,
and for each $i \in \{1\ldots n\}$, $K_i$ matches $T_i$.
\end{definition}

Now, it may be the case that we have multiple successors pointing to the same node
when checking if a graph matches a pattern, but this is OK.
As long as the node matches each sub pattern, then the graph will match.
We extend substitutions to graphs in the obvious way.
A substitution $\sigma$ maps variables to Nodes.
In this definition for matching $\sigma$ may have multiple variables map to the same node,
but this does not cause a problem.

\theoremstyle{definition}
\begin{definition}
A \emph{rewrite rule}\index{rewrite rule} is a pair $L \to R$ where $L$ is a 
term, and $R$ is a term graph.
A graph $G$ matches the rule if there exists subgraph $K$
where $K$ matches $L$ with matcher $\sigma$.
A \emph{rewrite} is a triple $(K, L \to R, \sigma)$,
and we apply the rewrite with $G[K \to \sigma(R)]$.
\end{definition}


From here we can define narrowing similarly to how we did for terms.
We do not give the definitions here, because they are similar to the definitions
in term rewriting.
At this point we have discussed the difference between graphs and terms,
and how a replacement can be done in a graph.
For our purposes in this compiler, that is all that is needed,
but the definition of narrowing and properties about inductively sequential GRSs
can be found in Echaned and Janodet \cite{graphRewriting}.
They also show that the needed narrowing strategy is still valid for graph rewriting systems.

\section{Previous Work}
This was not meant to be an exhaustive examination of rewriting, but rather an introduction to the concepts,
since they form this theoretical basis of the Curry language.
Most work on term rewriting up through 1990 has been summarized by Klop \cite{termRewriting},
and Baader and Nipkow \cite{termAndAllThat}.
The notation and ideas in this section largely come from
Ohlebusch \cite{AdvancedTRS}, although they are very similar to the previous two summaries.
The foundations of term rewriting were laid by Church, Rosser, Curry, Feys, Newman. 
\cite{churchRosser, CombLogic, Newman}
Most of the work on rewriting has centered on confluence and termination. \cite{termRewriting}
Narrowing has been developed by Slagle \cite{narrowing}.
Sequential strategies were developed by Huet and Levy \cite{StrongSequential},
who gave a decidable criteria for a subset of sequential systems.
This led to the work of Antoy on inductively sequential systems \cite{DefinitionalTrees}.
The needed narrowing strategy came from Hanus, Antoy, and Echahed \cite{Needed}.
Graph rewriting is a bit more disconnected.  
Currently there is not a consensus on how to represent graphs mathematically.
We went with the presentation in \cite{graphRewriting}, 
but there are also alternatives in \cite{termRewriting, termAndAllThat, AdvancedTRS}

Here we saw how we can rewrite terms and graphs.
We will use this idea in the next chapter to rewrite entire programs.
This will become the semantics for our language.
Now that we have some tools, It is time to find out how to make Curry!



The Curry language grew out of the efforts to combine the functional and 
logic programming paradigms \cite{multiparadigm}.
Originally there were two approaches to combine these paradigms,
adding functional features to logic languages,
and adding logic features to functional languages.
The former approach was very popular and spawned several new languages
including Ciao-Prolog \cite{ciao}, Mercury \cite{mercury}, HAL \cite{hal}, and Oz \cite{oz}.
The extension of functional languages lead to fewer new languages,
but it did lead to libraries like the logict monad in Haskell \cite{logict}.

Ultimately the solution came from the work on automated theorem proving \cite{narrowing}.
Instead of adding features from one paradigm to another,
it was discovered that narrowing was a good abstraction for combining 
the features from both paradigms.
This spawned the Curry \cite{IntegrationFunLog} and Toy \cite{toy} languages.

In this chapter we explore the Curry language syntax and semintics.
We give example programs to show how programming in Curry differs from Prolog and Haskell.
Then we discuss the choices we made in our implementation compared to previous implementations.
Finally we give an example of generated code to demonstrate how we compile Curry programs.

\section{The Curry Language} \label{The Curry Language}

In order to write a compiler for Curry, we need to understand how Curry works.
We will start by looking at some examples of Curry programs.
We will see how Curry programs differ from Haskell and Prolog programs.
We start with a simple first order functional language,
and show how adding higher order functions, non-determinism, and free variables
all affect the semantics.
Then we discuss an improvement to backtracking that can increase performance significantly.
Finally we discuss the effect of collapsing functions,
that is functions that may return a single variable.

Curry combines the two most popular paradigms of declarative programming:
Functional languages and logic languages.
Curry programs are composed of defining equations like Haskell or ML,
but we are allowed to have non-deterministic expressions and free variables like Prolog.
This will not be an introduction to modern declarative programming languages.
The reader is expected to be familiar with functional languages such as Haskell or ML,
and logic languages such as Prolog.
For an introduction to programming in Curry see \cite{CurryTutorial}.
For an exhaustive explanation of the syntax and semantics of Curry see \cite{CurryReport}.

To demonstrate the features of Curry, we will examine a small Haskell program to permute a list.
Then we will simplify the program by adding features of Curry.
This will demonstrate the features of Curry that we need to handle in the compiler,
and also give a good basis for how we can write the compiler.

First, let us consider an example of a permutation function.
This is not the only way to permute a list in Haskell,
and you could easily argue that it is not the most elegant way,
but we chose it for three reasons.
There is no syntactic sugar,
and the only two library functions are |concat| and |map|, both very common functions,
and the algorithm for permuting a list is similar to the algorithm we will use in Curry.

> perms         :: [a] -> [[a]]
> perms []      = [[]]
> perms (x:xs)  = concat (map (insert x) (perms xs))
>   where
>     insert x []      = [[x]]
>     insert x (y:ys)  = (x:y:ys) : map (y:) (insert x ys)

The algorithm itself is broken into two parts.
The |insert| function will return a list of lists,
where |x| is inserted into |ys| at every possible position.
For example: |insert 1 [2,3]| returns |[[1,2,3],[2,1,3],[2,3,1]]|.
The |perms| function splits the list into a head |x| and tail |xs|.
First, it computes all permutations of |xs|, then it will insert |x| into every possible position
of every permutation.

While this algorithm is not terribly complex, it is really more complex than it needs to be.
The problem is that we need to keep track of all of the permutations we generate.
This does not seem like a big problem here.
We just put each permutation in a list, and return the whole list of permutations.
However, now every part of the program has to deal with the entire list of results.
As our programs grow, we will need more data structures for this plumbing, and this problem will grow too.
This is not new.
Many languages have spent a lot of time trying to resolve this issue.
In fact, several of Haskell's most successful concepts,
such as monads, arrows, and lenses, are designed strictly to reduce this sort of plumbing.

We take a different approach in Curry.
Instead of generating every possible permutation, and searching for the right one,
we will non-deterministically generate a single permutation.
This seems like a trivial difference, but its really quite substantial.
We offload generating all of the possibilities onto the language itself.

We can simplify our code with the non-deterministic \textit{choice} operator |?|.
Choice is defined by the rules:
> x ? y = x
> x ? y = y

Now our permutation example becomes a little easier.
We only generate a single permutation,
and when we insert |x| into |ys|, we only insert into a single arbitrary position.


> perm         :: [a] -> [a]
> perm []      = []
> perm (x:xs)  = insert x (perm xs)
>   where
>     insert x []      = [x]
>     insert x (y:ys)  = x:y:ys ? y : insert x ys

In many cases functions that return multiple results can lead to much simpler code.
Curry has another feature that is just as useful.
We can declare a \textit{free variable} in Curry.
This is a variable that has not been assigned a value.
We can then constrain the value of a variable later in the program.
In the following example |begin|, |x|, and |end| are all free variables,
but they are constrained by the guard so that |begin++[x]++end| is equal to |xs|.
Our algorithm then becomes: pick an arbitrary |x| in the list,
move it to the front, and permute the rest of the list.

> perm     :: [a] -> [a]
> perm []  = []
> perm xs
>  | xs == (begin++[x]++end) = x : perm (begin++end)
>  where begin, x, end free

Look at that.
We have reduced the number of lines of code by 25\%.
In fact, this pattern of declaring free variables, and then immediately constraining them
is used so often in Curry that we have syntactic sugar for it.
A \textit{functional pattern} is any pattern that contains a function that is not at the
root.\footnote{
    This is not completely correct.  While the above code would fully evaluate the list,
    a functional pattern is allowed to be more lazy.
    Since the elements do not need to be checked for equality, they can be left unevaluated.
}
We can use functional patterns to simplify our |perm| function even further.


> perm                    :: [a] -> [a]
> perm []                 = []
> perm (begin++[x]++end)  = x : perm (begin++end)

Now the real work of our algorithm is a single line.
Even better, it is easy to read what this line means.
Decompose the list into |begin|, |x|, and |end|, then put |x| at the front, and permute |begin| and |end|.
This is almost exactly how we would describe the algorithm in English.

There is one more important feature of Curry.
We can let expressions fail.
In fact we have already seen it, but a more explicit example would be helpful.
We have shown how we can generate all permutations of a list by generating an arbitrary permutation,
and letting the language take care of the exhaustive search.
However, we usually do not need, or even want, every permutation.
So, how do we filter out the permutations we do not want?
The answer is surprisingly simple.  We just let expressions fail.
An expression fails if it cannot be reduced to a constructor form.
The common example here is |head []|, but a more useful example might be sorting a list.
We can build a sorting algorithm by permuting a list, and only keeping the permutation that is sorted.

> sort :: (Ord a) => [a] -> [a]
> sort xs | sorted ys = ys
>  where 
>   ys = perm xs
>   sorted []        = True
>   sorted [x]       = True
>   sorted (x:y:ys)  = x <= y && sorted (y:ys)

In this example every permutation of |xs| that is not sorted will fail in the guard.
Once an expression has failed, computation on it stops, and other alternatives are tried.
As we will see later on, this ability to conditionally execute a function will 
become crucial when developing optimizations.

These are some of the useful programming constructs in Curry.
While they are convenient for programming, we need to understand how they work
if we are going to implement them in a compiler.

\section{Semantics} \label{Semantics}

As we have seen, the syntax of Curry is very similar to Haskell.
Functions are declared by defining equations, and new data types are declared as algebraic data types.
Function application is represented by juxtaposition,
so |f x| represents the function |f| applied to the variable |x|.
Curry also allows for declaring new infix operators.
In fact, Curry really only adds two new pieces of syntax to Haskell, \textbf{fcase} and \textbf{free}.
However, the main difference between Curry and Haskell is not immediately clear from the syntax.
Curry allows for overlapping rules and free variables.
Specifically Curry programs are represented as 
\emph{Limited Overlapping Inductively Sequential (LOIS)}
\index{Limited Overlapping Inductively Sequential} Rewrite systems.
These are is indicatively sequential systems with a single overlapping rule.
On the other hand, Haskell programs are transformed into non-overlapping systems.

To see the difference consider the usual definition of factorial.

> fac :: Int -> Int
> fac 0 = 1
> fac n = n * fac (n-1)

This seems like an innocuous Haskell program, 
however It is non-terminating for every possible input for Curry.
The reason is that |fac 0| could match either rule.
In Haskell all defining equations are ordered sequentially,
which results in control flow similar to the following C implementation.
\begin{verbatim}
int fac(int n)
{
    if(n == 0)
    {
        return 1;
    }
    else
    {
        return n * fac(n-1);
    }
}
\end{verbatim}
In fact, every rule with multiple defining equations follows this pattern.
In the following equations let |p_i| be a pattern and |E_i| be an expression.
> f p_1  = E_1
> f p_2  = E_2
> ...
> f p_n  = E_n
Then this is semantically equivalent to the following.

> f p_1                                = E_1
> f not p_1  && p_2                    = E_2
> ...
> f not p_1  && not p_2 && ... && p_n  = E_n

Here |not p_i| means that we do not match pattern |i|.
This ensures that we will only ever reduce to a single expression.
Specifically we reduce to the first expression where we match the pattern.


Curry rules, on the other hand, are unordered.
If we could match multiple patterns, such as in the case of |fac|, 
then we non-deterministically return both expressions.
This means that |fac 0| reduces to both |1| and |fac (-1)|.
Exactly how Curry reduces an expression non-deterministically will be discussed throughout this dissertation,
but for now we can think in terms of sets.
If the expression |e -> e_1| and |e -> e_2|,
|e_1 ->* v_1| and |e_2 ->* v_2|, then 
|e ->* {v_1, v_2}|.\footnote{This should really be thought of as a multiset, since it is possible for |v_1| and |v_2| to be the same value.}

This addition of non-determinism can lead to problems if we we are not careful.
Consider the following example:\\

> coin = 0 ? 1
> double x = x + x

We would expect that for any |x|, |double x| should be an even number.
However, if we were to rewrite |double coin| using ordinary term rewriting,
then we could have the derivation.
> double coin => coin + coin => (0 ? 1) + (0 ? 1) => 0 + (0 ? 1) => 0 + 1 => 1

This is clearly not the derivation we want.
The problem here is that when we reduced |double coin|,
we made a copy of the non-deterministic expression |coin|.
This ability to clone non-deterministic expressions to get different answers
is known as run-time choice semantics. \cite{callTimeChoice}.

The alternative to this is call-time choice semantics.
When a non-deterministic expression is reduced,
all instances of the expression take the same value.
One way to enforce this is to represent expressions as graphs instead of terms.
Since no expressions are ever duplicated, all instances of |coin| will reduce the same way.
This issue of run-time choice semantics will appear throughout the compiler.


\subsection{FlatCurry} \label{FlatCurry}

The first step in the compiler pipeline is to parse a Curry program into FlatCurry\index{FlatCurry}.
The definition is given in figure \ref{fig:flatSyntax}.
The FlatCurry language is the standard for representing Curry programs
in compilers \cite{pakcs, kics2, Kics2Theory, sprite}, 
and has been used to define the semantics of Curry programs \cite{currySemantics}.



The semantics of Curry have already been studied extensively \cite{currySemantics},
so we informally recall some of the more important points.
A FlatCurry program consists of datatype and function definitions.
For simplicity we assume that all programs are self contained,
because the module system is not relevant to our work.
However, the Rice compiler does support modules.
A FlatCurry function contains a single rule, 
which is responsible for pattern matching and rewriting an expression.
Pattern matching is converted into case and choice expressions as defined in \cite{currySemantics}.
A function returns a new expression graph constructed out of |let, free, f_k, C_k, ?, l, v|
expressions.

Our presentation of FlatCurry differs from \cite{currySemantics} in three notable ways.
First, function and constructor applications
contain a count of the arguments they still need in order to be fully applied.
The application |f_k e_1 e_2 ... e_n| means that |f| is applied to |n| arguments,
but it needs |k| more to be fully applied,
so the arity of |f| is |n+k|.
Second, we include |let {v} free| to represent free variables.
This was not needed in \cite{currySemantics, kics2} because free variables we translated
to non-deterministic generators.
Since we narrow free variables instead of doing this transformation, 
we must represent free variables in FlatCurry.
Finally, we add an explicit failure expression |EXEMPT| to represent a branch
that is not present in the definitional tree.
While this is meant to simply represent a failing computation,
we have also occasionally found it useful in optimization.

\subsection{Evaluation} \label{Evaluation}

Each program contains a special function |main| that takes no arguments.
The program executes by reducing the expression |main| 
to a \emph{Constructor Normal 
Form}\index{Constructor Normal Form}\footnote{This is constructor normal form, and not simply a normal form,
              because a failing expression, like |head []|, is a normal form,
              since it can not be rewritten, but it contains a function at the root.} 
as defined in figure \ref{fig:normalForm}.
Similar to Kics2, Pakcs, and Sprite, \cite{kics2, pakcs, sprite}
we compute constructor normal form by first reducing the |main| to 
\emph{Head Constructor Form}\index{Head Constructor Form}.
That is where the expression is rooted by a constructor.
Then each child of the root is reduced to constructor normal form.

\begin{boxfigure}

> f           =>  f (vec v) = e
> e           =>  v                                        Variable
>             |   l                                        Literal
>             |   e_1 ? e_2                                Choice
>             |   EXEMPT                                   Failed
>             |   f_k (vec e)                              Function Application
>             |   C_k (vec e)                              Constructor Application
>             |   let (vec (v = e)) in e                   Variable Declaration
>             |   let (vec v) free in e                    Free Variable Declaration
>             |   case e of (vec (p -> e))                 Case Expression
> p           =>  C (vec v)                                Constructor Pattern
>             |   l                                        Literal Pattern
\caption{Syntax definition for FlatCurry\\
This is largely the same as other presentations \cite{currySemantics,icurry}
but we have elected to add more information that will become relevant for optimizations later.
The notation |vec e| refers to a variable length list |e_1 e_2 ... e_n|.}
\label{fig:flatSyntax}
\end{boxfigure}

\begin{boxfigure}
> n =>  l            literal
>   |   C_k (vec n)  constructor
\caption{constructor normal forms in FlatCurry.\\
         A CNF is an expression that contains only constructor and literal symbols.
         All CNFs are normal forms in our system.}
\label{fig:normalForm}
\end{boxfigure}

Most of the work of evaluation is reducing an expression to head constructor form.
Kics2 and Pakcs are able to transform FlatCurry programs into an equivalent rewrite
system, and reduce expressions using graph rewriting \cite{kics2, pakcs}.
The transformation simply created a new function for every nested case expression.
This created a series of tail calls for larger functions.

To see this transformation in action, we can examine the FlatCurry function |==| on lists \ref{fig:eqList}.
This function is inductively sequential, however both Pakcs and Kics2 will transform it
into a series of flat function calls with a single case at the root.
Since this would drastically increase the number of function calls, we avoid this transformation.
It would also defeat much of the purpose of an optimizing compiler 
if we were not allowed to inline functions.

\begin{boxfigure}
Original FlatCurry representation of |==| on lists.
> (==) v_2 v_3 = case  v_2 of
>                      [] -> case  v_3 of
>                                  [] -> True
>                                  v_4 : v_5 -> False
>                      v_6 : v_7 -> case  v_3 of
>                                         [] -> False
>                                         v_8 : v_9 -> v_6 == v_8 && v_7 == v_9

Transformed FlatCurry representation of |==| on lists.
> (==) v_2 v_3 = case  v_2 of
>                      [] -> eqListNil v_3
>                      v_6 : v_7 -> eqListCons v_3 v_6 v_7
> eqListNil v_3 = case  v_3 of
>                       [] -> True
>                       v_4 : v_5 -> False
> eqListCons v_3 v_6 v_7 = case  v_3 of 
>                                [] -> False
>                                v_8 : v_9 -> v_6 == v_8 && v_7 == v_9


\caption{Transformation of FlatCurry |==| function into a flat representation
         for Pakcs and Kics2.}
\label{fig:eqList}
\end{boxfigure}

\subsection{Non-determinism} \label{Non-determinism}


Currently there are three approaches to evaluating non-deterministic expression in Curry:
\emph{backtracking}\index{backtracking}, 
\emph{Pull-Tabbing}\index{pull-tabbing}\cite{pulltab}, and 
\emph{Bubbling}\index{bubbling}\cite{bubbling}.
At this time there are no complete strategies for evaluating Curry programs,
so we have elected to use backtracking.
It is the simplest to implement, and it is well understood.

In our system, backtracking is implemented in the usual way.
When an expression rooted by a node |n| with label by |f| 
is rewritten to an expression rooted by |e|,
we push the rewrite |(n,n_f,Continue)| onto a backtracking stack, where |n_f|
is a copy of the original node labeled by |f|.
If the expression is labeled by a choice |e_1 ? e_2|, and it is rewritten to the left hand side |e_1|,
then we push |(n, n_q, Stop)| onto the backtracking stack to denote that this 
was an alternative, and we should stop backtracking.

Unfortunately, while backtracking is well defined for rewriting systems,
our representation of FlatCurry programs is not a graph rewrite system.
This is because we do not flatten our FlatCurry functions like Pakcs and Kics2.
As an example of why FlatCurry programs are not a graph rewriting system,
consider the FlatCurry function |weird| \ref{fig:weird}.
This function defines a local variable |x| which is used in a case expression.
If this were a rewrite system, then we would be able to translate the
|case| expression into pattern matching, but a rule can not pattern match on a locally defined variable.
We show the reduction of |wierd| in figure \ref{fig:weirdEval}.

\begin{boxfigure}
> weird = let x = False ? True
>         in case  x of
>                  False -> True
>                  True -> False
\caption{The function |weird|\\
         This can not be expressed as rewrite rules, because the expression
         we are pattern matching on is defined locally.}
\label{fig:weird}
\end{boxfigure}

\begin{boxfigure}
\noindent
\begin{itemize}
   \item We start with a root |r| labeled by |weird|.
   \item Node |n_1| labeled by |?| is created with children |[False, True]|.
   \item |n_1| is rewritten to |False| and |(n_1,True,Stop)| is pushed on the backtracking stack.
   \item |r| is rewritten to |True| and |(r,weird,Continue)| is pushed on the backtracking stack.
   \item |r| is a constructor normal form.
   \item backtracking to the closest alternative.
   \item The backtracking stack is |[(r,weird,Continue), (n_1, True, Stop)]|.
   \item reduce |r|.
   \item Node |n_2| labeled by |?| is created with children |[False, True]|.
   \item $\ldots$
\end{itemize}
\caption{Evaluation of the function |weird|.}
\label{fig:weirdEval}
\end{boxfigure}


We have entered an infinite loop of computing the same rewrite.
The problem is that when we were backtracking,
and replacing nodes with their original versions, we were going too far
back in the computation.
In this example, when backtracking |weird|, we want to backtrack to a point
where |x| has been created, and we just want to evaluate the case again.

We solve this problem by creating a new function for each case expression in our original function.
Figure \ref{fig:caseFuncs} show an example for |weird| and |==| which were defined above.
This is actually very similar to how Pakcs and Kics2 transformed their programs into
rewrite systems by flattening them.
The difference is that we do not need to make any extra function calls unless
we are already backtracking.
There is no efficiency cost in either time or space with our solution.
The only cost is a little more complexity in the code generator,
and an increase in the generated code size.
This seems like an acceptable trade off, 
since our programs are still similar in size to equivalent programs compiled with GHC.

\begin{boxfigure}
> weird =  let x = False ? True
>          in case  x of
>                   False -> True
>                   True -> False
>
> weird_1 x = case  x of
>                   False -> True
>                   True -> False
>
> (==) v_2 v_3 = case  v_2 of
>                      []         -> case  v_3 of
>                                          [] -> True
>                                          v_4 : v_5 -> False
>                      v_6 : v_7  -> case  v_3 of
>                                          [] -> False
>                                          v_8 : v_9 -> v_6 == v_8 && v_7 == v_9
>
> eqList_1 v_3 = case  v_3 of
>                      [] -> True
>                      v_4 : v_5 -> False
>
> eqList_2 v_6 v_7 v_3 = case  v_3 of
>                              [] -> False
>                              v_8 : v_9 -> v_6 == v_8 && v_7 == v_9
\caption{Functions at case for |weird| and |==| for lists.}
\label{fig:caseFuncs}
\end{boxfigure}



As far as we are aware, this is a novel approach for improving the efficiency of backtracking
in rewriting systems.
The correctness of this method follows 
from the redex contraction theorem, which is proved later.


\subsection{Free Variables} \label{Free Variables}

Free variables are similar to non-deterministic expressions.
In fact, in both Kics2 and Sprite \cite{kics2,sprite} they are replaced
by non-deterministic generators of the appropriate type \cite{OverlappingRules}.
However, in Rice, free variables are instantiated by narrowing.
If a free variable is the scrutinee of a case expression, then 
we push copies of the remaining patterns onto the stack along with another 
copy of the variable.
If the free variable is replaced by a constructor with arguments, such as |Just|,
then we instantiate the arguments with free variables.


\begin{boxfigure}
> data Light = Red | Yellow | Green
>
> change x = case  x of
>                  Red     -> Green
>                  Green   -> Yellow 
>                  Yellow  -> Red
\caption{A simple traffic light program}
\label{fig:light}
\end{boxfigure}

This is easier to see with an example.
Consider the traffic light function in figure \ref{fig:light}.
The |change| function moves the light from |Red| to |Green| to |Yellow|.
When calling this function with a free variable, we have the derivation below in figure \ref{fig:lightEval}.

\begin{boxfigure}
%{
%format free = "\Varid{free}"
\begin{itemize}
   \item We start with root |r| labeled by |change|, with a child |x| labeled by |free|.
   \item |x| is rewritten to |Red| and |(x,Green,Stop), (x,Yellow,Stop), (x,free,Continue)| 
         are all pushed on the stack
   \item |r| is rewritten to |Green|, and |(r,change, Continue)| is pushed on the stack
   \item |r| is a constructor normal form
   \item backtracking to the closest alternative
   \item backtracking stack is \\
         |[(r,change,Continue), (x,Green,Stop), (x,Yellow,Stop), (x,free,Continue)]|.
   \item reduce |r|
   \item |x| is labeled by |Green|
   \item |r| is rewritten to |Yellow|, and |(r,change, Continue)| is pushed on the stack
   \item |r| is a constructor normal form
   \item backtracking to the closest alternative
   \item backtracking stack is 
         |[(r,change,Continue), (x,Yellow,Stop), (x,free,Continue)]|
   \item reduce |r|
   \item |x| is labeled by |Yellow|
   \item |r| is rewritten to |Red|, and |(r,change, Continue)| is pushed on the stack
   \item |r| is a constructor normal form
   \item backtracking to the closest alternative
   \item backtracking stack is 
         |[(r,change,Continue), (x,free,Continue)]|
   \item Both rewrites are popped, and the stack is empty with no alternatives.
\end{itemize}
%}
\caption{Evaluation of |change x where x free|}
\label{fig:lightEval}
\end{boxfigure}

\subsection{Higher Order Functions} \label{Higher Order Functions}

Now that we have a plan for the logic features of Curry, we move on to higher order functions.
This subject has been extensively studied by the function languages community,
and we take the approach of \cite{fastCurry}.
Higher order functions are represented using defunctionalization \cite{defunctionalization}.
Recall that in FlatCurry, an expression $f_k$\index{$f_k$} represents a partial application
that is missing |k| arguments.
We introduce an |apply| function that has an unspecified arity,
where |apply f_k e_1 e_2 ... e_n| applies |f_k| to the arguments |e_1 e_2 ... e_n|.

The behavior of |apply| is specified below.

\begin{minipage}{\textwidth}
> apply f_k x_1 ... x_n
>  | k > n   = f_kn x_1 ... x_n
>  | k == n  = f x_1 ... x_n
>  | k < n   = apply (f x_1 ... x_k) x_k1 ... x_n
\end{minipage}

If the first argument |f| of |apply| is not partially applied,
then evaluate |f| until it is, and proceed as above.
In the case that |f| is a free variable, then we return |EXEMPT|, 
because we do not support higher order narrowing.

\subsection{Backtracking Performance} \label{Backtracking Performance}

Now that we have established a method for implementing non-determinism,
we would like to improve the performance.
Currently we push nodes on the backtracking stack for every rewrite.
Often, we do not need to push most of these rewrites.
Consider the following code for computing Fibonacci numbers:

> fib n = case  n < 2 of
>               True   -> n
>               False  ->  fib (n-1) + fib (n-2)
>
> main = case  fib 20 == (1 ? 6765) of
>              True -> putStrLn "found answer"

This program will compute |fib 20|, pushing all of those rewrites onto the stack as it does,
and then, when it discoverers that |fib 20 /= 1|, it will undo all of those computations,
only to redo them immediately afterwards!
This is clearly not what we want.
Since |fib| is a deterministic function, we would like to avoid pushing these rewrites onto the stack.
Unfortunately, this is not as simple as it would first seems for two reasons.
First, determining if a function is non-deterministic in general is undecidable,
so any algorithm we developed would push rewrites for some deterministic computations.
Second, a function may have a non-deterministic argument.
For example, we could easily change the above program to:

> main = case  fib (1 ? 20) == 6765 of
>              True -> putStrLn "found answer"

Now the expression with |fib| is no longer deterministic.
We sidestep the whole issue by noticing that while it is impossible to tell 
if an expression is non-deterministic at compile time,
it is very easy to tell if it is at run time.

As far as we are aware, this is another novel solution.
Each expression contains a Boolean flag that marks if it is non-deterministic.
We called these \emph{nondet}\index{nondet} flags,
and we refer to an expression whose root node is marked with a nondet flag as nondet.
The rules for determining if an expression |e| is nondet are:
if |e| is labeled by a choice, then |e| is nondet;
if |e| is labeled by a function that has a case who is scrutinee is nondet,
or is a forward to a nondet, then |e| is nondet;
if |e' ->* e| and |e'| is nondet, then |e| is nondet.

Any node not marked as nondet does not need to be pushed on the stack
because it is not part of a choice, 
all of its case statements scrutinized deterministic nodes,
and it is not forwarding to a non-deterministic node.
However proving this is a more substantial problem.

We prove this for the class of limited overlapping inductively sequential graph rewriting systems, 
with the understanding our system is equivalent.
This proof is based on a corresponding proof for set functions in Curry \cite{setFunctions}[Lemma 2].
The original proof was concerned with a deterministic derivation from an expression to a value.
While the idea is similar, we do not want to necessarily derive an expression to a value.
Instead we define a deterministic redex, and deterministic step below, and show that there
is an analogous theorem for a derivation of deterministic steps, even if it does not compute a value.

\begin{definition}
Given a rewrite system $R$ with fixed strategy $\phi$, 
a \emph{computation space}\index{computation space} \cite{setFunctions} of expression $e$, 
$C(e)$\index{$C(e)$} is finitely branching tree defined inductively the rule
$C(e) = \langle e, C(e_1), C(e_2) \ldots C(e_n)\rangle$.
\end{definition}
\label{def:computationSpace}

We now need the notions of a deterministic redex and a deterministic rewrite.
Ultimately we want to show that if we have a deterministic reduction,
then we can perform that computation at any point without affecting the results.
One implication of this would be that performing a deterministic computation before
a non-deterministic choice was made would be the same as performing the computation
after the choice.
This would justify our fast backtracking scheme, because it would be equivalent
to performing the computation before the choice was made.

\begin{definition}
A redex $n$ in expression $e$ is deterministic if there is at most
one rewrite rule that could apply to $e\vert_n$.
A rewrite $e \to_n e'$ is deterministic if $n$ is a deterministic redex.
\end{definition} \label{def:deterministic}

Next we rephrase our notion of nondet for a LOIS system.

\begin{definition}
let $e \to e_1 \to \ldots v$ be a derivation for $e$ to $v$.
A node $n$ in $e_i$ is  \emph{nondet} iff\\
1. $n$ is labeled by a choice.\\
2. A node in the redex pattern \cite{bubblingCorrect} of $n$ is \emph{nondet}.\\
3. There exists some $j < i$ where $n$ is a subexpression of $e_j$ and $n$ is \emph{nondet}.
\end{definition}

The first property is that all choice nodes are nondet.
The second property is equivalent to the condition that any node 
that scrutinizes a nondet node should be nondet.
Finally, the third property is that nondet should be a persistent attribute.
This corresponds to the definition we gave for nondet nodes above.

If $n$ is a redex that is not marked as nondet,
then $n$ ca not be labeled by a choice.
Since choice is the only rule in a LOIS system that is non-deterministic,
$n$ must be a deterministic redex.
We recall a theorem used to prove the correctness of set function.\cite{setFunctions}[Def. 1, Lemma 1]

\begin{lemma}
Given an expression $e$ where $e \to_{n_1} e_1$ and $e \to_{n_2} e_2$,
if $n_1 \ne n_2$, then there exists a $u_1$ and $u_2$ where
$t_1 \to^= u_1$ and $t_2 \to^= u_2$ and $u_1 = u_2$ up to renaming of nodes.
\end{lemma}

This leads directly to our first important theorem.
If $n$ is a deterministic redex in a derivation, then we can move it earlier in the derivation.

\begin{theorem}[Redex Compression Theorem\index{Redex Compression Theorem}]
if $n$ is a deterministic redex of $e$ where $n \to n'$, and $e \to e_1 \to_n e_2$.
Then there exists a derivation $e[n \to n'] \to^= e'$ where $e_2 = e'$ up to renaming of nodes.
\end{theorem}

\begin{proof}
By definition of rewriting $e \to_n e[n\to n']$.
Since $n$ is a deterministic redex, it must be the case that the redex in $e \to e_1$
was not $n$. So by the previous lemma, we can swap the order of the rewrites.
\end{proof}

Finally we show that if $a$ is a subexpression of $e$ and $a \to^* b$ using only deterministic redexes,
then $e[b \from a]$ rewrites to the same values.

\begin{theorem}[Path Compression Theorem\index{Path Compression Theorem}]
if $a$ is a subexpression of $e$ and $a \to^* b$ using only deterministic rewrites,
and $e \to e_1 \to \ldots e_n$ is a derivation where $b$ is a subexpression of $e_n$,
then there is a derivation $e[b \from a] \to^* e_n$.
\end{theorem}

\begin{proof}
This follows by induction on the length of the derivation.
In the base case $a = b$, and there is nothing to prove.
In the inductive case $a \to_p a' \to^* b$.
Since $a \to_p a'$ is deterministic by assumption,
we can apply the path compression theorem and say that $e[a' \from a] \to^* e_n$.
By the inductive hypotheses we can say that $e[a' \from a][b \from a'] \to^* e_n$.
Therefore $e[b \from a] \to^* e_n$.
This establishes our result.
\end{proof}



\subsection{Collapsing Functions} \label{Collapsing Functions}

While the result of the previous section is great, 
and it allows us to avoid creating a large number of stack frames,
there is a subtle aspect of graph rewriting that gets in the way.
If a node |n_1| labeled by function |f| is rewritten to |n_2|,
then the definition of applying a rewrite rule \cite{graphRewriting}[Def. 8, Def. 10, Def. 19]
would require us to traverse the graph, and find every node that has |n_1| as a child,
and redirect that pointer to |n_2|.
This is clearly inefficient, so this is not done in practice.
A much faster method is to simply replace the contents, the label and children, of |n_1|
with the contents of |n_2|.
This works most of the time, but we run into a problem when
a function can rewrite to a single variable, such as the |id| function.
We call these functions \emph{collapsing functions}\index{collapsing functions}.
One option to solve this problem is to evaluate the contractum 
to head constructor form, and copy the constructor
to the root \cite{gmachine}.
This is commonly used in lazy functional languages,
however it does not work for Curry programs.
Consider the expression following expression.

> f = let x = True ? False
>         y = id x
>     in not y

When |y| is first evaluated, then it will evaluate |x|, and |x| will evaluate to |True|.
If we then copy the |True| constructor to |y|, then we have two copies of |True|.
But, since |y| is deterministic, we do not need to undo |y| when backtracking.
So, |y| will remain |True| after backtracking, instead of returning to |id x|.
While constructor copying is definitely invalid with fast backtracking,
it is unclear if it would be valid with a normal backtracking algorithm.

We can solve this problem by using forwarding nodes\index{forwarding node}, 
sometimes called indirection nodes
\cite{lazyFunctionalCompilers}.
The idea is that when we rewrite an expression rooted by a collapsing function,
instead of copying the constructor, we just replace the root with a special forwarding node,
$FORWARD(x)$\index{$FORWARD$}, where |x| is the variable that the function collapses to.

There is one more possibility to address before we move on.
One performance optimization with forwarding nodes is \emph{path compression}\index{path compression}.
If we have a chain of forwarding nodes \\
$FORWARD(FORWARD(FORWARD(x)))$, 
we want to collapse this to simply $FORWARD(x)$.
This is unequivocally invalid in non-deterministic backtracking systems.
Consider the following function.

> f =  let  x = True ? False
>           y = id x
>      in   case  y of
>                 False -> case  x of
>                                False -> ()

When reducing this function, we create two forwarding nodes
that are represented by the variables $x$ and $y$.
We refer to these nodes as $FORWARD_x$ and $FORWARD_y$ respectively.
So $x$ is reduced to $FORWARD_x(True)$, and $y$ is reduced to $FORWARD_y(FORWARD_x(True))$.
If we contract |y| to $FORWARD_y(True)$,
then when we backtrack we replace |x| with $FORWARD_x(False)$, and |y|
is replaced with $id(True)$.
The reason that $y$ does not change to $id(False)$ is because $y$ 
has lost its reference to $x$.
Now, not only do we fail to find a solution for |f|, we have ended up in a state
where |x| and |y| have different values.



In this chapter we have discussed the Curry language, and overviewed the semantics of Curry programs.
We have shown different approaches to implementing a system for running Curry programs,
and we have discussed the choices that we made.
When a decision needed to be made, we prioritized correctness, then efficient execution,
and then ease of implementation.
In the next chapter we discuss the implementation at a low level.
This will give us an idea of what the code we want to generate should look like.


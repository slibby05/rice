
In this chapter we introduce our engine for Generating and Altering Subexpressions, of the GAS system.
This system proves to be incredibly versatile and is the main workhorse of the compiler and optimizer.
We show how to construct, combine, and improve the efficiency of transformations,
as well as how the system in implemented.



\section{Building Optimizations}

Optimization is usually considered the most difficult aspect of writing a modern compiler.
It's easy to see why.
There are dozens of small optimizations to make, 
and each one needs to be written, shown correct, and tested.

Furthermore, there are several levels where an optimization can be applied.
Some optimizations apply to a programs AST, some to another intermediate representation,
some to the generated code, and even some to the runtime system.
There are even optimizations that are applied during transformations between representations.
For this chapter, we will be describing a system to apply optimizations to FlatCurry programs.
While this is not the only area of the compiler where we applied optimizations,
it is by far the most extensive, so it's worth understanding how our optimization engine works.

Generally speaking, most optimizations have the same structure.
Find an area in the AST where the optimization applies, and then replace it with the optimized version.
As an example, consider the code for the absolute value function defined below.

> abs x
>  | x < 0      = -x
>  | otherwise  = x

This will be translated into FlatCurry as

> abs x = case  (x < 0) of
>               True   -> -x
>               False  -> case  otherwise of
>                               True   -> x
>                               False  -> EXEMPT

While this transformation is obviously inefficient, 
it is general and has a straightforward implementation.
A good optimizer should be able to recognize that |otherwise| reduces to |True|,
and reduce the case-expression.
So for this one example, we have two different optimizations we need to implement.
We need to reduce |otherwise| to |True|, then we can reduce the second case expression to |x|.

There are two common approaches to solving this problem.
The first is to make a separate function for each optimization.
Each function will traverse the AST and try to apply its optimization.
The second option is to make a few large functions that attempt to apply several optimizations at once.
There are trade-offs for each.

The first option has the advantage that each optimization is easy to write and understand.
However, is suffers from a lot of code duplication,
and it's not very efficient.  We must traverse the entire AST every time we want to apply an optimization.
Both LLVM and the JVM fall into this category \cite{llvm, jvm}.
The second option is more efficient, and there is less code duplication,
but it leads to large functions that are difficult to maintain or extend.

Using these two options generally leads to optimizers that are difficult to maintain.
To combat this problem, many compilers will provide a language to describe optimization transformation.
Then the compiler writer can use this domain specific language to develop their optimizations.
With the optimization descriptions, the compiler can search the AST of a program to find
any places where optimizations apply.
However, it is difficult or impossible to write many common optimizations in this style \cite{playingByTheRules}.

The aim of our solution is to try to get the best of all three worlds.
We've developed an approach to simplify Generating and Altering Subexpressions (GAS)
\index{Generating and Altering Subexpressions (GAS)}.
Our approach was to do optimization entirely by rewriting.
This has several advantages, and might be the most useful result of this work.
First, developing new optimizations is simple.
We can write down new optimizations in this system within minutes.
It was often easier to write down the optimization and test it,
than it was to try to describe the optimization in English.
Second, any performance improvement we made to the optimization engine
would apply to every optimization.
Third, optimizations were easy to maintain and extend.
If one optimization didn't work, we could look at it and test it in isolation.
Fourth, this code is much smaller than a traditional optimizer.
This isn't really a fair comparison given the relative immaturity of our compiler,
but we were able to implement 16 optimizations and code transformations in under 150 lines of code.
This gives a sense of scale of how much easier it is to implement optimizations in this system.
Fifth, Since We're optimizing by rewrite rules,
the compiler can easily output what rule was used,
and the position where it was used.
This is enough information to entirely reconstruct the optimization derivation.
We found this very helpful in debugging.
Finally, optimizations are written in Curry.
We didn't need to develop a DSL to describe the optimizations,
and there are no new ideas for programmers to learn if they want to extend the compiler.

We should note that there are some potential disadvantages to the GAS system as well.
The first disadvantage is that there are some optimizations and transformations
that are not easily described by rewriting.
The second is that, while we've improved the efficiency of the algorithm considerably,
it still takes longer to optimize programs then we'd like.

The first problem isn't really a problem at all.
If there is an optimization that doesn't lend itself well to rewriting,
we can always write it as a traditional optimization.
Furthermore, as we'll see later, we don't have to stay strictly in the bounds of rewriting.
The second problem is actually more fundamental to Curry.
Our implementation relies on finding a single value from a set generated by a non-deterministic function.
Current implementations are inefficient, but there are new implementations being developed
\cite{synthesizedSetFunctions}.
We also believe that an optimizing compiler would help with this problem \cite{this}.

\subsection{The Structure of an Optimization}

The goal with GAS is to make optimizations simple to implement and easily readable.
While this is a challenging problem, we can actually leverage Curry here.
Remember that the semantics of Curry are already non-deterministic rewriting.

Each optimization is going to be a function from a FlatCurry expression to another FlatCurry expression.

> type Opt = Expr -> Expr

For readability we use the FlatCurry syntax defined in figure \ref{fig:flatSyntax},
While this version of FlatCurry is easier to read,
we will need the actual representation of FlatCurry programs to implement the compiler.
This representation is given in figure \ref{fig:flatRep}, and the transformation
from the FlatCurry syntax to the FlatCurry representation is given in figure \ref{fig:flatTranslate}.
We can describe an optimization by simply describing what it does to each expression.
As an example consider the definition for floating let-expressions:

\begin{figure}

> type QName = (String, String)
> type Arity = Int
> type VarIndex = Int
> data Visibility = Public | Private
>
> data FuncDecl = Func QName Arity Rule
> data Rule
>   = Rule [VarIndex] Expr
>   | External String
> data CombType = FuncCall | ConsCall | FuncPartCall Arity | ConsPartCall Arity
> data Expr
>   = Var VarIndex
>   | Lit Literal
>   | Comb CombType QName [Expr]
>   | Let [(VarIndex, Expr)] Expr
>   | Free [VarIndex] Expr
>   | Or Expr Expr
>   | Case Expr [BranchExpr]
> data BranchExpr = Branch Pattern Expr
> data Pattern
>   = Pattern QName [VarIndex]
>   | LPattern Literal
> data Literal
>   = Intc   Int
>   | Floatc Float
>   | Charc  Char
\caption{Curry representation of FlatCurry programs\\
This is the standard representation of FlatCurry programs as defined in \cite{FlatCurry},
We have removed |CaseType| and |Typed| from |Expr|, and 
|TypeExpr| and |Visibility| from |FuncDecl|, because they are not used in our translation.}
\label{fig:flatRep}
\end{figure}

\begin{figure}

> code (f {v} = e )              = FuncDecl f n (Rule [{v}] (code e))
> emptyspace
> code v                         = Var v
> code l                         = Lit (code l)
> code (e_1 ? e_2)               = Or (code e_1) (code e_2)
> code EXEMPT                    = Comb ConsCall ("","FAIL") []
> code (f_k {e})   | k == 0      = Comb FuncCall f [{code e}]
>                  | otherwise   = Comb (FuncPartCall k) f [{code e}]
> code (C_k {e})   | k == 0      = Comb ConsCall f [{code e}]
>                  | otherwise   = Comb (ConsPartCall k) f [{code e}]
> code (let {v = e} in e')       = Let [{(v,code e)}] (code e')
> code (let {v} free in e)       = Free [{v}] (code e)
> code (case e of alts)          = Case e (code alts)
> emptyspace
> code (l -> e)                  = Branch (LPattern (code l)) (code e)
> code (C {v} -> e)              = Branch (Pattern C [{v}]) (code e)
> emptyspace
> code l   | isInt l    = Intc l
>          | isFloat l  = Floatc l
>          | isChar l   = Charc l
\caption{Translation from FlatCurry syntax to the Curry representation of FlatCurry.}
\label{fig:flatTranslate}
\end{figure}



> float (Comb ct f (as++[Let vs e]++bs)) = Let vs (Comb ct f (as++[e]++bs))

This optimization tells us that, if an argument to a function application is a |let| expression,
then we can move the let-expression outside.
This works for let-expressions, but what if there's a free variable declaration inside of a function?
Well, we can define that case with another rule.

> float (Comb ct f (as++[Let vs e]++bs))   = Let vs (Comb ct f (as++[e]++bs))
> float (Comb ct f (as++[Free vs e]++bs))  = Free vs (Comb ct f (as++[e]++bs))

This is where the non-determinism comes in.
Suppose we have an expression:

> f (let x = 1 in x) (let r free in 2)

This could be matched by either rule.
The trick is that we don't care which rule matches, as long as they both do eventually.
This will be transformed into one of the following:

> let r free in let x = 1 in f x 2
> let x = 1 in let r free in f x 2

Either of these options is acceptable.
In fact, we could remove the ambiguity by making our rules a confluent system,
as shown by the code below.
However, we will not worry about confluence for most optimizations.

> float (Comb ct f (as++[Let vs e]++bs))   = Let vs (Comb ct f (as++[e]++bs))
> float (Comb ct f (as++[Free vs e]++bs))  = Free vs (Comb ct f (as++[e]++bs))
> float (Let vs (Free ws e))               = Free ws (Let vs e)

Great, now we can make an optimization.
It was easy to write, but it's not a very complex optimization.
In fact, most optimizations we write won't be very complex.
The power of optimization comes from making small improvements several times.

Now that we can do simple examples, let's look at a more substantial transformation.
Let-expressions are deceptively complicated.
They allow us to make arbitrarily complex, mutually recursive, definitions.
However, most of the time a large let expression could be broken down into several small let expressions.
Consider the definition below:
> let  a = b
>      b = c
>      c = d + e
>      d = b
>      e = 1
> in   a

This is a perfectly valid definition,
but we could also break it up into the three nested let expressions below.

> let  e = 1
> in   let  b = c
>           c = d + e
>           d = b
>      in   let  a = b
>           in   a

It's debatable which version is better coding style, but the second version
is inarguably more useful for the compiler.
There are several optimizations that can be safely performed on a single let bound variable.
Unfortunately, splitting the let expression into blocks isn't trivial.
The algorithm involves making a graph out of all references in the let block,
then finding the strongly connected components of that reference graph,
and, finally, rebuilding the let expression from the component graph.
The full algorithm is given below in figure \ref{fig:blocks}

\begin{figure}
> blocks (Let vs e) | numBlocks > 1 = e'
>   where (e', numBlocks) = makeBlocks es e

> makeBlocks vs e = (letExp, length comps)
>  where  letExp = foldr makeBlock e comps
>         makeBlock comp = \exp -> Let (map getExp comp) exp
>         getExp (_++[(n,exp)] ++ _) = (n,exp)
> 
>         comps = scc (vs >>= makeEdges)
>         makeEdges (v, exp) = [(v,f) | f <- freeVars exp `intersect` map fst vs]
\caption{Transformation for splitting let expressions into mutually recursive blocks.}
\label{fig:blocks}
\end{figure}

While this optimization is significantly more complicated then the |float| example,
We can still implement it in our system.
Furthermore, we're able to factor out the code for building the graph
and finding the strongly connected components.
This is the advantage of using Curry functions as opposed to strict rewrite rules.
We have much more freedom in constructing the right-hand side of our rules.

Now that we can create optimizations, what if we want both |blocks| and |float| to be able to run?
This is an important part of the compilation process to get expressions into a canonical form.
It turns out that combining two optimizations is simple.
We just make a non-deterministic choice between them.

> floatBlocks = float ? blocks

This is a new optimization that will apply either |float| or |blocks|.
The ability to compose optimizations with |?| is the heart of the GAS system.
Each optimization can be developed and tested in isolation,
then they can be combined for efficiency.

\subsection{An Initial Attempt}

Our first attempt is quite simple, really.
We pick an arbitrary subexpression with |subExpr|
and apply an optimization.
We can then use a non-deterministic fix point operator to find
all transformations that can be applied to the current expression.
We can define the non-deterministic fix point operator using either
the Findall library, or Set Function \cite{findall, setFunctions}.
The full code is given in figure \ref{fig:optFirst}.

\begin{figure}
> fix :: (a -> a) -> a -> a
> fix f x
>  | f x == emptyset  = x
>  | otherwise        = fix f (f x)

> subExpr :: Expr -> Expr
> subExpr e               = e
> subExpr (Comb ct f vs)  = subExpr (foldr1 (?) es)
> subExpr (Let vs e)      = subExpr (foldr1 (?) (e : map snd es))
> subExpr (Free vs e)     = subExpr e
> subExpr (Or e_1 e_2)    = subExpr e_1 ? subExpr e_2
> subExpr (Case e bs)     = subExpr (e : map branchExpr bs)
>  where  branchExpr (Branch _ e) = e

> reduce :: Opt -> Expr -> Expr
> reduce opt e = opt (subExpr e)

> simplify :: Opt -> Expr -> Expr
> simplify opt e = fix (reduce opt) e
\caption{A first attempt at an optimization engine.
         Pick an arbitrary subexpression and try to optimize it.}
\label{fig:optFirst}
\end{figure}

While this attempt is short and readable,
there's a problem with it.
It is unusably slow.
While looking at the code, it's pretty clear to see what the problem is.
Every time we traverse the expression, we can only apply a single transformation.
This means that if we need to apply 100 transformations, which is not uncommon,
then we need to traverse the expression 100 times.

\subsection{A Second Attempt: Multiple Transformations Per Pass}

Our second attempt runs much faster.
Instead of picking an arbitrary subexpression,
we choose to traverse the expression manually.
Now, we can check at each node if an optimization applies.
We only need to make two changes.
The biggest is that we eliminate |subExpr| and change |reduce| to traverse the entire expression.
Now |reduce| can apply an optimization at every step.
We've also made |reduce| completely deterministic.
The second change is that since |reduce| is deterministic, we can change |fix|
to be a more traditional implementation of a fix point operator.
The new implementation is given in figure \ref{fig:optReduce}

\begin{figure}

> fix :: (a -> a) -> a -> a
> fix f x
>  | f x == x = x
>  | otherwise = fix f (f x)
> emptyspace
> reduce :: Opt -> Expr -> Expr
> reduce opt (Var v)             = runOpts opt (Var v)
> reduce opt (Lit l)             = runOpts opt (Lit l)
> reduce opt (Comb ct f es)      = runOpts opt (Comb ct f (map (reduce opt) es))
>  where  es' = map (reduce opt) es
> reduce opt (Let vs e)          = runOpts opt Let (map runLet vs) (reduce opt e)
>  where  runLet (v,e) = (v, reduce opt e)
> reduce opt (Free vs e)         = runOpts opt (Free vs (reduce opt e))
> reduce opt (Or a b)            = runOpts opt (Or (reduce opt a) (reduce opt a))
> reduce opt (Case e bs)         = runOpts opt (Case (reduce opt e) bs')
>  where  runBranch (Branch p e) = Branch p (reduce opt e)
>         bs' = map runBranch bs
> emptyspace
> runOpts :: Opt -> Expr -> Expr
> runOpts opt e =  case  oneValue (opt e) of
>                        Nothing  -> e
>                        Just e'  -> e'
> emptyspace
> simplify :: Opt -> Expr -> Expr
> simplify opt e = fix (reduce opt) e
\caption{A second attempt.
         Traverse the expression and, at each node, check if an optimization applies.}
\label{fig:optReduce}
\end{figure}

This approach is significantly better.
Aside from applying multiple rules in one pass,
we also limit our search space when applying our optimizations.
While there's still more we can do,
the new approach makes the GAS library usable on larger Curry programs,
like the standard Prelude.

\subsection{Adding More Information}

Rather surprisingly our current approach is actually sufficient for compiling FlatCurry.
However, to optimize Curry we're going to need more information when we apply a transformation.
Specifically, we'll be able to create new variables.
To simplify optimizations, we'll require that each variable name can only be used once.
Regardless, we need a way to know what is a safe variable name that we're allowed to use.
We may also need to know if we're rewriting the root of an expression.
Fortunately, all we need to change is to define |Opt| to accept more parameters.
For each optimization, we'll pass in an |n :: Int|
that represents the next variable |v_n| that is guaranteed to be fresh.
We'll also pass in a |top :: Bool| that tells us if we're at root of a function we're optimizing.
We also return a pair of |(Expr,Int)| to denote the optimized expression,
and the number of new variables we used.

> type Opt = (Int,Bool) -> Expr -> (Expr,Int)

If we later decide that we want to add more information, then we just update the first parameter.
The only problem is, how do we make sure we're calling each optimization with the correct |n| and |top|?
We just need to update |reduce| and |runOpt|.
In order to keep track of the next available free variable we use the |State| monad.
We do need to make minor changes to |fix| and |simplify|,
but this is just to make them compatible with |State|.
The full implementation is in figure \ref{fig:optAddInfo}.


\begin{figure}
> reduce :: Opt -> Bool -> Expr -> State Int Expr
> reduce opt top (Var v)          = runOpts opt top (Var v)
> reduce opt top (Lit l)          = runOpts opt top (Lit l)
> reduce opt top (Comb ct f es)   = do  es' <- mapM (reduce opt False es)
>                                       runOpts opt top (Comb ct f es')
> reduce opt top (Let vs in e)    = do  vs'  <- mapM runVar vs
>                                       e'   <- mapM reduce opt False e
>                                       runOpts opt top (Let vs' in e')
>  where  runVar (v, e) = do  e' <- reduce opt False e
>                             return (v, e')
> reduce opt top (Free vs e)      = do  e' <- reduce opt False e
>                                       runOpts opt top (Free vs e')
> reduce opt top (Or a b)         = do  a'  <- reduce opt False a
>                                       b'  <- reduce opt False b
>                                       runOpts opt (Or a' b')
> reduce opt top (Case e bs)      = do  e'    <- reduce opt False e
>                                       bs'   <- mapM runBranch bs
>                                       runOpts opt (Case e' bs')
>  where runBranch (Branch pat e)  = do  e' <- reduce opt False e
>                                        return (Branch pat e')
> emptyspace
> runOpts :: Opt -> Bool -> Expr -> State Int Expr
> runOpts opt top e =  do  v <- get
>                          case  opt (v,top) e of
>                                Nothing       -> return e
>                                Just (e',dv)  -> do  put (v+dv)
>                                                     return e'
> fix :: (a -> State b a) -> a -> b -> a
> fix f x s =  let  (x',s') = runState (f x) s
>              in   if x == x' then x else fix f x' s'
\caption{A third attempt.
         keep track of the next fresh variable, and if we're at the root.}
\label{fig:optAddInfo}
\end{figure}


\subsection{Reconstruction}

Right now we have everything we need to write all of our optimizations.
However, we've found it useful to be able to 
track which optimizations were applied and where they were applied.
This helps with testing, debugging, and designing optimizations,
as well as generating optimization derivations that we'll see later in this dissertation.
It is difficult to overstate just how helpful this addition was in building this compiler.

If we want to add this, then we need to make a few changes.
First, we need to decide on a representation for a rewrite derivation.
Traditionally a rewrite derivation is a sequence of rewrite steps,
where each step contains the rule and position of the rewrite.
We describe paths in figure \ref{fig:path}.
To make reconstruction easier, we also include the expression that is the result of the rewrite.
This gives us the type:
> type Path = [Int]
> type Step = (String, Path, Expr)
> type Derivation = [Step]


\begin{figure}
> ndpath e                 = []
> ndpath (Comb ct f es)    = anymap argPath es
>  where argPath (i,e) = i : ndpath e_i
> ndpath (Or e_0 e_1)      = 0 : ndpath e_0 ? 1 : ndpath e_1
> ndpath (Let vs e_n1)     = anymap letPath es
>                          ? -1 : ndpath e_n1
>  where letPath (i,(_, e)) = i : ndpath e
> ndpath (Case e of alts)  = -1 : ndpath e
>                          ? anymap altPath alts
>  where altPath (i,Branch _ e) = i : ndpath e
> emptyspace
> anymap f = anyof . map f . zip [1..]
\caption{The definition of a path for Curry expressions.\\
         This function non-deterministically returns a path to a subexpression.}
\label{fig:path}
\end{figure}


This leads to the last change we need to make to our |Opt| type.
We need each optimization to also tell us its name.
This is good practice in general, 
because it forces us to come up with unique names for each optimization.

> type Opt = (Int,Bool) -> Expr -> (Expr, String, Int)

We only need to make a few change to the algorithm.
Instead of using the |State| monad, we use a combination of the |State| and |Writer| monads,
so we can keep track of the derivation.
We've elected to call this the |ReWriter| 
monad.
We add a function |update :: Expr -> Step -> Int -> ReWriter Expr| 
that is similar to |put| from |State|.  This updates the state variable, and creates a single step.
The |reduce| function requires few changes.
We change the Boolean variable |top| to a more general |Path|.
Because of this change, we need to add the correct subexpression position,
instead of just changing |top| to |False|.
The |RunOpts| function is similar.  We just change |top| to a |Path|,
and check if it's null.
Finally |fix| and |simplify| are modified to remember the rewrite steps we've already computed.
We change the return type of |simplify| so that we have the list of steps.
The full implementation is in figure \ref{fig:reconstruct}

\begin{figure}
> reduce :: Opt -> Path -> Expr -> ReWriter Expr
> reduce opt p (Var v)         = return (Var v)
> reduce opt p (Lit l)         = return (Lit l)
> reduce opt p (Comb ct f es)  = do  es' <- mapM runArg (zip [0..] es)
>                                    runOpts opt p (Comb ct f es)
>  where  runArg (n, e)        = reduce opt (n:p) e
> reduce opt p (Let vs e)      = do  vs'  <- mapM runVar (zip [0..] vs)
>                                    e'   <- mapM reduce opt (-1:p) e
>                                    runOpts opt p (Let vs' e')
>  where  runVar (n, (v, e))   = fmap (\x -> (v,x)) (reduce opt (n:p) e)
> reduce opt p (Free vs in e)  = do  e' <- reduce opt (0:p) e
>                                    runOpts opt p (Free vs e')
> reduce opt p (Or a b)        = do  a  <- reduce opt (0:p) a
>                                    b  <- reduce opt (1:p) b
>                                    runOpts opt (Or (a' ? b'))
> reduce opt p (Case e bs)     = do  e'    <- reduce opt (-1:p) e
>                                    bs'   <- mapM runBranch (zip [0..] bs)
>                                    runOpts opt (Case e' bs')
>  where runBranch (n, (Branch pat e))  = fmap (Branch pat) (reduce opt (n:p) e)
>         
> emptyspace
> runOpts :: Opt -> Path -> Expr -> ReWriter Expr
> runOpts opt p e =  do  v <- get
>                        case  oneValue (opt (v, null p)) of
>                              Nothing            ->  return e
>                              Just (e',rule,dv)  ->  do update (e',rule,p) dv
>                                                     return e'
> fix :: (a -> ReWriter a) -> a -> Int -> [Step] -> (a,[Step])
> fix f x n steps =  let  (x',n',steps') = runRewriter (f x) n
>                    in   if x == x' then x else fix f x' n' (steps++steps')
\caption{The final version of GAS with reconstruction.}
\label{fig:reconstruct}
\end{figure}

Now that we've computed the rewrite steps, it's a simple process to reconstruct them into a string.
The |pPrint| function comes from the FlatCurry Pretty Printing Library.

> reconstruct :: Expr -> [Step] -> String
> reconstruct _ [] = ""
> reconstruct e ((rule, p, rhs):steps) =  let e' = extend e p rhs
>                                         in  "=>_"++rule++" "++(show p)++"\n" ++
>                                             pPrint e' ++"\n" ++
>                                             reconstruct e' steps


Now that our optimization engine is running and printing out optimization derivations,
there are a few tricks we can use to improve the efficiency.
Remember that our optimizing engine is going to run for every optimization,
so it's worth taking the time to tune it to be as efficient as possible.
The first trick is really simple.  We add a Boolean variable |seen| to the ReWriter monad.
This variable starts as |False|, and we set it to |True| if we apply any optimization.
This avoids the linear time check for every call of |fix| to see if we actually ran any optimizations.
The second quick optimization is to notice that variables, literals, and type expressions 
are never going to run an optimization, 
so we can immediately return in each of those cases without calling |runOpt|.
This is actually a much bigger deal than it might first appear.
All of the leaves are going to either be variables, literals, or constructors applied to no arguments.
For expression trees the leaves are often the majority of the nodes in the tree.
Finally, we can put a limit on the number of optimizations to apply.
If we ever reach that number, then we can immediately return.
This can stop our optimizer from taking too much time.

Now that the GAS system is in place, we can move onto compiling FlatCurry programs.
In This chapter we've introduced the GAS system that allows us to represent transformations 
In a simple form that's easy to extend and test.
We've seen how we can represent an optimization as a function from expressions to expressions.
Then we showed that we can extend this idea to create more powerful optimizations,
and automatically generate optimization derivations.
In the next chapter we put this system to work.
We use the GAS system to implement several transformations to turn FlatCurry code
in to a form that can be more easily compiled.
Then we show how to generate efficient C code for FlatCurry programs.
We also introduced the idea

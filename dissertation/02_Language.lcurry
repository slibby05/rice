
\section{The Curry Language}

In order to write a compiler for Curry, we need to understand how Curry works.
We'll start by looking at some examples of Curry programs.
We'll see how Curry programs differ from Haskell and Prolog programs.
Then we'll move on to defining a small interpreter for Curry.
Finally we'll use this interpreter to define equivalent C code.

Curry combines the two most popular paradigms of declarative programming:
Functional languages and logic languages.
Curry programs are composed of defining equations like Haskell or ML,
but we are allowed to have non-deterministic expressions and free variables like Prolog.
This will not be an introduction to modern declarative programming languages.
The reader is expected to be familiar with functional languages such as Haskell or ML,
and logic languages such as Prolog.
For an introduction to programming in Curry see \cite{CurryTutorial}.
For an exhaustive explanation of the syntax and semantics of Curry see \cite{CurryReport}.

To demonstrate the features of Curry, we will examine a small Haskell program to permute a list.
Then we will simplify the program by adding features of Curry.
This will demonstrate the features of Curry that we need to handle in the compiler,
and also give a good basis for how we can write the compiler.

First, let's consider an example of a permutation function.
This is not the only way to permute a list in Haskell,
and you could easily argue that it's not the most elegant way,
but I chose it for two reasons.
There is no syntactic sugar,
and the only two library functions are |concat| and |map|, both very common functions.
and the algorithm for permuting a list is similar to the algorithm we will use in Curry.

> perms         :: [a] -> [[a]]
> perms []      = [[]]
> perms (x:xs)  = concat (map (insert x) (perms xs))
>   where
>     insert x []      = [[x]]
>     insert x (y:ys)  = (x:y:ys) : map (y:) (insert x ys)

The algorithm itself is broken into two parts.
The |insert| function will return a list of lists,
where |x| is inserted into |ys| at every possible position.
For example: |insert 1 [2,3]| returns |[[1,2,3],[2,1,3],[2,3,1]]|.
The |perms| function splits the list into a head |x| and tail |xs|.
First, it computes all permutations of |xs|, then it will insert |x| into every possible position
of every permutation.

While this algorithm is not terribly complex, it's really more complex than it needs to be.
The problem is that we need to keep track of all of the permutations we generate.
This doesn't seem like a big problem here.
We just put each permutation in a list, and return the whole list of permutations.
However, now every part of the program has to deal with the entire list of results.
As our programs grow, we will need more data structures for this plumbing, and this problem will grow too.
This is not new.
Many languages have spent a lot of time trying to resolve this issue.
In fact, several of Haskell's most successful concepts,
such as monads, arrows, and lenses, are designed strictly to reduce this sort of plumbing.

We take a different approach in Curry.
Instead of generating every possible permutation, and searching for the right one,
we will non-deterministically generate a single permutation.
This seems like a trivial difference, but its really quite substantial.
We offload generating all of the possibilities onto the language itself.

We can simplify our code with the non-deterministic \textit{choice} operator |?|.
Choice is defined by the rules:
> x ? y = x
> x ? y = y

Now our permutation example becomes a little easier.
We only generate a single permutation,
and when we insert |x| into |ys|, we only insert into a single arbitrary position.


> perm         :: [a] -> [a]
> perm []      = []
> perm (x:xs)  = insert x (perm xs)
>   where
>     insert x []      = [x]
>     insert x (y:ys)  = x:y:ys ? y : insert x ys

In many cases functions that return multiple results can lead to much simpler code.
Curry has another feature that's just as useful.
We can declare a \textit{free variable} in Curry.
This is a variable that hasn't been assigned a value.
We can then constrain the value of a variable later in the program.
In the following example |begin|, |x|, and |end| are all free variables,
but they're constrained by the guard so that |begin++[x]++end| is equal to |xs|.
Our algorithm then becomes: pick an arbitrary |x| in the list,
move it to the front, and permute the rest of the list.

> perm     :: [a] -> [a]
> perm []  = []
> perm xs
>  | xs == (begin++[x]++end) = x : perm (begin++end)
>  where begin, x, end free

Look at that.
We've reduced the number of lines of code by 25\%.
In fact, this pattern of declaring free variables, and then immediately constraining them
is used so often in Curry that we have syntactic sugar for it.
A \textit{functional pattern} is any pattern that contains a function that is not at the
root.\footnote{
    This isn't completely correct.  While the above code would fully evaluate the list,
    a functional pattern is allowed to be more lazy.
    Since the elements don't need to be checked for equality, they can be left unevaluated.
}
We can use functional patterns to simplify our |perm| function even further.


> perm                    :: [a] -> [a]
> perm []                 = []
> perm (begin++[x]++end)  = x : perm (begin++end)

Now the real work of our algorithm is a single line.
Even better, it's easy to read what this line means.
Decompose the list into |begin|, |x|, and |end|, then put |x| at the front, and permute |begin| and |end|.
This is almost exactly how we would describe the algorithm in English.

There is one more important feature of Curry.
We can let expressions fail.
In fact we've already seen it, but a more explicit example would be helpful.
We've shown how we can generate all permutations of a list by generating an arbitrary permutation,
and letting the language take care of the exhaustive search.
However, we usually don't need, or even want, every permutation.
So, how do we filter out the permutations we don't want?
The answer is surprisingly simple.  We just let expressions fail.
An expression fails if it cannot be reduced to a constructor form.
The common example here is |head []|, but a more useful example might be sorting a list.
We can build a sorting algorithm by permuting a list, and only keeping the permutation that's sorted.

> sort :: (Ord a) => [a] -> [a]
> sort xs | sorted ys = ys
>  where 
>   ys = perm xs
>   sorted []        = True
>   sorted [x]       = True
>   sorted (x:y:ys)  = x <= y && sorted (y:ys)

In this example every permutation of |xs| that isn't sorted will fail in the guard.
Once an expression has failed, computation on it stops, and other alternatives are tried.
As we'll see later on, this ability to conditionally execute a function will 
become crucial when developing optimizations.

These are some of the useful programming constructs in Curry.
While they are convenient for programming, we need to understand how they work
if we are going to implement them in a compiler.

\section{Semantics}

As we've seen, the syntax of Curry is very similar to Haskell.
Functions are declared by defining equations, and new data types are declared as algebraic data types.
Function application is represented by juxtaposition,
so |f x| represents the function |f| applied to the variable |x|.
Curry also allows for declaring new infix operators.
In fact, Curry really only adds two new pieces of syntax to Haskell, \textbf{fcase} and \textbf{free}.
However, the main difference between Curry and Haskell is not immediately clear from the syntax.
Curry allows for overlapping rules and free variables.
Specifically Curry programs are represented as Limited Overlapping Inductively Sequential (LOIS) Rewrite systems.
On the other hand, Haskell programs are transformed into non-overlapping systems.

To see the difference consider the usual definition of factorial.

> fac :: Int -> Int
> fac 0 = 1
> fac n = n * fac (n-1)

This seems like an innocuous Haskell program, 
however It's non-terminating for every possible input for Curry.
The reason is that |fac 0| could match either rule.
In Haskell all defining equations are ordered sequentially,
which results in control flow similar to the following C implementation.
\begin{verbatim}
int fac(int n)
{
    if(n == 0)
    {
        return 1;
    }
    else
    {
        return n * fac(n-1);
    }
}
\end{verbatim}
In fact, every rule with multiple defining equations follows this pattern.
In the following equations let |p_i| be a pattern and |E_i| be an expression.
> f p_1  = E_1
> f p_2  = E_2
> ...
> f p_n  = E_n
Then this is semantically equivalent to the following.

> f p_1                         = E_1
> f not p_1  && p_2             = E_2
> ...
> f not p_1  && not p_2 && p_n  = E_n

Here |not p_i| means that we don't match pattern |i|.
This ensures that we will only ever reduce to a single expression.
Specifically we reduce to the first expression where we match the pattern.


Curry rules, on the other hand, are unordered.
If we could match multiple patterns, such as in the case of |fac|, 
then we non-deterministically return both expressions.
This means that |fac 0| reduces to both |1| and |fac (-1)|.
Exactly how Curry reduces an expression non-deterministically will be discussed throughout this dissertation,
but for now we can think in terms of sets.
If the expression |e -> e_1| and |e -> e_2|,
|e_1 ->* v_1| and |e_2 ->* v_2|, then 
|e ->* {v_1, v_2}|.\footnote{This should really be thought of as a multiset, since it's possible for |v_1| and |v_2| to be the same value.}

This addition of non-determinism can lead to problems if we're not careful.
Consider the following example:\\

> coin = 0 ? 1
> double x = x + x

We would expect that for any |x|, |double x| should be an even number.
However, if we were to rewrite |double coin| using ordinary term rewriting,
then we could have the derivation.
> double coin => coin + coin => (0 ? 1) + (0 ? 1) => 0 + (0 ? 1) => 0 + 1 => 1

This is clearly not the derivation we want.
The problem here is that when we reduced |double coin|,
we made a copy of the non-deterministic expression |coin|.
This ability to clone non-deterministic expressions to get different answers
is known as run-time choice semantics. \cite{callTimeChoice}.

The alternative to this is call-time choice semantics.
When a non-deterministic expression is reduced,
all instances of the expression take the same value.
One way to enforce this is to represent expressions as graphs instead of terms.
Since no expressions are ever duplicated, all instances of |coin| will reduce the same way.
This issue of run-time choice semantics will appear throughout the compiler.


\subsection{FlatCurry}

The first step in the compiler pipeline is to parse a Curry program into \gls{FlatCurry}.
The definition is given in figure \ref{fig:flatSyntax}.
The FlatCurry language is the standard for representing Curry programs
in compilers \cite{pakcs, kics2, kics2phd, sprite}, 
and has been used to define the semantics of Curry programs \cite{currySemantics}.



The semantics of Curry have already been studied extensively \cite{currySemantics},
so we informally recall some of the more important points.
A FlatCurry program consists of datatype and function definitions.
For simplicity we assume that all programs are self contained,
because the module system isn't relevant to our work.
However, the Rice compiler does support modules.
A FlatCurry function contains a single rule, 
which is responsible for pattern matching and rewriting an expression.
Pattern matching is converted into case and choice expressions as defined in \cite{patternCase}.
A function returns a new expression graph constructed out of |let, free, f_k, C_k, ?, l, v|
expressions.

Our presentation of FlatCurry differs from \cite{currySemantics} in three notable ways.
First, function and constructor applications
contain a count of the arguments they still need in order to be fully applied.
The application |f_k e_1 e_2 ... e_n| means that |f| is applied to |n| arguments,
but it needs |k| more to be fully applied,
so the arity of |f| is |n+k|.
Second, we include |let {v} free| to represent free variables.
This was not needed in \cite{currySemantics, kics2} because free variables we translated
to non-deterministic generators.
Since we narrow free variables instead of doing this transformation, 
we must represent free variables in FlatCurry.
Finally, we add an explicit failure expression |EXEMPT| to represent a branch
that is not present in the definitional tree.
While this is meant to simply represent a failing computation,
we've also occasionally found it useful in optimization.

\subsection{Evaluation}

Each program contains a special function |main| that takes no arguments.
The program executes by reducing the expression |main| 
to a 
\gls{CNF}\footnote{This is constructor normal form, and not simply a normal form,
              because a failing expression, like |head []|, is a normal form,
              since it can't be rewritten, but it contains a function at the root.} 
as defined in figure \ref{fig:normalForm}.
Similar to Kics2, Pakcs, and Sprite, \cite{kics2, pakcs, sprite}
we compute constructor normal form by first reducing the |main| to \gls{HCF}.
That is where the expression is rooted by a constructor.
Then each child of the root is reduced to \gls{CNF}.

\begin{figure}

> f           =>  f {v} = e
> e           =>  v                                        Variable
>             |   l                                        Literal
>             |   e_1 ? e_2                                Choice
>             |   EXEMPT                                   Failed
>             |   f_k {e}                                  Function Application
>             |   C_k {e}                                  Constructor Application
>             |   let {v = e} in e                         Variable Declaration
>             |   let {v} free in e                        Free Variable Declaration
>             |   case e of {p -> e}                       Case Expression
> p           =>  C {v}                                    Constructor Pattern
>             |   l                                        Literal Pattern
\caption{Syntax definition for FlatCurry\\
This is largely the same as other presentations \cite{currySemantics,icurry}
but we have elected to add more information that will become relevant for optimizations later.
The notation |{e}| refers to a variable length list |e_1 e_2 ... e_n|.}
\label{fig:flatSyntax}
\end{figure}

\begin{figure}
> n =>  l                literal
>   |   C_k n_1 ... n_k  constructor
\caption{constructor normal forms in FlatCurry.\\
         A CNF is an expression that contains only constructor and literal symbols.
         All CNFs are normal forms in our system.}
\label{fig:normalForm}
\end{figure}

Most of the work of evaluation is reducing an expression to head constructor form.
Kics2 and Pakcs are able to transform FlatCurry programs into an equivalent rewrite
system, and reduce expressions using graph rewriting \cite{kics2, pakcs}.
The transformation simply created a new function for every nested case expression.
This created a series of tail calls for larger functions.

To see this transformation in action, we can examine the FlatCurry function |==| on lists \ref{fig:eqList}.
This function is \gls{inductively sequential}, however both Pakcs and Kics2 will transform it
into a series of flat function calls with a single case at the root.
Since this would drastically increase the number of function calls, we avoid this transformation.
It would also defeat much of the purpose of an optimizing compiler 
if we weren't allowed to inline functions.

\begin{figure}
Original FlatCurry representation of |==| on lists.
> (==) v_2 v_3 = case  v_2 of
>                    [] -> case  v_3 of
>                                [] -> True
>                                v_4 : v_5 -> False
>                    v_6 : v_7 -> case  v_3 of
>                                     [] -> False
>                                     v_8 : v_9 -> v_6 == v_8 && v_7 == v_9

Transformed FlatCurry representation of |==| on lists.
> (==) v_2 v_3 = case  v_2 of
>                    [] -> eqListNil v_3
>                    v_6 : v_7 -> eqListCons v_3 v_6 v_7
> eqListNil v_3 = case  v_3 of
>                      [] -> True
>                      v_4 : v_5 -> False
> eqListCons v_3 v_6 v_7 = case  v_3 of 
>                             [] -> False
>                             v_8 : v_9 -> v_6 == v_8 && v_7 == v_9


\caption{Transformation of FlatCurry |==| function into a flat representation
         for Pakcs and Kics2.}
\label{fig:eqList}
\end{figure}

\subsection{Non-determinism}


Currently there are three approaches to evaluating non-deterministic expression in Curry:
\gls{backtracking}, \gls{Pull-Tabbing}\cite{pulltab}, and \gls{Bubbling}\cite{bubbling}.
At this time there are complete strategies for evaluating Curry programs,
so we have elected to use backtracking.
It is the simplest to implement, and it's well understood.

In our system, backtracking is implemented in the usual way.
When an expression rooted by a node |n| with label by |f| 
is rewritten to an expression rooted by |e|,
we push the rewrite |(n,n_f,Continue)| onto a backtracking stack, where |n_f|
is a copy of the original node labeled by |f|.
If the expression is labeled by a choice |e_1 ? e_2|, and it is rewritten to the left hand side |e_1|,
then we push |(n, n_q, Stop)| onto the backtracking stack to denote that this 
was an alternative, and we should stop backtracking.

Unfortunately, while \gls{backtracking} is well defined for rewriting systems,
our representation of FlatCurry programs isn't a graph rewrite system.
This is because we don't flatten our FlatCurry functions like Pakcs and Kics2.
As an example of why FlatCurry programs aren't a graph rewriting system,
consider the FlatCurry function |weird| \ref{fig:weird}.
This function defines a local variables |x| which is used in a case expression.
If this were a rewrite system, then we would be able to translate the
|case| expression into pattern matching, but a rule can't pattern match on a locally defined variable.
We show the reduction of |wierd| in figure \ref{fig:weirdEval}.

\begin{figure}
> weird = let x = False ? True
>         in case  x of
>                  False -> True
>                  True -> False
\caption{The function |weird|\\
         This can't be expressed as rewrite rules, because the expression
         we're pattern matching on is defined locally.}
\label{fig:weird}
\end{figure}

\begin{figure}
\noindent
\begin{itemize}
   \item We start with a root |r| labeled by |weird|.
   \item Node |n_1| labeled by |?| is created with children |[False, True]|.
   \item |n_1| is rewritten to |False| and |(n_1,True,Stop)| is pushed on the backtracking stack.
   \item |r| is rewritten to |True| and |(r,weird,Continue)| is pushed on the backtracking stack.
   \item |r| is a constructor normal form.
   \item backtracking to the closest alternative.
   \item The backtracking stack is |[(r,weird,Continue), (n_1, True, Stop)]|.
   \item reduce |r|.
   \item Node |n_2| labeled by |?| is created with children |[False, True]|.
   \item $\ldots$
\end{itemize}
\caption{Evaluation of the function |weird|.}
\label{fig:weirdEval}
\end{figure}


We've entered an infinite loop of computing the same rewrite.
The problem is that when we're backtracking,
and replacing nodes with their original versions, we're going too far
back in the computation.
In this example, when backtracking |weird|, we want to backtrack to a point
where |x| has been created, and we just want to evaluate the case again.

We solve this problem by creating a new function for each case expression in our original function.
Figure \ref{fig:caseFuncs} show an example for |weird| and |==| which were defined above.
This is actually very similar to how Pakcs and Kics2 transformed their programs into
rewrite systems by flattening them.
The difference is that we don't need to make any extra function calls unless
we are already backtracking.
There is no efficiency cost in either time or space with our solution.
The only cost is a little more complexity in the code generator,
and an increase in the generated code size.
This seems like an acceptable trade off, 
since our programs are still similar in size to equivalent programs compiled with GHC.

\begin{figure}
> weird =  let x = False ? True
>          in case  x of
>                   False -> True
>                   True -> False
>
> weird_1 x = case  x of
>                   False -> True
>                   True -> False
>
> (==) v_2 v_3 = case  v_2 of
>                      []         -> case  v_3 of
>                                          [] -> True
>                                          v_4 : v_5 -> False
>                      v_6 : v_7  -> case  v_3 of
>                                          [] -> False
>                                          v_8 : v_9 -> v_6 == v_8 && v_7 == v_9
>
> eqList_1 v_3 = case  v_3 of
>                      [] -> True
>                      v_4 : v_5 -> False
>
> eqList_2 v_6 v_7 v_3 = case  v_3 of
>                              [] -> False
>                              v_8 : v_9 -> v_6 == v_8 && v_7 == v_9
\caption{Functions at case for |weird| and |==| for lists.}
\label{fig:caseFuncs}
\end{figure}



As far as we're aware, this is a novel approach for improving the efficiency of backtracking
in rewriting systems.
The correctness of this method follows 
from the redex contraction theorem, which is proved later.


\subsection{Free Variables}

Free variables are similar to non-deterministic expressions.
In fact, in both Kics2 and Sprite \cite{kics2,sprite} they are replaced
by non-deterministic generators of the appropriate type \cite{freeVar}.
However, in Rice, free variables are instantiated by narrowing.
If a free variable is the scrutinee of a case expression, then 
we push copies of the remaining patterns onto the stack along with another 
copy of the variable.
If the free variable is replaced by a constructor with arguments, such as |Just|,
then we instantiate the arguments with free variables.


\begin{figure}
> data Light = Red | Yellow | Green
>
> change x = case  x of
>                  Red     -> Green
>                  Green   -> Yellow 
>                  Yellow  -> Red
\caption{A simple traffic light program}
\label{fig:light}
\end{figure}

This is easier to see with an example.
Consider the traffic light function in figure \ref{fig:light}.
The |change| function moves the light from |Red| to |Green| to |Yellow|.
When calling this function with a free variable, we have the derivation below in figure \ref{fig:lightEval}.

\begin{figure}
%{
%format free = "\Varid{free}"
\begin{itemize}
   \item We start with root |r| labeled by |change|, with a child |x| labeled by |free|.
   \item |x| is rewritten to |Red| and |(x,Green,Stop), (x,Yellow,Stop), (x,free,Continue)| 
         are all pushed on the stack
   \item |r| is rewritten to |Green|, and |(r,change, Continue)| is pushed on the stack
   \item |r| is a constructor normal form
   \item backtracking to the closest alternative
   \item backtracking stack is 
         |[(r,change,Continue), (x,Green,Stop), (x,Yellow,Stop), (x,free,Continue)]|.
   \item reduce |r|
   \item |x| is labeled by |Green|
   \item |r| is rewritten to |Yellow|, and |(r,change, Continue)| is pushed on the stack
   \item |r| is a constructor normal form
   \item backtracking to the closest alternative
   \item backtracking stack is 
         |[(r,change,Continue), (x,Yellow,Stop), (x,free,Continue)]|
   \item reduce |r|
   \item |x| is labeled by |Yellow|
   \item |r| is rewritten to |Red|, and |(r,change, Continue)| is pushed on the stack
   \item |r| is a constructor normal form
   \item backtracking to the closest alternative
   \item backtracking stack is 
         |[(r,change,Continue), (x,free,Continue)]|
   \item The stack is empty, and there are no alternatives.
\end{itemize}
%}
\caption{Evaluation of |change x where x free|}
\label{fig:lightEval}
\end{figure}

\subsection{Higher Order Functions}

Now that we have a plan for the logic features of Curry, we move on to higher order functions.
This subject has been extensively studied by the function languages community,
and we take the approach of \cite{fastCurry}.
Higher order functions are represented using defunctionalization \cite{defunctionalization}.
Recall that in FlatCurry, an expression $\glssymbol{f_k}$ represents a partial application
that is missing |k| arguments.
We introduce an |apply| function that has an unspecified arity.
where |apply f_k e_1 e_2 ... e_n| applys |f_k| to the arguments |e_1 e_2 ... e_n|.

The behavior of |apply| is specified below.

> apply f_k x_1 ... x_n
>  | k > n   = f_kn x_1 ... x_n
>  | k == n  = f x_1 ... x_n
>  | k < n   = apply (f x_1 ... x_k) x_k1 ... x_n

If the first argument |f| of |apply| is not partially applied,
then evaluate |f| until it is, and proceed as above.
In the case that |f| is a free variable, then we return |EXEMPT|, 
because we don't support higher order narrowing.

\subsection{Backtracking Performance}

Now that we've established a method for implementing non-determinism,
we would like to improve the performance.
Currently we push nodes on the backtracking stack for every rewrite.
Often, we don't need to push most of these rewrites.
Consider the following code for computing Fibonacci numbers:

> fib n = case  n < 2 of
>               True   -> n
>               False  ->  fib (n-1) + fib (n-2)
>
> main = case  fib 20 == (1 ? 6765) of
>              True -> putStrLn "found answer"

This program will compute |fib 20|, pushing all of those rewrites onto the stack as it does,
and then, when it discoverers that |fib 20 /= 1|, it will undo all of those computations,
only to redo them immediately afterwards!
This is clearly not what we want.
Since |fib| is a deterministic function, we would like to avoid pushing these rewrites onto the stack.
Unfortunately, this isn't as simple as it would first seems for two reasons.
First, determining if a function is non-deterministic in general is undecidable,
so any algorithm we developed would push rewrites for some deterministic computations.
Second, a function may have a non-deterministic argument.
For example, we could easily change the above program to:

> main = case  fib (1 ? 20) == 6765 of
>              True -> putStrLn "found answer"

Now the expression with |fib| is no longer deterministic.
We sidestep the whole issue by noticing that while it's impossible to tell 
if an expression is non-deterministic at compile time,
it's very easy to tell if it is at run time.

As far as we're aware, this is another novel solution.
Each expression contains a Boolean flag that marks if it is non-deterministic.
We called these \emph{\gls{nondet}} flags,
and we refer to an expression whose root node is marked with a nondet flag as nondet.
The rules for determining if an expression |e| is nondet are:
if |e| is labeled by a choice, then |e| is nondet;
if |e| is labeled by a function that has a case who's scrutinee is nondet,
or is a forward to a nondet, then |e| is nondet;
if |e' ->* e| and |e'| is nondet, then |e| is nondet.

Any node not marked as nondet doesn't need to be pushed on the stack
because it's not part of a choice, 
all of its case statements scrutinized deterministic nodes,
and it's not forwarding to a non-deterministic node.
However proving this is a more substantial problem.

We prove this for the class of \gls{LOIS} \gls{GRS}, 
with the understanding our system is equivalent.
This proof is based on a corresponding proof for set functions in Curry \cite{setFunctions}[Lemma 2].
The original proof was concerned with a deterministic derivation from an expression to a value.
While the idea is similar, we don't want to necessarily derive an expression to a value.
Instead we define a deterministic redex, and deterministic step below, and show that there
is an analogous theorem for a derivation of deterministic steps, even if it doesn't compute a value.

\begin{definition}
Given a rewrite system $R$ with fixed strategy $\phi$, 
a \emph{\gls{computation space}} \cite{setFunctions} of expression $e$, 
$\glssymbol{C(e)}$ is finitely branching tree defined inductively the rule
$C(e) = \langle e, C(e_1), C(e_2) \ldots C(e_n)\rangle$.
\end{definition}

We now need the notions of a deterministic redex and a deterministic rewrite.
Ultimately we want to show that if we have a deterministic reduction,
then we can perform that computation at any point without affecting the results.
One implication of this would be that performing a deterministic computation before
a non-deterministic choice was made would be the same as performing the computation
after the choice.
This would justify our fast backtracking scheme, because it would be equivalent
to performing the computation before the choice was made.

\begin{definition}
A redex $n$ in expression $e$ is deterministic if there is at most
one rewrite rule that could apply to $e\vert_n$.
A rewrite $e \to_n e'$ is deterministic if $n$ is a deterministic redex.
\end{definition}

Next we rephrase our notion of nondet for a LOIS system.

\begin{definition}
let $e \to e_1 \to \ldots v$ be a derivation for $e$ to $v$.
A node $n$ in $e_i$ is  \emph{nondet} iff\\
1. $n$ is labeled by a choice.\\
2. A node in the redex pattern \cite{?} of $n$ is \emph{nondet}.\\
3. There exists some $j < i$ where $n$ is a subexpression of $e_j$ and $n$ is \emph{nondet}.
\end{definition}

The first property is that all choice nodes are nondet.
The second property is equivalent to the condition that any node 
that scrutinizes a nondet node should be nondet.
Finally, the third property is that nondet should be a persistent attribute.
This corresponds to the definition we gave for nondet nodes above.

If $n$ is a redex that isn't marked as nondet,
then $n$ can't be labeled by a choice.
Since choice is the only rule in a LOIS system that is non-deterministic,
$n$ must be a deterministic redex.
We recall a theorem used to prove the correctness of set function.\cite{setFunctions}[Def. 1, Lemma 1]

\begin{lemma}
Given an expression $e$ where $e \to_{n_1} e_1$ and $e \to_{n_2} e_2$,
if $n_1 \ne n_2$, then there exists a $u_1$ and $u_2$ where
$t_1 \glssymbol{->^=} u_1$ and $t_2 \to^= u_2$ and $u_1 = u_2$ up to renaming of nodes.
\end{lemma}

This leads directly to our first important theorem.
If $n$ is a deterministic redex in a derivation, then we can move it earlier in the derivation.

\begin{theorem}[\gls{Redex Compression Theorem}]
if $n$ is a deterministic redex of $e$ where $n \to n'$, and $e \to e_1 \to_n e_2$.
Then there exists a derivation $e[n \to n'] \to^= e'$ where $e_2 = e'$ up to renaming of nodes.
\end{theorem}

\begin{proof}
By definition of rewriting $e \to_n e[n\to n']$.
Since $n$ is a deterministic redex, it must be the case that the redex in $e \to e_1$
was not $n$. So by the previous lemma, we can swap the order of the rewrites.
\end{proof}

Finally we show that if $a$ is a subexpression of $e$ and $a \to^* b$ using only deterministic redexes,
then $e[a \to b]$ rewites to the same values.

\begin{theorem}[\gls{Path Compression Theorem}]
if $a$ is a subexpression of $e$ and $a \to^* b$ using only deterministic rewrites,
and $e \to e_1 \to \ldots e_n$ is a derivation where $b$ is a subexpression of $e_n$,
then there is a derivation $e[a\to b] \to^* e_n$.
\end{theorem}

\begin{proof}
This follows by induction on the length of the derivation.
In the base case $a = b$, and there is nothing to prove.
In the inductive case $a \to_p a' \to^* b$.
Since $a \to_p a'$ is deterministic by assumption,
we can apply the path compression theorem and say that $e[a \to a'] \to^* e_n$.
By the inductive hypotheses we can say that $e[a \to a'][a' \to b] \to^* e_n$.
Therefore $e[a \to b] \to^* e_n$.
This establishes our result.
\end{proof}



\subsection{Collapsing Functions}

While this result is great, and it allows us to avoid creating a large number of stack frames,
there's a subtle aspect of graph rewriting that gets in the way.
If a node |n_1| labeled by function |f| is rewritten to |n_2|,
then the definition of applying a rewrite rule \cite{graphRewriting}[Def. 8, Def. 10, Def. 19]
would require us to traverse the graph, and find every node that has |n_1| as a child,
and redirect that pointer to |n_2|.
This is clearly inefficient, so this isn't done in practice.
A much faster method is to simply replace the contents, the label and children, of |n_1|
with the contents of |n_2|.
This works most of the time, but we run into a problem when
function a rewrites to a single variable, such as the |id| function.
We call these functions \emph{\gls{collapsing functions}}.
One option to solve this problem is to evaluate the contractum 
to head constructor form, and copy the constructor
to the root \cite{ImplLazyFunc}.
This is commonly used in lazy functional languages,
however it does not work for Curry programs.
Consider the expression following expression.

> f = let x = True ? False
>         y = id x
>     in not y

When |y| is evaluated, then it will evaluate |x|, and |x| will evaluate to |True|.
If we then copy the |True| constructor to |y|, then we have two copies of |True|.
But, since |y| is deterministic, we don't need to undo |y| when backtracking.
So, |y| will remain |True| after backtracking, instead of returning to |id x|.
While constructor copying is definitely invalid with fast backtracking,
it's unclear if it would be valid with a normal backtracking algorithm.

We can solve this problem by using \glspl{forwarding node}, sometimes called indirection nodes
\cite{lazyFunctionalCompilers}.
The idea is that when we rewrite an expression rooted by a collapsing function,
instead of copying the constructor, we just replace the root with a special forwarding node,
$\glssymbol{FORWARD}(x)$, where |x| is the variable that the function collapses to.

There is one more possibility to address before we move on.
One performance optimization with forwarding nodes is \gls{path compression}.
If we have a chain of forwarding nodes $FORWARD(FORWARD(FORWARD(x)))$, 
we want to collapse this to simply $FORWARD(x)$.
This is unequivocally invalid in non-deterministic backtracking systems.
Consider the following function.

> f =  let  x = True ? False
>           y = id x
>      in   case  y of
>                 False -> case  x of
>                                False -> ()

When reducing this function, we create two forwarding nodes
that are represented by the variables $x$ and $y$.
We refer to these nodes as $FORWARD_x$ and $FORWARD_y$ respectively.
While evaluating |y| in the case expression, we create two forwarding nodes
that are represented by the variables $x$ and $y$ respectively.
We refer to these nodes as $FORWARD_x$ and $FORWARD_y$ respectively.
So $x$ is reduced to $FORWARD_x(True)$, and $y$ is reduced to $FORWARD_y(FORWARD_x(True))$.
If we contract |y| to $FORWARD_y(True)$,
then when we backtrack we replace |x| with $FORWARD_x(False)$, and |y|
is replaced with $id(True)$.
The reason that $y$ doesn't change to $id(False)$ is because $y$ 
has lost its reference to $x$.
Now, not only do we fail to find a solution for |f|, we've ended up in a state
where |x| and |y| have different values.


\section{Generated Code}

Now that we've examined all of the different choices to make in constructing a compiler,
we can start to design the generated code and runtime system for the compiler.
In this section we give examples of generated code to implement Curry functions,
and discuss the low level details of the Rice runtime.
We start with a first order deterministic subset of Curry,
then we add higher order function,
finally we add non-determinism and free variables.
Throughout this section we will use \texttt{teletype font} to represent generated
C code to distinguish it from Curry or FlatCurry code.


We will introduce the generated code by looking at the |not| function defined below.
We choose this function, because it's small enough to be understandable,
but it still demonstrates most of the decisions in designing the generated code
and runtime system.

> not x = case  x of
>               False  -> True
>               True   -> False

Before we discuss generated code, 
we need to discuss expressions and the runtime system for programs.

When a FlatCurry module is compiled, it's translated into a C program.
Every function |f| defined in the FlatCurry module is compiled into
a C function that can reduce an expression, rooted by a node labeled with |f|,
into \gls{HCF}.
These functions are called \texttt{\gls{f_hnf}} for historical reasons \cite{pakcs}.

An expression in our compiled code is a rooted labeled graph.
\Glspl{node} in the graph are given the following definition.
\begin{verbatim}
typedef struct Node
{
    int missing;
    bool nondet;
    Symbol* symbol;
    field children[4];
} Node;
\end{verbatim}

A \texttt{\gls{field}} is a union of a \texttt{Node*} and the representations
of the primitive types \texttt{Int}, \texttt{Float}, and \texttt{Char},
as well as a \texttt{field*} to be described shortly.
The \texttt{\gls{children}} field contains an array of children for this node.
If a node could have more than three children, such as a node representing
the |(,,,,,,)| constructor, then \texttt{children[3]} holds a pointer
to a variable length array that holds the rest of the children.
This leads to non-uniform indexing into nodes.
For example \texttt{n->children[1]} returns the second child of the node,
but the sixth child must be retrieved with \texttt{n->children[3].a[2]}.
We use a \texttt{child\_at} macro to simplify the code,
so \texttt{child\_at(n,5)} returns the sixth child.
The \texttt{symbol} field is a pointer to the static information
of the node.
This includes the name, arity, and \gls{tag} for the node,
as well as a function pointer responsible for reducing the node to 
head constructor from.
We include a \texttt{TAG} macro to access the tag of a node.
This is purely for convenience.
For a node labeled by function |f|, this is a pointer to \texttt{f\_hnf}.
Because the calling convention is complicated, we hide this detail with an
\texttt{HNF} macro, so \texttt{HNF(f)} evaluated the node labeled by |f|
to head constructor from.
The \texttt{missing} field represents a partial function application.
If \texttt{missing} is greater than 0, then \texttt f is partially applied.
The \texttt{nondet} field represents the nondet marker described in the
fast backtracking algorithm.

Each function and constructor generates a \texttt{set} and \texttt{make} function.
For the |not| function, we would generate 

\begin{verbatim}
void set_not(field root, field x);
field make_not(field x);
\end{verbatim}

The \texttt{set\_not} function sets the \texttt{root} parameter to be a |not| node.
This is accomplished by changing the symbol and children for \texttt{root}.
The \texttt{make\_not} function allocates memory for a new |not| node.



Each program in our language defines an expression |main|,
and runs until |main| is evaluated to constructor normal form.
This evaluation is broken up into two pieces.
The primary driver of a program is the \texttt{nf} function,
which is responsible for evaluating the main expression to constructor normal form.
The \texttt{nf} function computes this form by first evaluating an expression to
head constructor form.
When an expression is in head constructor from, \texttt{nf} evaluates each subexpression
to constructor normal from, producing the following execution loop:

\begin{verbatim}
void nf(field expr)
{
    HNF(expr);

    for(int i = 0; i < expr.n->symbol->arity; i++)
    {
        nf(child_at(expr, i));
    }
}
\end{verbatim}

All that's missing here is the \texttt{hnf} functions.
We give a simplified version of the \texttt{not\_hnf} function in \ref{fig:notInit},
and we will fill in details as we progress.

\begin{figure}
\begin{verbatim}
void Prelude_not_hnf(field root)
{
  field x = child_at(root, 0);

  field scrutenee = x;
  while(true)
  {
    switch(TAG(scrutenee))
    {
      case FAIL_TAG:
      {
        fail(root);
        return;
      }
      case FORWARD_TAG:
      {
        scrutenee = child_at(scrutenee,0);
        break;
      }
      case FUNCTION_TAG:
      {
        HNF(scrutenee);
        break;
      }
      case Prelude_True_TAG:
      {
        set_Prelude_False(root, 0);
        return;
      }
      case Prelude_False_TAG:
      {
        set_Prelude_True(root, 0);
        return;
      }
    }
  }
}
\end{verbatim}
\caption{Initial implementation of |not|}
\label{fig:notInit}
\end{figure}

We can see that the main driver of this function is the \texttt{while(true)} loop.
The loop looks up the tag of \texttt x, and if it's a function tag, when we evaluate it to
head constructor form.
If the tag for \texttt x is \texttt{FAIL}, which represents an exempt node, then
we set the root to \texttt{FAIL} and return.
If the tag is \texttt{Prelude\_True} or \texttt{Prelude\_False},
we set the root to the corresponding expression, and return from the loop.
Finally, in order to implement collapsing functions, we introduce a \texttt{FORWARD} tag.
If the tag is \texttt{FORWARD}, then we traverse the forwarding chain,
and continue evaluating the \texttt x.

Finally, while we are evaluating the node stored in the local variable \texttt x,
we introduce a new variable \texttt{scrutenee}.
This is because if \texttt{x} evaluates to a forwarding node,
we need to evaluate the child of \texttt x.
If we were to update \texttt x, and then return an expression containing \texttt x
later, then we would have compressed the forwarding path.
As mentioned previously, this is not valid.

At this point we have a strategy for how to compile first order deterministic Curry functions.
Next we show how we handle partial application and higher order functions.

\subsection{Higher Order Functions}

Earlier we gave an interpretation of how to handle |apply| nodes,
but there are still a few details to work out.
Recall the semantics we gave for apply nodes:

> apply f_k [x_1, ... x_n]
>  | k > n   = f_kn x_1 ... x_n
>  | k == n  = f x_1 ... x_n
>  | k < n   = apply (f x_1 ... x_k) [x_k1, ... x_n]

If |f| is missing any arguments, then we call |f| a partial application.
Let's look at a concrete example.
In the expression |foldr_2 (pl_2)|, |foldr| is a partial application that is missing 2 arguments.
We will write this as |foldr (pl_2) \* \*| where |\*| denotes a missing argument.
Now, suppose that we want to apply the following expression.

\Tree[.|apply| [.|foldr| |+| |\*| |\*| ] |0| [.|:| |1| |...| ] ]

Remember that each node represents either a function or a constructor,
and each node has a fixed arity.
For example, |+| has an arity of 2, and |foldr| has an arity of 3.
This is true for every |+| or |foldr| node we encounter.
However, it's not true for |apply| nodes.
In fact, an |apply| node may have any positive arity.
Furthermore, by definition, an |apply| node can't be missing any arguments.
For this reason, we use the \texttt{\gls{missing}} field to hold the number of 
arguments the node is applied
to.\footnote{In reality we set missing to the 
             negative value of the arity to distinguish an 
             apply node from a partial application.}


The algorithm for reducing apply nodes is straightforward, but brittle.
There are several easy mistakes to make here.
The major problem with function application is getting the arguments in the correct positions.
To help alleviate this problem we make a non-obvious change to the structure of nodes.
We store the arguments in reverse order.
To see why this is helpful, let's consider the |foldr| example above.
But this time, let's decompose it into 3 apply nodes, so we have 
|apply (apply (apply foldr_3 (pl_2)) 0) [1,2,3]|.
In our innermost apply node, which will be evaluated first, we apply |foldr_3| to |pl_2| to get
|foldr_2 (pl_2) \* \*|.
This is straightforward.
We simply put |+| as the first child.
However, when we apply |foldr_2 (pl_2) \* \*| to |0|, we need to put |0| in the second child slot.
In general, when we apply an arbitrary partial application |f| to |x|, what child do we put |x| in?
Well, if we're storing the arguments in reverse order, then we get a really handy result.
Given function |f_k| that is missing |k| arguments, then |apply f_k x| reduces to |f_k1 x| where |x|
is the |k-1| child.
The missing value for a function tells us exactly where to put the arguments.
This is completely independent of the arity of the function.
> apply (apply (apply (foldr_3 \* \* \*) (pl_2)) 0) [1,2,3]
> => apply (apply (foldr_2 \* \* (pl_2)) 0) [1,2,3]
> => apply (foldr_1 \* 0 (pl_2)) [1,2,3]
> => foldr_0 [1,2,3] 0 (pl_2))


The algorithm is given in figure \ref{fig:apply}.
There are a few more complications to point out.
To avoid complications, we assume arguments that a function is being applied to are stored in the
array at \texttt{children[3]} of the apply node.
That gives us the structure |apply f \* \* a_n ... a_1|.
This isn't done in the runtime system because it would be inefficient,
but it simplifies the code for this presentation.
We also make use of the \texttt{set\_child\_at} macro, which simplifies setting child nodes
and is similar to \texttt{child\_at}.
Finally, the loop to put the partial function in \gls{HCF} uses
\texttt{while(f.n->missing <= 0)} instead of \texttt{while(true)}.
This is because our normal form is a partial application, which does not have its own tag.

We reduce an apply node in two steps.
First get the function \texttt f, which is the first child of an apply node.
Then, reduce it to a partial application.
If \texttt f came from a non-deterministic expression, the save the apply node on the stack.
We split the second step into two cases.
If \texttt f is under applied, or has exactly the right number of arguments,
then copy the contents of \texttt f into the root, and move the arguments over and reduce.
If \texttt f is over applied, then make a new copy of \texttt f,
and copy arguments into it until it's fully applied.
Finally we reduce the fully applied copy of \texttt f and apply the rest of the arguments.

\begin{figure}
\begin{verbatim}
void apply_hnf(field root)
{
    field f = child_at(root,0);
    field* children = root.n->children[3].a;

    while(f.n->missing <= 0)
    {
        // Normal HNF loop
    }

    int nargs = -root.n->missing;
    int missing = f.n->missing;

    if(missing <= nargs)
    {
        set_copy(root, f);
        for(int i = nargs; i > 0; i--, missing--)
        {
            set_child_at(root, missing-1, children[i-1]);
        }

        root.n->missing = missing;

        if(missing == 0)
        {
            HNF(root);
        }
    }
    else
    {
        field newf = copy(f);

        while(missing > 0)
        {
            set_child_at(newf, missing-1, children[nargs-1]);
            nargs--;
            missing--;
        }

        newf.n->missing = 0;
        HNF(newf);

        set_child_at(root,0,newf);
        root.n->missing = -nargs;
        apply_hnf(root);
    }
}
\end{verbatim}
\caption{The \texttt{apply\_hnf}  function}
\label{fig:apply}
\end{figure}


\subsection{Implementing Non-determinism}

Now, we that we can reduce a higher order functional language,
we would like to extend our implementation to handle features from logic languages.

The implementation doesn't change too much.
First we add two new tags \texttt{CHOICE} and \texttt{FREE}
to represent non-deterministic choice and free variable nodes respectively.
The choice nodes are treated in a similar manner to a function.
We call the \texttt{choose} function to reduce a choice to \gls{HCF},
and push the alternative on the stack.

The choose function in \ref{fig:choose} reduces a choice node to head constructor form.
Since choice is a collapsing rule, we return a forwarding node.
The function is also responsible for keeping track of which branch of the choice we should reduce,
and pushing the alternative on the backtracking stack.
We accomplish this by keeping a marker in the second child of a choice node.
This marker is 0 if we should reduce to the left hand side, and 1 if we should reduce to the righ
hand side.

\begin{figure}
\begin{verbatim}
void choose(field root)
{
    field choices[2] = {child_at(root,0), child_at(root,1)};
    int side = child_at(root,2).i;

    field saved;
    saved.n = (Node*)alloc(sizeof(Node));
    memcpy(saved.n, root.n, sizeof(Node));

    child_at_i(saved,2) = !side;
    stack_push(bt_stack, root, saved, side == 0);

    set_forward(root,choices[side]);
    root.n->nondet = true;

}
\end{verbatim}
\caption{Implementation of the \texttt{choose} function.}
\label{fig:choose}
\end{figure}

Free variables are more interesting.
To narrow a free variable we pick a possible constructor,
and replace the \texttt{scrutinee} node with that constructor.
All arguments to the constructor are instantiated with free variables.
Then, we push a rewrite on the stack to replace \texttt{scrutinee} with a free variable
using the \texttt{push\_frame} function.
This is because after each possible choice has been exhausted, we want to reset
this node back to a free variable in case it is used in another non-deterministic branch
of the computation.
Finally, for every other constructor, we push an alternative on the backtracking stack
using the \texttt{push\_choice} function.

The only other necessary change is to push a rewrite onto the backtracking stack
when we reach either a fail, or constructor case.
The \texttt{Prelude\_not\_1} function is a function at a case expression discussed
in the \ref{Non-determinism} section.
The changes to the |not| function are give in figure \ref{fig:notNondet}.

\begin{figure}
\begin{verbatim}
void Prelude_not_hnf(field root)
{
  field x = child_at(root, 0);

  field scrutenee = x;
  while(true)
  {
    switch(TAG(scrutenee))
    {
      case FAIL_TAG:
        push_frame(root, make_Prelude_not_1(x));
        fail(root);
        return;

      case FORWARD_TAG:
        ...
      case FUNCTION_TAG:
        ...
      case CHOICE_TAG:
        choose(scrutenee);
        break;

      case FREE_TAG:
        push(bt_stack(free_var())choose(scrutenee);
        push_frame(scrutenee, free_var());
        push_choice(scrutenee, make_Prelude_False(0));
        set_Prelude_True(scrutenee, 0);
        break;

      case Prelude_True_TAG:
        push_frame(root, make_Prelude_not_1(x));
        set_Prelude_False(root, 0);
        return;

      case Prelude_False_TAG:
        push_frame(root, make_Prelude_not_1(x));
        set_Prelude_True(root, 0);
        return;
    }
  }
}
\end{verbatim}
\caption{implementation of |not| with possible non-deterministic values.}
\label{fig:notNondet}
\end{figure}


\subsection{Fast Backtracking}

Finally we show how we implement the fast backtracking technique described earlier.
The implementation actually doesn't change much,
we simply make use of the \texttt{nondet} flag in each node.
While we're evaluating \texttt{scrutinee},
we keep track of whether or not we've seen
a non-deterministic node in a local variable, and if we have, we push
the root on the backtracking stack.
If we haven't seen a non-deterministic node,
then we can simply avoid pushing this rewrite.
The generated code for |not| is given in figure \ref{fig:notFull}.

\begin{figure}
\begin{verbatim}
void Prelude_not_hnf(field root)
{
  field x = child_at(root, 0);
  field scrutenee = x;
  bool nondet = false;

  while(true)
  {
    nondet |= scrutenee->nondet;
    switch(TAG(scrutenee))
    {
      case FAIL_TAG:
        if(nondet) push_frame(root, make_Prelude_not_1(x));
        fail(root);
        return;

      case FORWARD_TAG:
        ...
      case FUNCTION_TAG:
        ...
      case CHOICE_TAG:
        choose(scrutenee);
        nondet = true;
        break;

      case FREE_TAG:
        push_frame(scrutenee, free_var());
        push_choice(scrutenee, make_Prelude_False(0));
        set_Prelude_True(scrutenee, 0);
        nondet = true;
        break;

      case Prelude_True_TAG:
        if(nondet) push_frame(root, make_Prelude_not_1(x));
        set_Prelude_False(root, 0);
        return;

      case Prelude_False_TAG:
        if(nondet) push_frame(root, make_Prelude_not_1(x));
        set_Prelude_True(root, 0);
        return;
    }
  }
}
\end{verbatim}
\caption{implementation of |not| with possible non-deterministic values.}
\label{fig:notFull}
\end{figure}

In this chapter we've discussed the Curry language, and overviewed the semantics of Curry programs.
We've shown different approaches to implementing a system for running Curry programs,
and we've discussed the choices we've made.
When a decision needed to be made, we prioritized correctness, then efficient execution,
and then ease of implementation.

In some sense, we've given a recipe of how to translate Curry into C.
In the next chapter we introduce the tools to make this recipe.
We introduce a system for implementing transformations as rewrite rules.
We then show how this system can simplify the construction of a compiler,
and use it to transform FlatCurry programs into a form that is easier to optimize and compile to C.




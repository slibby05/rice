
In the last chapter we saw how the \gassp tool
let us write transformation rules as rewrite rules in Curry.
The power of this tool came from two aspects.
The first is that it is easy to write rules syntactically.
The second is that the rules are written in Curry,
so we are not limited by our rewriting system.
We will put this second part to use in optimizing Curry expressions.

In this chapter we outline a number of optimizations that were necessary
to implement in order for unboxing, deforestation, and shortcutting to be effective.
We start by introducing a new restriction on FlatCurry expressions
called Administrative Normal Form, or A-Normal Form.
This is a common form for functional program optimizers to take,
and it provides several benefits to Curry too.
We describe the transformation, and why it is useful,
then we detail a few smaller optimizations that move let-expressions around.
The goal is to move the let-expression to a position just before the variable is used in the expression.
Finally we discuss four optimizations that will do most of the work in the compiler:
Case canceling, dead code elimination, inlining, and reduction.
These optimization are an important part of any optimizing compiler,
but they are often tricky to get right.
In fact, with the exception of dead code elimination, 
It is not clear at all that they are even valid for Curry.
We show an effective method to implement them
in a way that they remain valid for Curry expressions.

In this chapter we discuss one of the major hurdles to optimizing 
FlatCurry programs,
we then present a solution in A-Normal form,
We do on to develop some standard optimizations
for FlatCurry including dead code elimination, case canceling, and inlining.
Finally, we show these optimizations at work optimizing the implementation
the implementation |<=| for the |Bool| type.

\section{A-Normal Form} \label{sec:opts:anf}

Before we discuss any substantial optimizations, we need to deal with a significant
roadblock to optimizing Curry.
Equational reasoning,
in the sense of replacing expressions with their derived values,
is not valid when optimizing FlatCurry programs.
The reason is that expressions in FlatCurry are not referentially transparent \cite{whyFPmatters}.
The evaluation of Curry programs is graph rewriting,
which maintains referential transparency,
but since FlatCurry is composed of terms, and not graph,
we can not substitute expressions with their values.

While there have been graph intermediate representation proposed for
languages \cite{graph_ir, dactl} FlatCurry is not one of these.
We do think that incorporating the graph based IR
might improve the optimization process, and we believe
it is a promising area of future work.

To see an example of why this an issue, let us consider the following program.

\begin{minipage}{\textwidth}
> double x  = x + x
> main      = double (0 ? 1)
\end{minipage}

In pure lazy functional languages, it is always safe to replace a function
with its definition.
So we should be able to rewrite |main| to |(0 ? 1) + (0 ? 1)|,
but this expression will produce a different set of answers.
This is the primary problem with optimizing functional logic languages,
but exactly why this happens is a bit tricky to pin down.
The non-determinism is not the only problem, for example evaluating |id (0 ? 1)| 
at compile time is fine.
We can even duplicate non-deterministic expressions with the following example.

\begin{minipage}{\textwidth}
> double x  = x + x
> main      =  let y = (0 ? 1)
>              in double y
\end{minipage}

Here |y| is a non-deterministic expression, because it produces two answers when evaluated,
but the expression |let y = (0 ? 1) in y + y| is still equivalent to our example.
The real problem with our first example is a bit more subtle,
and we have to step back into the world of graph rewriting.
If we construct the graph for the first expression we see:

\begin{minipage}{\textwidth}
\begin{mdframed}
\centerline{
  \graphxy{
      & \xynode{$double$} \xyS{d} & \\
      & \xynode{$?$} \xyD{dl} \xyU{dr} & \\
      \xynode{$0$} & & \xynode{$1$} \\
  }
  \hspace*{2em}
  $\Rightarrow$
  \hspace*{2em}
  \graphxy{
      & \xynode{$+$} \xyD{dl} \xyU{dr} & \\
      \xynode{$?$} \xyD{d} \xyD{drr} & & \xynode{$?$} \xyU{dll} \xyU{d} \\
      \xynode{$0$} & & \xynode{$1$}
  }

}
\end{mdframed}

\begin{mdframed}
\centerline{
  \graphxy{
      & \xynode{$double$} \xyS{d} & \\
      & \xynode{$?_y$} \xyD{dl} \xyU{dr} & \\
      \xynode{$0$} & & \xynode{$1$} \\
  }
  \hspace*{2em}
  $\Rightarrow$
  \hspace*{2em}
  \graphxy{
      & \xynode{$+$} \xyD{d} \xyU{d} & \\
      & \xynode{$?_y$} \xyD{dl} \xyU{dr} & \\
      \xynode{$0$} & & \xynode{$1$}
  }

}
\end{mdframed}
\end{minipage}

Now the real issue comes to light.
In the second example, while we copied a non-deterministic expression in the code,
we did not copy the non-deterministic expression in the graph.
This gives us a powerful tool when reasoning about Curry expressions.
Even if a variable is duplicated in the source code, it is not copied in the graph.
Since this duplication of non-deterministic expressions was the main concern for correctness,
the solution is pretty straightforward.
If we copy an expression in FlatCurry, then we should instead store that expression
in a variable and copy the variable.

We can enforce this restriction by disallowing any compound expressions.
Specifically, all function calls, constructor calls, choices, and case expression must either
be applied to literal values or variables.
Fortunately we are not the first to come up with this idea.
In fact this restricted form is used in many functional compilers,
and is known as Administrative Normal Form (ANF) \cite{ANormal}\index{Administrative Normal Form (ANF)}.
The idea originally was to take CPS, another well known intermediate representation
for functional languages, and remove common ``administrative redexes''.
After removing the administrative redexes, we can remove the continuations,
and rewrite the program using let-expressions.
Flanagan et al. showed that these transformations can be reduced into a single
A-Normal form transformation.
We give the definition of A-Normal Form for Curry programs in figure
\ref{fig:anormal}
and we implement the transformation using \gassp in figure
\ref{fig:anormalTransform} with the Curry implementation in Figure \ref{fig:anormalCurry}.

\begin{boxfigure}
\textbf{ANF:}
> a  =>  v                       Variable
>    |   l                       Literal
>    |   EXEMPT                  Failed
> e  =>  a_1 ? a_2               Choice
>    |   f_k (vec a)             Function Application
>    |   C_k (vec a)             Constructor Application
>    |   let (vec v = e) in e    Variable Declaration
>    |   let (vec v) free in e   Free Variable Declaration
>    |   case a of (vec alt)     Case Expression
\caption[Administrative Normal Form]{
Restricting Curry expressions to A-Normal Form.\\
An atom is either a variable, a literal, or a failure.
Compound expressions are only allowed to contain atoms.}
\label{fig:anormal}
\end{boxfigure}

\begin{boxfigure}
> case e of alts              ==> let x = e    in case x of alts
> f a_1 a_2 ... e_k ... e_n   ==> let x = e_k  in f a_1 a_2 ... x ... e_n
> C a_1 a_2 ... e_k ... e_n   ==> let x = e_k  in C a_1 a_2 ... x ... e_n
> e_1 ? e_2                   ==> let x = e_1  in x ? e_2
> a_1 ? e_2                   ==> let x = e_2  in a_1 ? x
\caption[The ANF Transformation]{
    Rules for transforming Curry expression to A-Normal Form.\\
    |a| is used for atoms, |e| is used for arbitrary expressions,
    and |x| is a fresh variable name.  
    }
\label{fig:anormalTransform}
\end{boxfigure}
\begin{boxfigure}
> toANF :: Opt
> toANF (n,_) (Case ct e bs)
>  | not (trivial e) = Let [(n,e)] (Case ct (Var n) bs)
> 
> toANF (n,_) (Comb ct f (as++[e]++bs))
>  | all trivial as && not (trivial e) = Let [(n,e)] (Comb ct f (as++[Var n]++bs))
> 
> toANF (n,_) (Or e_1 e_2)
>  | not (trivial e_1) = Let [(n,e_1)] (Or (Var n) e_2)
>  | not (trivial e_2) = Let [(n,e_2)] (Or e_1 (Var n))
\caption[The ANF Transformation Implementation]{
    Curry implementation of A-Normal Form transformation.
    }
\label{fig:anormalCurry}
\end{boxfigure}

As long as we enforce this A-Normal Form structure,
we restore equational reasoning for Curry programs.
We do not even need to enforce A-Normal Form strictly here.
During optimization, it is often useful to be able to replace
variable bound to constructors and partial applications with their definitions.
Since these nodes have no rewrite rules that can apply at the root,
we can do this replacement without fear of problems with non-deterministic expressions.
This will be referred to as limited A-Normal Form.

In fact, this is exactly how the operational semantics were defined for FlatCurry.
In \cite{currySemantics} FlatCurry programs are translated into
a normalized form before evaluation begins.
We choose to flatten these expressions as well because
it produces more uniform programs,
and more optimizing transformations become valid.
Some examples of programs in ANF are given in Figure \ref{fig:anormalExample}.

\begin{boxfigure}
\fbox{
\begin{minipage}{.40\textwidth}
> fib n =  case n < 1
>                True   ->  n
>                False  ->  fib (n-1) + 
>                           fib (n-2)
> 
> emptyspace
> emptyspace
> emptyspace
> emptyspace
\end{minipage} |==>*|
\begin{minipage}{.45\textwidth}
> fib n =  let  x = n < 1
>          in   case x of
>                     True   ->  n
>                     False  ->  let  n_1 = n-1
>                                     n_2 = n-2
>                                     f_1 = fib n_1
>                                     f_2 = fib n_2
>                                in   f_1 + f_2
\end{minipage}
}
\fbox{
\begin{minipage}{.40\textwidth}
> sumPrimes  = foldr (+) 0
>            . filter isPrime
>            . enumFromTo 1
>
>
>
>
\end{minipage} |==>*|
\begin{minipage}{.45\textwidth}
>   sumPrimes =  let v_1 = (+)
>                in let v_2 = foldr v_1 0
>                in let v_3 = isPrime
>                in let v_4 = filter v_3
>                in let v_5 = enumFromTo 1
>                in let v_6 = v_4 . v_5
>                in v_2 . v_6
\end{minipage}
}
\caption[ANF Examples]{Examples of Curry programs translated to A-Normal Form}
\label{fig:anormalExample}
\end{boxfigure}

\section{Case Canceling} \label{sec:opts:case}

Finally, we come to our first example of an optimization.
In fact, this is arguably our most important optimization.
It is a very simple optimization, but it proves to be very powerful.
Consider the following code:

\begin{minipage}{\textwidth}
> notTrue = case  True of
>                 True -> False
>                 False -> True
\end{minipage}

Expressions like this come up frequently during optimization.
This is fantastic, because it is clear what we should do here.
We know that the |True| branch will be taken, 
so we might as well evaluate the case expression right now.

> notTrue = False

This transformation is called Case Canceling,
and it is the workhorse of all of our other optimizations.
The transformation is given and \ref{fig:caseCode} 
and examples of the transformation are given in \ref{fig:CaseCancel}.
If the scrutinee of a case is labeled by a constructor,
then we find the appropriate branch, and reduce to that branch.
The only real complication is that we need to keep the expression in
A-Normal form.
However, we can simply add let-expressions for every variable that the constructor binds.

We also include two other optimizations.
These optimizations are really about cleaning up after Case Canceling runs.
The first is Case Variable elimination.
Consider the expression from the optimization of |compare| for |Bool| in Figure \ref{fig:caseBool}.
\begin{boxfigure}
>     in case  v_2 of
>              True   -> LT
>              False  -> case  appOpt v_2 of
>                              True   -> EQ
>                              False  -> case  v_1 of
>                                              True   -> GT
>                                              False  -> EQ
> ==> CASE_VAR [-1,0,-1]
>     in case  v_2 of
>              True   -> LT
>              False  -> case  appOpt False of
>                              True   -> EQ
>                              False  -> case  v_1 of
>                                              True   -> GT
>                                              False  -> EQ
> ==> CASE_CANCEL [-1,0,-1,1]
>     in case  v_2 of
>              True   -> LT
>              False  -> case  v_1 of
>                              True   -> GT
>                              False  -> EQ
> ...
\caption[Optimization Deriviation Example]{a piece of the optimization derivation for the implementation of |compare|
for |Bool|.}
\label{fig:caseBool}
\end{boxfigure}
The use of Case Variable elimination allows us to set up
a situation where a case can cancel later.
This occurs a lot in practice, but this optimization may raise red flags for some.
In general it is not valid to replace a variable with an expression in FlatCurry.
That variable could be shared, and it could represent a non-deterministic expression.
Fortunately, this is still viable in Curry.

We give a short sketch of why Case Variable elimination is viable in Curry with the following example.
Suppose I have the following FlatCurry definition for |notHead|.
This function will look at the first element of a list, and return |not True|
if the head of the list evaluates to True.

\begin{minipage}{\textwidth}
> notHead xs = case  xs of
>                    x:_ -> case  x of
>                                 True -> not x
\end{minipage}

We use a key fact from Brassels work \cite{Kics2Theory}[Lemma 4.1.10].
Lifting a case into it is own function Does not change the set of values
an expression evaluates to.
We can use this to lift the inner case into it is own function.

\begin{minipage}{\textwidth}
> notHead xs = case  xs of
>                    x:_ -> notHead_1 x
> notHead_1 x = case x of
>                    True -> not x
\end{minipage}

Since uniform programs can be viewed as inductively sequential rewrite systems.
The function |notHead_1| should be equivalent to the following Curry program.

\begin{minipage}{\textwidth}
> notHead_1 True = not True
> notHead_1 False = EXEMPT
\end{minipage}

Now this program could be compiled into the following semantically equivalent FlatCurry program.

\begin{minipage}{\textwidth}
> notHead_1 x = case  x of
>                     True   -> not True
>                     False  -> EXEMPT
\end{minipage}

Finally, by the path compression theorem we can reduce the call in |notHead| to get the following result.

\begin{minipage}{\textwidth}
> notHead xs = case  xs of
>                    x:_ -> case  x of
>                                 True -> not True
\end{minipage}

This gives us a general procedure for converting FlatCurry programs to the same program
after performing Case Variable elimination.
While we do not perform these steps in practice, each one has already been shown to be valid
on their own, so our transformation is also valid.

Finally we have Dead Code Elimination.
This is a standard optimization.
In short, if we have an empty |let| or |free| expression,
then we can remove them.
This may happen due to the aliasing rule from last chapter.
Furthermore if a variable is never used, then it can also be removed.
Finally, if we have |let x = e in x|, then we do not need to create the variable |x|.
These are correct
as long as we are careful to make sure that our variable definitions are not recursive.

\begin{boxfigure}
> caseCancel :: Opt
> caseCancel (Case (Comb ConsCall n es) (_++[Branch (Pattern n vs) e] ++ _))
>  = foldr Let e (zip vs es)
> caseCancel (Case (Lit l) (_++[Branch (LPattern l) e] ++ _)) = e
> caseCancel (Case EXEMPT of _) = EXEMPT
> emptyspace
> caseVar (Case (Var x) bs)
>  | x `elem` vars bs = Case (Var x) (map (repCaseVar x) bs)
> repCaseVar x (Branch (Pattern n vs) e)  = Branch (Pattern n vs)  (sub f e)
>   where f v = if v == x then Comb ConsCall n (map Var vs) else Var v
> repCaseVar x (Branch (LPattern l) e)    = Branch (LPattern l)    (sub f e)
>   where f v = if v == x then Lit l else Var v
> emptyspace
> deadCode (Free [] e)  = e
> deadCode (Let [] e)   = e
> deadCode (Free (as++[v]++bs) e)
>  | not (hasVar v e)   = Free (as++bs) e
> deadCode (Let [(v,_)] e)
>  | not (hasVar v e)   = e
> deadCode (Let [(x,e)] (Var x))
>  | not (hasVar x e)   = e
\caption[Case Canceling Implementation]{The code for Case Canceling, Case Variable Elimination, and Dead Code Elimination.}
\label{fig:caseCode}
\end{boxfigure}

\begin{boxfigure}
\textbf{Case Cancel}\\
\begin{minipage}{.40\textwidth}
> case C_i (vec e) of
>     C_i (vec x) -> e'
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> let vec x = vec e
> in e'
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> case l_i of
>     l_i -> e'
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> e'
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> case EXEMPT of alts
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> EXEMPT
\end{minipage}\\
\textbf{Case Var}\\
\begin{minipage}{.40\textwidth}
> case v of
>      vec (C (vec x) -> e')
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> case  v of
>       vec (C (vec x) -> (extend e' v (C (vec x))))
\end{minipage}\\
\textbf{Dead Code}\\
\begin{minipage}{.40\textwidth}
> let free in e
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> e
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> let in e
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> e
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> let v free   in e | v `notelem` e
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> e
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> let v = e'  in e | v `notelem` e
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> e
\end{minipage}\\
\begin{minipage}{.40\textwidth}
> let v = e in v | v `notelem` e
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> e
\end{minipage}\\
    \caption[Case Canceling Optimization]{Case Canceling, Case Variable, and Dead Code Elimination optimizations.}
    \label{fig:CaseCancel}
\end{boxfigure}

Now that we have finally created an optimization,
we can get back to moving code around in convoluted patterns.
In the next section we look at how we can inline functions.
Unlike Case Canceling, It is harder to determine the correctness of Inlining.
In fact, we have to do a lot of work to inline functions in Curry.

\section{Inlining} \label{sec:opts:inline}

As mentioned at the start of this chapter, inlining is not generally valid in Curry.
So, we need to establish cases when inlining is valid,
determine when it is a good idea to inline,
and ensure that our inlining algorithm is correct.
This work is largely based on \cite{haskellInliner, CarruthInline}.

Similarly to \cite{haskellInliner}, we need to make a distinction between
inlining and reduction.
When we use the term \textit{inlining} we are referring to replacing a let bound variable with
it is definition.
For example |let x = True in not x| could inline to |not True|.
When we use the term \textit{reduction}, we are referring to replacing a function call
with the body of the function where the parameters of the function are replaced
with the arguments of the call.
Again, as an example |let x = True in not x| could reduce to:

\begin{minipage}{\textwidth}
> let  x = True
> in   case  x of
>            True   -> False
>            False  -> True
\end{minipage}

The first problem with inlining and reduction we encounter is recursion.
Consider the expression:

> let loop = loop in ...

If we were to inline this variable, we could potentially send the optimizer into an infinite loop.
So, we need to somehow mark all recursive variables and functions.
The next problem follows immediately after that.
So far we have done transformations with local information,
but reduction is going to require global information.
In fact, for reduction to be effective, it will require information from different modules.
Consider the function:

> sumPrimes = foldr (+) 0 . filter isPrime . enumFromTo 1

Aside from the fact that |sumPrimes| contains mostly recursive functions,
we would not be able to optimize it anyway, because |.| is defined in the standard Prelude.
If we can not reduce the definition of |.|, then we are fighting a losing battle.

This brings us to our third problem with inlining.
The |sumPrimes| function is actually partially applied.
Its type should be |sumPrimes :: Int -> Int|,
but |sumPrimes| is defined in a point-free style.
Point-free programming causes a lot of problems, specifically because
FlatCurry is a combinator language.
In IRs like Haskell's Core, we could solve this problem by inlining 
a lambda expression, but it is not clear at all that inlining a lambda
expression is valid in Curry.
Instead, to solve this problem, we convert functions to be fully applied.

In order to solve these problems, 
we keep a map from function names to several attributes about the function.
This includes: if the function is defined externally;
if the function is known to be deterministic;
if the function contains cases;
the parameters of the function; 
the current number of variables in a function;
the size of the function;
and the function definition.
This map is updated every time we optimize a new function,
so we can reduce all functions that we have already optimized.
We will use this map to determine when it is safe and effective to reduce a function.

\subsection{Partial Applications} \label{sec:opts:part}

Dealing with partial applications is a bit more tricky.
In fact, we can not use the \gassp system to solve this problem
because we may not know if a function is a partial application until we have optimized it.
Consider the |sumPrimes| function again.
It does not look like a partial application because the root function, |.|, is fully applied.
Let us look at the definition for |.|.
In Curry it is defined using a lambda expression.

> f . g = \x -> f (g x)

However, when translated to FlatCurry, this lambda expression is turned into a combinator.

\begin{minipage}{\textwidth}
> f . g = compLambda_1 f g
> compLambda f g x = f (g x)
\end{minipage}

So, when we try to optimize |sumPrimes| we end up with 
the derivation in Figure \ref{fig:sumPrimesDeriv}.

\begin{boxfigure}
> let v_1 = p_2
> in let v_2 = foldr_1 v_1 0
> in let v_3 = isPrime_1
> in let v_4 = filter_1 v_3
> in let v_5 = enumFromTo_1 1
> in let v_6 = v_4 . v_5
> in (appOpt (v_2 . v_6))
> REDUCE_BASE ==> [-1,-1,-1,-1,-1,-1]
> let v_1 = p_2
> in let v_2 = foldr_1 v_1 0
> in let v_3 = isPrime_1
> in let v_4 = filter_1 v_3
> in let v_5 = enumFromTo_1 1
> in let v_6 = (appOpt (v_4 . v_5))
> in compLambda_1 v_2 v_6
> REDUCE_LET ==> [-1,-1,-1,-1,-1]
> let v_1 = p_2
> in let v_2 = foldr_1 v_1 0
> in let v_3 = isPrime_1
> in let v_4 = filter_1 v_3
> in let v_5 = enumFromTo_1 1
> in let v_6 = compLambda_1 v_4 v_5
> in compLambda_1 v_2 v_6
    \caption[|sumPrimes| example]{Initial optimization of |sumPrimes|}
    \label{fig:sumPrimesDeriv}
\end{boxfigure}

The |REDUCE_BASE| and |REDUCE_LET| transformations will be described later.
At this point there is no more optimization that can be done,
because everything is a partial function.
But this is not a great result.
We have created a pipeline, and when we pass it a variable, then everything will be fully applied.
So, how do we solve the problem?

The key is to notice that if the root of the body of a function is a partial application,
then we can rewrite our definition.
We simply add enough variables to the function definition so the body of the function
is fully applied.
The transformation \\
$\ $\\
\begin{minipage}{\textwidth}
\noindent
\textbf{Add Missing Variables}\\
\begin{minipage}{.40\textwidth}
> f (vec v) = g_k (vec e)
\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> f (vec v) (vec x) = apply (g_k (vec e)) (vec x)
\end{minipage}\\
\end{minipage}

The |sumPrimes| functions is transformed with the derivation in \ref{fig:addvarDeriv}
and we can continue to optimize the function.

\begin{boxfigure}
> in let v_2 = foldr (pl_1) 0
> in let v_3 = isPrime
> in let v_4 = filter v_3
> in let v_5 = enumFromTo 1
> in let v_6 = compLambda_1 v_4 v_5
> in compLambda_1 v_2 v_6
> ==> MISSING_VAR 
> appOpt apply (  in let v_2 = foldr (pl_1) 0
>                 in let v_3 = isPrime
>                 in let v_4 = filter v_3
>                 in let v_5 = enumFromTo 1
>                 in let v_6 = compLambda_1 v_4 v_5
>                 in compLambda_1 v_2 v_6) x_1
> ==> FLOAT 
> in let v_2 = foldr (pl_1) 0
> in let v_3 = isPrime
> in let v_4 = filter v_3
> in let v_5 = enumFromTo 1
> in let v_6 = compLambda_1 v_4 v_5
> in (appOpt (apply (compLambda_1 v_2 v_6) x_1))
> ==> UNAPPLY
> in let v_2 = foldr (pl_1) 0
> in let v_3 = isPrime
> in let v_4 = filter v_3
> in let v_5 = enumFromTo 1
> in let v_6 = compLambda_1 v_4 v_5
> in compLambda v_2 v_6 x_1
    \caption[Filling in Partial Applications]{Adding a missing variable to |sumPrimes|}
    \label{fig:addvarDeriv}
\end{boxfigure}

\subsection{The Function Table} \label{sec:opts:funTab}
In order to keep track of all of the functions we have optimized we create a function lookup table
called |rfunction| \index{$F_{\mathcal{F}}$}.
The function table is just a map from function names to information about the function.
We use the following definitions for lookups into the function table.
|rinlinable f| \index{$I_{\mathcal{F}}$} returns true if we believe that |f|
is a good candidate for reduction.
We have designed the compiler so that whatever heuristic we use to decide
if a function can be inlined, it is easy to tweak,
but at the very least |f| should not be external, nor too big, and inlining |f|
should not lead to an infinite derivation.
|ruseful x f e| \index{$U_{\mathcal{F}}$} attempts to determine if reducing the function |f| 
in the expression |let x = f ... in e| would be useful.
Again this heuristic is easily tweakable, but currently,
a function is useful if |x| is returned from the function,
it is used as the scrutinee of a case expression,
or it is used in a function that is likely to be reduced.
|rsimple f| \index{$S_{\mathcal{F}}$} returns True if |f| is a simple reduction with no case expressions.
It is always useful to reduce these functions.
|rcancels f [e_1, ... e_n]| \index{$C_{\mathcal{F}}$} returns true if reducing |f| with |e_1 ... e_n|
will likely cause Case Canceling.


\subsection{Function Ordering} \label{sec:opts:order}

The problem of function ordering seems like it should be pretty inconsequential,
but it turns out to be very important.
However, this problem has already been well studied \cite{CarruthInline, haskellInliner},
and the solutions for other languages apply equally well to Curry.

The problem seems very complicated at the start.
We want to know what is the best order to optimize functions.
Fortunately there is a very natural solution.
If possible we should optimize a function before we optimize any function that calls it.
This turns out to be an exercise in Graph Theory.

We define the Call Graph of a set of functions |F = {f_1,f_2, ... f_n}|
to be the graph |G_F = (F, {f_i -> f_j `vert` f_i calls f_j})|.
This problem reduces to finding the topological ordering of |G_F|.
Unfortunately, if |F| contains any recursion, then the topological ordering is not defined.
So, instead, we split |G_F| into strongly connected components, and find the topological ordering
of those components.
Within each component, we pick an arbitrary function, called the \emph{loop breaker}\index{loop breaker},
which is removed from the graph.
This is done with a heuristic based on the number of incoming edges in the graph,
and how likely we think it is to be inlined.
We then attempt to find the topological order of each component again.
This process repeats until our graph is acyclic.

These loop breakers are marked in |rfunction|, and they are never allowed to be reduced.
Every other function can be reduced, because all functions that it calls,
except for possibly the loop breakers, have been optimized.

Consider the program:
\begin{mdframed}
\begin{minipage}{.4\textwidth}
> f x = g x
> g x = h x
> h x = case x of
>            0 -> 0
>            _ -> 1 + f x
\end{minipage}
\begin{minipage}{.6\textwidth}
\centerline{
  \graphxy{
      & \xynode{$f$} \xydS{dl} & \\ %\xydS{dr} & \\
      \xynode{$g$} \xydS{rr} & & \yxnode{$h$} \xydS{ul}\\
  }
  $\Rightarrow$
  \hspace*{2em}
  \graphxy{
      \xynode{$f$} \xydS{d} \\
      \xynode{$g$} \xydS{d} \\
      \xynode{$h$}\\
  }
}
\end{minipage}
\end{mdframed}

The graph for this function is a triangle, because |f| calls |g| which calls |h| which calls |f|.
However, if we mark |h| as a loop breaker, then suddenly this problem is easy.
When we optimize |h|, we are free to reduce |f| and |g|.
We can see the derivation in Figure \ref{fig:loopOptDeriv}.

\begin{boxfigure}
> h x = case x of
>            0 ->  0
>            _ ->  let y = (appOpt (f x))
>                  in 1 + y
> ==> REDUCE_LET
> h x = case x of
>            0 ->  0
>            _ ->  let y = (appOpt (g x))
>                  in 1 + y
> ==> REDUCE_LET
> h x = case x of
>            0 ->  0
>            _ ->  let y = (appOpt (h x))
>                  in 1 + y
    \caption[Optimization of Strongly Connected Functions]{Optimization of strongly connected functions.}
    \label{fig:loopOptDeriv}

\end{boxfigure}


\subsection{Inlining} \label{sec:opts:inline2}

Now that we have everything in order, we can start developing the inlining transformation.
As mentioned before, we need to be careful with inlining.
In general, unrestricted inlining is not valid in Curry.
This is a large change from lazy languages like Haskell, where it is valid,
but not always a good idea.
The other major distinction is that FlatCurry is a combinator language.
This means that we have no lambda expressions,
which limits what we can even do with inlining.

Fortunately for us, these problems actually end up canceling each other out.
In Peyton-Jones work \cite{haskellInliner} most of the focus was on inlining let bound variables,
because this is where duplication of computation could occur.
However, we have two things working for us.
The first is that we can not inline a lambda since they do not exist.
The second is that we have translated FlatCurry to A-Normal Form.
While Haskell programs are put into A-Normal Form when translating to STG code \cite{stg},
this is not the case for Core.
Certain constraints are enforced, such as the trivial constructor argument invariant,
but in general Core is less restricted.

Translating to A-Normal form gives us an important result.
If we inline a constructor then we do not affect the computed results.
This same result holds for literal values, but we will discuss how we handle literals
in Curry in the next chapter.

\begin{theorem}
If |let x = e_1 in e| is a Curry expression in limited A-Normal Form, 
and |e_1| is rooted by a constructor application, or partial application,
then $e[e_1 \from x]$ computes the same results.
\end{theorem}

\begin{proof}
First note that given our semantics for partial application,
a partially applied function is a normal form.
There are no rules for evaluating a partial application,
only for examining one while evaluating an apply node.

If $e_1$ is a constructor, or partial application,
then it is a normal form.
Therefore it is a deterministic expression by definition \ref{def:deterministic}.
Since $e_1$ is deterministic by the path compression theorem,
$e$ evaluates to the same values as |extend e x e_1|
\end{proof}

Now we have enough information to inline variables as long as we restrict inlining to
literals, constructors, variables, and partial applications,
although the case for variables is already subsumed by the |ALIAS| rule \ref{fig:canonical2}.
We add two new rules.  |LET_FOLDING| allows us to move variable definitions
closer to where they are actually used,
and |UNAPPLY| allows us to simplify expressions involving |apply|.
Both of these are useful for inlining and reduction.
The \gassp rules are given in Figure \ref{fig:inline}.
Note that the |UNAPPLY| rule corresponds exactly to the evaluation step for application nodes
in our semantics.
The inlining rules correspond to the cases discussed above.
We add one more rule.  We inline a variable bound to a case expression,
if that expression occurs once in a needed position.
Since we can not determine if the variable occurs in a needed position at compile time,
we can use check if it occurs in a strict position \cite{strictAbstract}.
This is usually good enough.
The implementation of Inlining in the \gassp system is given in Figure \ref{fig:inlineCode}.
The combinator |(x,e) @> sigma| is used to build up substitutions.
It means that we add we add $x \to e$ to the substitution |sigma|.
The |idSub| substitution is just the identity.
In the |letFold| rule, |hasVar| checks is expression |e| contains variable |v|.
In the fist Inlining rule, the |strict| and |uses| functions are just to ensure that |x|
is a in a strict position, and that it is only at one position in |e|.
These are not required for correctness,
we have found that these restrictions generate better code.

\begin{boxfigure}
\textbf{Let Folding}\\
\begin{minipage}{.30\textwidth}
> let  v = e_v 
> in   case e of vec (C_i (vec x) -> e_i)
\end{minipage} \begin{minipage}{.1\textwidth}$\vert$ | v `notelem` e |\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> case  e of 
>       vec (C_i (vec x) -> let v = e_v in e_i)
\end{minipage}\\
\textbf{Unapply}\\
\begin{minipage}{.30\textwidth}
> apply f_k (a_1 ... a_k ... a_n)
\end{minipage} \begin{minipage}{.1\textwidth} $\ $ \end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> let x = f_0 a_1 ... a_k 
> in apply x a_k1 ... a_n
\end{minipage}\\
\begin{minipage}{.30\textwidth}
> apply f_n (a_1 ... a_n)
\end{minipage} \begin{minipage}{.1\textwidth} $\ $ \end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> f (a_1 ... a_n)
\end{minipage}\\
\begin{minipage}{.30\textwidth}
> apply f_k (a_1 ... a_n) | k >   n
\end{minipage} \begin{minipage}{.1\textwidth} $\ $ \end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> f_kn (a_1 ... a_n)
\end{minipage}\\
\textbf{Inlining}\\
\begin{minipage}{.30\textwidth}
> let  x = case e of alts 
> in   e'
\end{minipage} \begin{minipage}{.1\textwidth}$\vert$ | x `elem1` e' |\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> extend e' x (case e of alts)
\end{minipage}\\
\begin{minipage}{.30\textwidth}
> let x = C (vec v) in e
\end{minipage} \begin{minipage}{.1\textwidth}$\vert$ | x `notelem` e |\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> extend e x (C (vec v))
\end{minipage}\\
\begin{minipage}{.30\textwidth}
> let x = f_k (vec v)  in e
\end{minipage} \begin{minipage}{.1\textwidth}$\vert$ | x `notelem` e |\end{minipage} |==>|
\begin{minipage}{.40\textwidth}
> extend e x (f_k (vec v))
\end{minipage}\\
\begin{minipage}{.30\textwidth}
> let x = l in e
\end{minipage} \begin{minipage}{.1\textwidth} $\ $ \end{minipage} |==>|
\begin{minipage}{.40\textwidth}
>  extend e x l
\end{minipage}\\
\caption[Rule for Variable Inlining]{rules for variable inlining.\\
         We need to ensure that |x| is not used recursively before we inline it.
         Guards indicate that the rule fire only if the guard is satisfied.
         The notation $\in_1$ indicates that the variable $x$ occurs exactly once in 
         a strict position in $e'$, so the case will be evaluated.}
\label{fig:inline}
\end{boxfigure}

\begin{boxfigure}
> unapply :: Opt
> unapply (v,_) (apply (Comb (FuncPartCall k) f es) as)
>  = case  compare k n of
>          LT -> Let [(v, Comb FuncCall f (es++as_1))] (apply (Var n) as_2)
>          EQ -> Comb FuncCall f (es++as)
>          GT -> Comb (FuncPartCall (k-n)) f (es++as)
>  where  n             = length as
>         (as_1, as_2)  = splitAt (n-k)
>
> letFold :: Opt
> letFold (n,_) (Let [(v,e_v)] c@(Case e bs))
>  | not (hasVar e v)
>  = Case e (zipWith addVar [1..] bs)
>   where  addVar k (Branch p e)  = Branch p (Let [(n+k,sigma k e_v)] (sigma k e))
>          sigma k                = sub ((v,n+k) @> idSub)
> 
> inline :: Opt
> inline _ (Let [v@(x, Case _ _)] e)
>  | strict x e & uses x e == 1 = sub (v @> idSub) e
>
> inline _ (Let [(x, f@(Comb ct _ _))] e)
>  | (isCons ct || isPart ct) & nonRecursive x f = sub ((x,f) @> idSub) e
>
> inline _ (Let [v@(_, Lit _)] e) = sub (v @> idSub) e

\caption[Implementation of Inlining Rules]{\gassp implementation of the inlining rules.}
\label{fig:inlineCode}
\end{boxfigure}

\subsection{Reduce} \label{sec:opts:reduce}

Finally we come to reduction.
While this was a simpler task than inlining in GHC,
it becomes a very tricky prospect in Curry.
Fortunately, we have already done the hard work.
At this point, in any given function definition,
the only place a function symbol is allowed to appear in our expressions is
as the root of the body,
as the root of a branch in a |case| expression,
as the root the result of a |let| expression,
or as a variable assignment in a let-expression.
Furthermore our functions only contain trivial arguments,
so it is now valid to reduce any function we come across.

\begin{theorem}[reduction]
Let |e| be an expression in limited A-Normal Form,
let |path e p = f (vec e)|,
where |f| is a function symbol with definition |f (vec v) = b|,
and let $\sigma = \{\overline{ v } \mapsto \overline{e}\}$.
Then |extend e p ( sigma b )|
has the same values as |e|.
\end{theorem}

\begin{proof}
First note that There is only one way to replace an expression
where the root has symbol |f|, with the body of the definition for |f|.
Therefore This is a deterministic step, and by the path compression theorem
|e| and |extend e p ( sigma b )| have the same values.
\end{proof}

We give the \gassp rules for reduction in Figure \ref{fig:reduce}.
These rule make use of the function table
We make sure that |rbody f| replaces the definition with fresh variables.
Therefore, we avoid any need to deal with shadowing and name capture.
This strategy was taken from \cite{haskellInliner} and it works very well.
Although, since FlatCurry uses numbers exclusively to represent variables,
we do not get the same readable code.

\begin{boxfigure}
\textbf{Reduce Base:}\\
\begin{minipage}{.30\textwidth}
> f (vec e)
\end{minipage} \begin{minipage}{.14\textwidth} $\vert$ | top && finlinable f | \end{minipage} |==>|
\begin{minipage}{.30\textwidth}
> extend (fbody f) (vec v) (vec e)
\end{minipage}\\
\textbf{Reduce Branch:}\\
\begin{minipage}{.30\textwidth}
> case e' of vec (Ctr -> f (vec e))
\end{minipage} \begin{minipage}{.14\textwidth} $\vert$ | finlinable f | \end{minipage} |==>|
\begin{minipage}{.30\textwidth}
> case e' of vec (extend (rbody f) (vec v) (vec e))
\end{minipage}\\
\textbf{Reduce Let:}\\
\begin{minipage}{.30\textwidth}
> let vec (v_i = e_i) in f (vec e)
\end{minipage} \begin{minipage}{.14\textwidth} $\vert$ | finlinable f | \end{minipage} |==>|
\begin{minipage}{.30\textwidth}
> let vec (v_i = e_i) in extend (rbody f) (vec v) (vec e)
\end{minipage}\\
\textbf{Reduce Useful:}\\
\begin{minipage}{.30\textwidth}
> let x = f (vec e) in e'
\end{minipage} \begin{minipage}{.14\textwidth} $\vert$ | fuseful f | \end{minipage} |==>|
\begin{minipage}{.30\textwidth}
> let x = extend (rbody f) (vec v) (vec e) in e'
\end{minipage}\\
\textbf{Reduce Simple:}\\
\begin{minipage}{.30\textwidth}
> let x = f (vec e) in e'
\end{minipage} \begin{minipage}{.14\textwidth} $\vert$ | fsimple f | \end{minipage} |==>|
\begin{minipage}{.30\textwidth}
> let x = extend (rbody f) (vec v) (vec e) in e'
\end{minipage}\\
\textbf{Reduce Cancels:}\\
\begin{minipage}{.30\textwidth}
> let x = f (vec e) in e'
\end{minipage} \begin{minipage}{.14\textwidth} $\vert$ | fcancels f | \end{minipage} |==>|
\begin{minipage}{.30\textwidth}
> let x = extend (rbody f) (vec v) (vec e) in e'
\end{minipage}\\
\caption[Rules for Reduction]{Rules for reduction.\\
         All expressions are kept in A-Normal Form.
         |REDUCE_BASE| is only run at the root of the body.
         While the last three rules are very similar,
         It is useful to keep them separated for debugging reduction derivations.}
\label{fig:reduce}
\end{boxfigure}

\begin{boxfigure}
> reduce :: FunTable -> Opt
> reduce funs  = reduce_base funs    ? reduce_branch funs
>              ? reduce_let funs     ? reduce_useful funs
>              ? reduce_simple funs  ? reduce_cancels funs
> 
> reduce_base funs (n,True) b@(Comb FuncCall f _)
>   | inlinable funs f = b'
>    where b' = makeReduce funs n b
> 
> reduce_branch funs (n,_)  (Case ct e 
>                           (as++[Branch p b@(Comb FuncCall f _)]++bs))
>   | inlinable funs f = Case ct e (as++[Branch p b']++bs)
>    where b' = makeReduce funs n b
>
> reduce_let funs (n,_) (Let vs b@(Comb FuncCall f _))
>   | inlinable funs f = Let vs b'
>    where b' = makeReduce funs n b
> 
> reduce_useful funs (n,_) (Let [(x,b@(Comb FuncCall f _))] e)
>   |   inlinable funs f &&  useful funs False x e = Let [(x,b')] e
>    where b'   = makeReduce funs n b
> 
> reduce_simple funs (n,_) (Let [(x,b@(Comb FuncCall f _))] e)
>   | simple funs f = Let [(x,b')] e
>    where b' = makeReduce funs n b
> 
> reduce_cancels funs (n,_) (Let [(x,b@(Comb FuncCall f es))] e)
>   |   inlinable funs f &&  cancels funs f (map isConsExpr es) = Let [(x,b')] e
>    where b' = makeReduce funs n b
> 
\caption[Implementation of Reduction Rules]{\gassp code for the Reduce optimizations.\\
         The |makeReduce| function corresponds to |rbody|.
         The function |inlinable|, |useful|, |simple|, and |cancels|
         correspond to |rinlinable, ruseful, rsimple| and |rcancels| respectively.}
\label{fig:reduce_code}

\end{boxfigure}

We end by giving a couple of examples of reductions to see how they work in practice.
The first example returns from the start of this chapter.
We see that |double (0 ? 1)| is reduced so we do not make a needless call to |double|,
but we have avoided the problem of run time choice semantics.

Our next function comes from a possible implementation of |<=| for Boolean values.
In fact, this is the implementation we chose for the instance of the |Ord| class for |Bool|.
The example is a bit long, but it shows how many of these optimizations work together
to produce efficient code.

In the next chapter we discuss three more optimizations, Unboxing, Case Shortcutting, and Deforestation.
While Unboxing and Deforestation are in common use in lazy function compilers,
they have not been used for functional-logic languages before.
Case Shortcutting is a new optimization to Curry.


\begin{boxfigure}
> double x = x + x
> main = double (0 ? 1)
>
> double (appOpt (0 ? 1))
> ==> ANF_APP
> let v_1 = 0 ? 1
> in (appOpt (double v_1))
> ==> REDUCE_LET
> let v_1 = 0 ? 1
> in v_1 + v_1
\caption[Derivation of |double (0 ? 1)|]{Derivation of |double (0 ? 1)| showing that we still arrive at an equivalent expression.}
\label{fig:doubleReduce}
\end{boxfigure}

\begin{boxfigure}
> not v_1 = case  v_1 of 
>                 True   -> False
>                 False  -> True
> v_1 && v_2 = case  v_1 of 
>                    True   -> v_2
>                    False  -> False
> v_1 || v_2 = case  v_1 of 
>                    True   -> True
>                    False  -> v_2
> v_1 <= v_2 = not v_1 || v_2
\caption{[Definition of |<=| for |Bool|]Definition of |<=| for |Bool|.}
\label{fig:boolLeq}
\end{boxfigure}

\begin{boxfigure}
> appOpt ((not v_1) || v_2)
> ==> ANF_APP []
> let v_3 = appOpt (not v_1)
> in v_3 || v_2
> ==> REDUCE_USEFUL []
> let v_3 = case  v_1 of 
>                 True   -> False 
>                 False  -> True 
> in appOpt (v_3 || v_2)
> ==> REDUCE_LET []
> let  v_3 = case  v_1 of
>                  True   -> False
>                  False  -> True
> in   case  v_3 of
>            True   -> appOpt v_3
>            False  -> v_2
> ==> CASE_VAR [-1]
> let  v_3 = case  v_1 of
>                  True   -> False
>                  False  -> True
> in   case  v_3 of
>            True   -> True
>            False  -> v_2
\caption[Derivation of |<=| 1]{Derivation of |<=| for |Bool|}
\label{fig:LtEqReduce}
\end{boxfigure}

\begin{boxfigure}
> ==> INLINE_CASE []
> case  (case  v_1 of
>              True   -> False
>              False  -> True) of
>       True   -> True
>       False  -> v_2
> ==> CASE_IN_CASE []
> case  v_1 of
>       True   ->  let appOpt (v7 = False)  in case  v7 of 
>                                                    True   -> True 
>                                                    False  -> v_2
>       False  ->  let v8 = True            in case  v8 of 
>                                                    True   -> True 
>                                                    False  -> v_2
> ==> INLINE_CONS [0]
> case  v_1 of
>       True -> case  appOpt False of
>                     True   -> True
>                     False  -> v_2
>       False  ->  let v8 = True  in case  v8 of 
>                                          True   -> True 
>                                          False  -> v_2
\caption[Derivation of |<=| 2]{Derivation of |<=| for |Bool|}
\label{fig:LtEqReduce2}
\end{boxfigure}

\begin{boxfigure}
> ==> CASE_CANCEL_CONS [0]
> case  v_1 of
>       True   -> v_2
>       False  -> let appOpt (v8 = True) in case  v8 of 
>                                                 True   -> True 
>                                                 False  -> v_2
> ==> INLINE_CONS [1]
> case  v_1 of
>       True   -> v_2
>       False  -> case  appOpt True of
>                       True   -> True
>                       False  -> v_2
> ==> CASE_CANCEL_CONS [1]
> case  v_1 of
>       True   -> v_2
>       False  -> True
\caption[Derivation of |<=| 3]{Derivation of |<=| for |Bool| continued}
\label{fig:LtEqReduce3}

\end{boxfigure}

